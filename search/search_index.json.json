{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Customer analysis with cognitive and analytics in hybrid cloud This project is part of the 'IBM Hybrid Analytics and Big Data Architecture' reference architecture implementation, available at https://github.com/ibm-cloud-architecture/refarch-analytics. The goal of this implementation is to deliver a reference implementation for data management and service integration to consume structured and unstructured data to assess customer attrition. Modern applications are leveraging a set of capabilities to do a better assessment of customer characteristics and deliver the best actions or recommendations. The technologies involved, include artificial intelligence, data governance, ingestion, enrichment, storage, analysis, machine learning, unstructured data classifications, natural language understanding, tone analysis, and hybrid integration.... Update 05/10/19 Target audiences IT Architects who want to understand the components involved and the architecture constraints and design considerations Developers who want to get starting code, and educate themselves on the related technologies Data Scientists who want to complement machine learning with cognitive output like classification Key points Data scientists need different source of data, structured from traditional SQL based database (e.g. the customers and accounts data) and unstructured output of new cognitive services. Data Scientists work hand by hand with application developers to quickly deliver solution to the business. Data access layer to traditional relational data should be done with a micro service approach exposing RESTful API or using modern z OS Connect application. Cloud native apps, microservices, can be deployed in public or private cloud, like IBM Cloud private or IKS based on Kubernetes and containers. Public services like the Watson services can be easily integrated within the solution: Watson Data Platform, Tone Analyzer, Watson Assistant. API management is used to present a unique API for customer data, standardize for consumers like the webapp, even if the back end is Java based or z Connected based. Product recommendations may be added to the solution to support business decision from a chatbot conversation taking into account the churn scoring risk. See this note to explain how to leverage IBM Operational Decision Management for that. Further readings Data Analysis using Spark on zOS and Jupyter Notebooks Data Science eXperience ODM Decision Service for product recommendation","title":"Welcome"},{"location":"#customer-analysis-with-cognitive-and-analytics-in-hybrid-cloud","text":"This project is part of the 'IBM Hybrid Analytics and Big Data Architecture' reference architecture implementation, available at https://github.com/ibm-cloud-architecture/refarch-analytics. The goal of this implementation is to deliver a reference implementation for data management and service integration to consume structured and unstructured data to assess customer attrition. Modern applications are leveraging a set of capabilities to do a better assessment of customer characteristics and deliver the best actions or recommendations. The technologies involved, include artificial intelligence, data governance, ingestion, enrichment, storage, analysis, machine learning, unstructured data classifications, natural language understanding, tone analysis, and hybrid integration.... Update 05/10/19","title":"Customer analysis with cognitive and analytics in hybrid cloud"},{"location":"#target-audiences","text":"IT Architects who want to understand the components involved and the architecture constraints and design considerations Developers who want to get starting code, and educate themselves on the related technologies Data Scientists who want to complement machine learning with cognitive output like classification","title":"Target audiences"},{"location":"#key-points","text":"Data scientists need different source of data, structured from traditional SQL based database (e.g. the customers and accounts data) and unstructured output of new cognitive services. Data Scientists work hand by hand with application developers to quickly deliver solution to the business. Data access layer to traditional relational data should be done with a micro service approach exposing RESTful API or using modern z OS Connect application. Cloud native apps, microservices, can be deployed in public or private cloud, like IBM Cloud private or IKS based on Kubernetes and containers. Public services like the Watson services can be easily integrated within the solution: Watson Data Platform, Tone Analyzer, Watson Assistant. API management is used to present a unique API for customer data, standardize for consumers like the webapp, even if the back end is Java based or z Connected based. Product recommendations may be added to the solution to support business decision from a chatbot conversation taking into account the churn scoring risk. See this note to explain how to leverage IBM Operational Decision Management for that.","title":"Key points"},{"location":"#further-readings","text":"Data Analysis using Spark on zOS and Jupyter Notebooks Data Science eXperience ODM Decision Service for product recommendation","title":"Further readings"},{"location":"businessproblem/","text":"An unhappy customer Eddie is an existing Green Telco Inc customer living in Orlando Fl. He has been using the services provided by the Green Telco for the last 2 years. Currently, he is not under any contract. Eddie signed for a new phone which was \"buy one get one\" free bundle. He bought one of the phone with cash and he put the other phone on a monthly plan. For Eddie to get the second phone free he has to submit the receipt to the Green Telco to get an equivalent value of the phone as a credit card. Eddie was traveling to conferences in Madrid and in Bangkok so he also signed up for international text and data service. So he owns two phones: Sam Milky Way Phone 1 $750 paid in full with cash Sam Milky Way Phone 2 $750 on a monthly plan of $30 Upon submission of receipt the Green Telco company sends a Credit Card of $750. Eddie can use the Credit card to pay the balance of $750 for phone 2. Further, Eddie signed for free international text and data for a fee of $10 a month when he is traveling overseas to Madrid and, Bangkok. After Eddie submitted the receipt to the Green Telco provider, Eddie went to Madrid and then to Bangkok. While in Bangkok the data plan and the text plan was very slow and did not perform to the level that the sales person at the Green Telco had suggested Eddie that it will work. By paying $10 per month extra Eddie was told that he will get double the speed. In reality the original speed was very less and even after double the speed, the speed was very less. So, Eddie was not very happy with the service. Eddie, felt that the sales person did not reveal all the information. To add insult to injury when Eddie came back to check on his status for the free phone (he had not received any notification from the Green Telco). So, Eddie contacts the Telecom provider support via a chat bot on the web application. The telecom provider informs Eddie that his claim was rejected and that he does not qualify for the $750 phone. Eddie, who was already not happy with the International Data and Text service he had signed-up for and now he was not getting his refund. The chat bot then checks the customer churn scoring service and finds out that the customer churn predicts a churn with a very high level of confidence and immediately notifies Eddie that a supervisor will contact Eddie to resolve the issue. The supervisor contacts Eddie immediately. He apologizes to Eddie that the speed to begin with is very low and even by adding $10 the double speed does not make much of a difference. This service is mostly designed for texting and browsing emails and some important work and not designed for the kind of quality of service that Eddie is used to in the US. Further the supervisor explains to Eddie that the reason he was not given the $750 is because both the phones have to be on contract payment. Since one of the phone was a cash payment the promotion does not apply. The supervisor acknowledged that to fix the problem, she had called the store and if Eddie goes to the store the previous transaction will be reversed and both the phone would be put on contract and his $750 payment will be expedited. Eddie walks in the store and his problem was resolved. Once, he got the refund in one week he paid the first phone with his cash and the second phone with the $750 visa card. The customer churn service helped Eddie to remain as a customer for the Green Telco.","title":"Presentation"},{"location":"businessproblem/#an-unhappy-customer","text":"Eddie is an existing Green Telco Inc customer living in Orlando Fl. He has been using the services provided by the Green Telco for the last 2 years. Currently, he is not under any contract. Eddie signed for a new phone which was \"buy one get one\" free bundle. He bought one of the phone with cash and he put the other phone on a monthly plan. For Eddie to get the second phone free he has to submit the receipt to the Green Telco to get an equivalent value of the phone as a credit card. Eddie was traveling to conferences in Madrid and in Bangkok so he also signed up for international text and data service. So he owns two phones: Sam Milky Way Phone 1 $750 paid in full with cash Sam Milky Way Phone 2 $750 on a monthly plan of $30 Upon submission of receipt the Green Telco company sends a Credit Card of $750. Eddie can use the Credit card to pay the balance of $750 for phone 2. Further, Eddie signed for free international text and data for a fee of $10 a month when he is traveling overseas to Madrid and, Bangkok. After Eddie submitted the receipt to the Green Telco provider, Eddie went to Madrid and then to Bangkok. While in Bangkok the data plan and the text plan was very slow and did not perform to the level that the sales person at the Green Telco had suggested Eddie that it will work. By paying $10 per month extra Eddie was told that he will get double the speed. In reality the original speed was very less and even after double the speed, the speed was very less. So, Eddie was not very happy with the service. Eddie, felt that the sales person did not reveal all the information. To add insult to injury when Eddie came back to check on his status for the free phone (he had not received any notification from the Green Telco). So, Eddie contacts the Telecom provider support via a chat bot on the web application. The telecom provider informs Eddie that his claim was rejected and that he does not qualify for the $750 phone. Eddie, who was already not happy with the International Data and Text service he had signed-up for and now he was not getting his refund. The chat bot then checks the customer churn scoring service and finds out that the customer churn predicts a churn with a very high level of confidence and immediately notifies Eddie that a supervisor will contact Eddie to resolve the issue. The supervisor contacts Eddie immediately. He apologizes to Eddie that the speed to begin with is very low and even by adding $10 the double speed does not make much of a difference. This service is mostly designed for texting and browsing emails and some important work and not designed for the kind of quality of service that Eddie is used to in the US. Further the supervisor explains to Eddie that the reason he was not given the $750 is because both the phones have to be on contract payment. Since one of the phone was a cash payment the promotion does not apply. The supervisor acknowledged that to fix the problem, she had called the store and if Eddie goes to the store the previous transaction will be reversed and both the phone would be put on contract and his $750 payment will be expedited. Eddie walks in the store and his problem was resolved. Once, he got the refund in one week he paid the first phone with his cash and the second phone with the $750 visa card. The customer churn service helped Eddie to remain as a customer for the Green Telco.","title":"An unhappy customer"},{"location":"labs/","text":"Labs tutorial If you want to deploy and run the solution and even tune it you can do the following labs. The overall time will be around 3 hours.","title":"Tutorial"},{"location":"labs/#labs-tutorial","text":"If you want to deploy and run the solution and even tune it you can do the following labs. The overall time will be around 3 hours.","title":"Labs tutorial"},{"location":"run/","text":"Run the different components of the solution For Watson Conversation service Create the service in IBM Cloud and get the credentials, update the configuration file (config.json and values.yaml) for the webapp to access the conversation service. See this note about implementation For Tone analyzer Create the service in IBM Cloud and get the credentials. See also this note for more details. Jenkins pipeline We deployed a Jenkins server on IBM Cloud Private following the instructions described here , with a PVC named jenkins-home under the greencompute namespace and the commands: $ helm install --name greenjenkins --set Persistence.ExistingClaim=jenkins-home --set Master.ImageTag=2.67 stable/jenkins --namespace greencompute $ cd chart/jenkins $ kubectl create -f docker-reg-configMap.yaml --namespace greencompute $ kubectl create -f registry-secret.yaml --namespace greencompute Then we added configMap and secret in kubernetes cluster to keep docker private registry information so that build job can publish docker images to the registry automatically. The webapp and customer service projects have jenkinsfile that can be used in a Jenkins Pipeline. Pipelines are made up of multiple steps that allow you to build, test and deploy applications. Then define the URL of the github repository and specify the branch to checkout. The pipeline is created. You can manually start the build job that runs the pipeline. The job pulls the code from Git repository's master branch and runs the Jenkins file. Then, it builds the Docker image and pushes it to the IBM Cloud Private Docker registry. Run Run the web application locally To start the application using node monitoring tool use the command: npm run dev To run in non-development mode npm start The trace should display a message like below with the url to use [1] starting `node server/server server/server` [1] Server v0.0.1 starting on http://localhost:3001 Point your web browser to the url: http://localhost:3001 to get access to the user interface of the home page. The demonstration script is described in this note Run web app on IBM Cloud Private The application can be deployed with helm install . Once the docker image is built, you need to remote connect to the master node where the docker private repository resides, and push the image to the repo: $ docker build -t ibmcase/greenapp . $ docker tag ibmcase/greenapp greencluster.icp:8500/greencompute/greenapp:v0.0.1 $ docker login greencluster.icp:8500 $ docker push greencluster.icp:8500/greencompute/greenapp:v0.0.1 Deploy the release using the helm First you need to rename the file values-tmpl.yaml to values.yaml and set the parameters as the config.json. We are using the mechanism of config map to externalize the configuration as defined by the config.json . While using cloud foundry or pure local nodejs deployment this file is read from the filesystem by the server.js. But with kubernetes pods the best practice is to export this configuration into ConfigMap . To do so we need to create a new template: templates/configmap.yaml . This file uses the same structure as the config.json file: apiVersion: v1 kind: ConfigMap metadata: name: {{ template \"fullname\" . }} labels: chart: \"{{ .Chart.Name }}-{{ .Chart.Version | replace \"+\" \"_\" }}\" data: config.json: |+ { \"conversation\" :{ \"version\" : \"{{ .Values.config.conversation.version }}\", \"versionDate\":\"{{ .Values.config.conversation.versionDate }}\", \"username\":\"{{ .Values.config.conversation.username }}\", \"password\":\"{{ .Values.config.conversation.password }}\", \"conversationId\":\"{{ .Values.config.conversation.conversationId }}\", \"workspace\":\"{{ .Values.config.conversation.workspace }}\", \"usePersistence\": \"{{ .Values.config.conversation.usePersistence }}\" }, \"customerAPI\":{ \"url\":\"{{ .Values.config.customerAPI.url }}\", \"host\":\"{{ .Values.config.customerAPI.host }}\", \"xibmclientid\": \"{{ .Values.config.customerAPI.xibmclientid }}\" }, \"toneAnalyzer\":{ \"url\": \"{{ .Values.config.toneAnalyzer.url }}\", \"versionDate\": \"{{ .Values.config.toneAnalyzer.versionDate }}\", \"username\": \"{{ .Values.config.toneAnalyzer.username }}\", \"password\": \"{{ .Values.config.toneAnalyzer.password }}\" }, \"scoringService\":{ \"type\": \"{{ .Values.config.scoringService.type }}\", \"baseUrl\": \"{{ .Values.config.scoringService.baseUrl }}\", \"instance\": \"{{ .Values.config.scoringService.instance }}\", \"username\": \"{{ .Values.config.scoringService.username }}\", \"password\": \"{{ .Values.config.scoringService.password }}\" }, \"dbCredentials\" : { \"url\": \"{{ .Values.config.dbCredentials.url }}\" }, \"debug\": \"{{ .Values.config.debug }}\", \"port\": \"{{ .Values.config.port }}\", \"version\": \"{{ .Values.config.version }}\" } As you can see the real values are set in the values.yaml file. This is an implementation decision to externalize all values in this file, we could have set the value directly in the template as they are not used anywhere else. Modify deployment.yaml To 'inject' the configuration from the configMap to the server nodejs app, the trick is to specify that the config.json file is coming from a logical volume: In the deployment.yaml we add a volumeMount point to the container specification: spec: containers: - name: {{ .Chart.Name }} image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\" imagePullPolicy: {{ .Values.image.pullPolicy }} ports: - containerPort: {{ .Values.service.internalPort }} volumeMounts: - name: config mountPath: /greenapp/server/config/config.json subPath: config.json the path /greenapp comes from the dockerfile, working directory declaration: COPY . /greenapp WORKDIR /greenapp so the mountPath will overwrite the config.json file. The volume name (config) is arbitrary but needs to match a volume declared later in the deployment.yaml. volumes: - name: config configMap: name: {{ template \"fullname\" . }} One volume, named config uses the configMap named using the template name of the helm package and match the configMap we defined above. Deploy the chart as a new release $ cd ../chart $ helm install green-customerapp/ --name green-customerapp --namespace greencompute Assess deployment is successful: $ kubectl logs <pod name> --namespace greencompute Once deployed and started the Web application can be seen at the URL: http://greenapp.green.case Run the Jupyter notebook The main root project for green compute includes a dockerfile to get all the interesting components you may want to run to execute and develop Jupyter notebooks on your own. If you use docker build -t pysparktf . , you should get the image with python, sklearn, all spark python modules and even Tensorflow. REPOSITORY TAG IMAGE ID CREATED SIZE pysparktf latest ee5d45e40097 2 months ago 5.29G pyspark latest 06821066e790 3 months ago 5.01GB jupyter/pyspark-notebook latest f4f7de14d4a7 3 months ago 4.56GB From those docker images you can run a container that mounts your local folder where you have notebook under the work folder. The script ./startLocalJupyter.sh , and then run the kernel.","title":"Run the different components of the solution"},{"location":"run/#run-the-different-components-of-the-solution","text":"","title":"Run the different components of the solution"},{"location":"run/#for-watson-conversation-service","text":"Create the service in IBM Cloud and get the credentials, update the configuration file (config.json and values.yaml) for the webapp to access the conversation service. See this note about implementation","title":"For Watson Conversation service"},{"location":"run/#for-tone-analyzer","text":"Create the service in IBM Cloud and get the credentials. See also this note for more details.","title":"For Tone analyzer"},{"location":"run/#jenkins-pipeline","text":"We deployed a Jenkins server on IBM Cloud Private following the instructions described here , with a PVC named jenkins-home under the greencompute namespace and the commands: $ helm install --name greenjenkins --set Persistence.ExistingClaim=jenkins-home --set Master.ImageTag=2.67 stable/jenkins --namespace greencompute $ cd chart/jenkins $ kubectl create -f docker-reg-configMap.yaml --namespace greencompute $ kubectl create -f registry-secret.yaml --namespace greencompute Then we added configMap and secret in kubernetes cluster to keep docker private registry information so that build job can publish docker images to the registry automatically. The webapp and customer service projects have jenkinsfile that can be used in a Jenkins Pipeline. Pipelines are made up of multiple steps that allow you to build, test and deploy applications. Then define the URL of the github repository and specify the branch to checkout. The pipeline is created. You can manually start the build job that runs the pipeline. The job pulls the code from Git repository's master branch and runs the Jenkins file. Then, it builds the Docker image and pushes it to the IBM Cloud Private Docker registry.","title":"Jenkins pipeline"},{"location":"run/#run","text":"","title":"Run"},{"location":"run/#run-the-web-application-locally","text":"To start the application using node monitoring tool use the command: npm run dev To run in non-development mode npm start The trace should display a message like below with the url to use [1] starting `node server/server server/server` [1] Server v0.0.1 starting on http://localhost:3001 Point your web browser to the url: http://localhost:3001 to get access to the user interface of the home page. The demonstration script is described in this note","title":"Run the web application locally"},{"location":"run/#run-web-app-on-ibm-cloud-private","text":"The application can be deployed with helm install . Once the docker image is built, you need to remote connect to the master node where the docker private repository resides, and push the image to the repo: $ docker build -t ibmcase/greenapp . $ docker tag ibmcase/greenapp greencluster.icp:8500/greencompute/greenapp:v0.0.1 $ docker login greencluster.icp:8500 $ docker push greencluster.icp:8500/greencompute/greenapp:v0.0.1","title":"Run web app on IBM Cloud Private"},{"location":"run/#deploy-the-release-using-the-helm","text":"First you need to rename the file values-tmpl.yaml to values.yaml and set the parameters as the config.json. We are using the mechanism of config map to externalize the configuration as defined by the config.json . While using cloud foundry or pure local nodejs deployment this file is read from the filesystem by the server.js. But with kubernetes pods the best practice is to export this configuration into ConfigMap . To do so we need to create a new template: templates/configmap.yaml . This file uses the same structure as the config.json file: apiVersion: v1 kind: ConfigMap metadata: name: {{ template \"fullname\" . }} labels: chart: \"{{ .Chart.Name }}-{{ .Chart.Version | replace \"+\" \"_\" }}\" data: config.json: |+ { \"conversation\" :{ \"version\" : \"{{ .Values.config.conversation.version }}\", \"versionDate\":\"{{ .Values.config.conversation.versionDate }}\", \"username\":\"{{ .Values.config.conversation.username }}\", \"password\":\"{{ .Values.config.conversation.password }}\", \"conversationId\":\"{{ .Values.config.conversation.conversationId }}\", \"workspace\":\"{{ .Values.config.conversation.workspace }}\", \"usePersistence\": \"{{ .Values.config.conversation.usePersistence }}\" }, \"customerAPI\":{ \"url\":\"{{ .Values.config.customerAPI.url }}\", \"host\":\"{{ .Values.config.customerAPI.host }}\", \"xibmclientid\": \"{{ .Values.config.customerAPI.xibmclientid }}\" }, \"toneAnalyzer\":{ \"url\": \"{{ .Values.config.toneAnalyzer.url }}\", \"versionDate\": \"{{ .Values.config.toneAnalyzer.versionDate }}\", \"username\": \"{{ .Values.config.toneAnalyzer.username }}\", \"password\": \"{{ .Values.config.toneAnalyzer.password }}\" }, \"scoringService\":{ \"type\": \"{{ .Values.config.scoringService.type }}\", \"baseUrl\": \"{{ .Values.config.scoringService.baseUrl }}\", \"instance\": \"{{ .Values.config.scoringService.instance }}\", \"username\": \"{{ .Values.config.scoringService.username }}\", \"password\": \"{{ .Values.config.scoringService.password }}\" }, \"dbCredentials\" : { \"url\": \"{{ .Values.config.dbCredentials.url }}\" }, \"debug\": \"{{ .Values.config.debug }}\", \"port\": \"{{ .Values.config.port }}\", \"version\": \"{{ .Values.config.version }}\" } As you can see the real values are set in the values.yaml file. This is an implementation decision to externalize all values in this file, we could have set the value directly in the template as they are not used anywhere else.","title":"Deploy the release using the helm"},{"location":"run/#modify-deploymentyaml","text":"To 'inject' the configuration from the configMap to the server nodejs app, the trick is to specify that the config.json file is coming from a logical volume: In the deployment.yaml we add a volumeMount point to the container specification: spec: containers: - name: {{ .Chart.Name }} image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\" imagePullPolicy: {{ .Values.image.pullPolicy }} ports: - containerPort: {{ .Values.service.internalPort }} volumeMounts: - name: config mountPath: /greenapp/server/config/config.json subPath: config.json the path /greenapp comes from the dockerfile, working directory declaration: COPY . /greenapp WORKDIR /greenapp so the mountPath will overwrite the config.json file. The volume name (config) is arbitrary but needs to match a volume declared later in the deployment.yaml. volumes: - name: config configMap: name: {{ template \"fullname\" . }} One volume, named config uses the configMap named using the template name of the helm package and match the configMap we defined above. Deploy the chart as a new release $ cd ../chart $ helm install green-customerapp/ --name green-customerapp --namespace greencompute Assess deployment is successful: $ kubectl logs <pod name> --namespace greencompute Once deployed and started the Web application can be seen at the URL: http://greenapp.green.case","title":"Modify deployment.yaml"},{"location":"run/#run-the-jupyter-notebook","text":"The main root project for green compute includes a dockerfile to get all the interesting components you may want to run to execute and develop Jupyter notebooks on your own. If you use docker build -t pysparktf . , you should get the image with python, sklearn, all spark python modules and even Tensorflow. REPOSITORY TAG IMAGE ID CREATED SIZE pysparktf latest ee5d45e40097 2 months ago 5.29G pyspark latest 06821066e790 3 months ago 5.01GB jupyter/pyspark-notebook latest f4f7de14d4a7 3 months ago 4.56GB From those docker images you can run a container that mounts your local folder where you have notebook under the work folder. The script ./startLocalJupyter.sh , and then run the kernel.","title":"Run the Jupyter notebook"},{"location":"Z/","text":"Z OS Deployment In this section we are detailing part of the backend service implementation using z Connect and DB2 on Z. The JAXRS micro service that manages the operation on the customer data source is mapped to a zConnect implementation. The DB2 schema is kept to have the 3 or 4 tables of the CUSTDB instance. The API used for the implementation in the zOS Connect service can be found here You can see that there are 4 different Db2 tables associated to this API and a single INSERT will not work. To make this work, you will need to create a Native SQL procedure that you can invoke using a single CALL SQL statement which satisfies the requirement of a single SQL statement by z/OS Connect but internally is doing 4 INSERT SQL statements. The API to do POST and GET for the products could be quick since it seems to map directly to the PRODUCTS table but I need to understand what is expected on the products list below (red box).","title":"Z deployment"},{"location":"Z/#z-os-deployment","text":"In this section we are detailing part of the backend service implementation using z Connect and DB2 on Z. The JAXRS micro service that manages the operation on the customer data source is mapped to a zConnect implementation. The DB2 schema is kept to have the 3 or 4 tables of the CUSTDB instance. The API used for the implementation in the zOS Connect service can be found here You can see that there are 4 different Db2 tables associated to this API and a single INSERT will not work. To make this work, you will need to create a Native SQL procedure that you can invoke using a single CALL SQL statement which satisfies the requirement of a single SQL statement by z/OS Connect but internally is doing 4 INSERT SQL statements. The API to do POST and GET for the products could be quick since it seems to map directly to the PRODUCTS table but I need to understand what is expected on the products list below (red box).","title":"Z OS Deployment"},{"location":"apim/","text":"Defining customer API product We are detailing how the customer API product was developed to integrate with the REST api back end. This is not a tutorial, this is more for education purpose. Pre-requisite You need to have access to IBM API Connect management product. We have one instance within our on-premise server, soon we will use one on ICP. You need to access the swagger file from the service project . Development steps done Step 1: Create product from swagger Connect to the API manager, our url is 172.16.40.88/apim. From the main page click Add and select Import API from file . Select the swagger src/main/webapp/META-INF/stub/swagger.yaml from the integation-service project: this should create a project customer-management-micro-service-api. The Paths section of the definition includes the CRUD operation on the customers resource: You do not need to use all the operations of the imported swagger, and it is possible to delete operations that we do not want to expose to external client applications. Instead of using an already defined swagger you can develop the API definition manually in the API Designer tool and the following product documentation gives you some how to do it. https://www.ibm.com/support/knowledgecenter/en/SSMNED_5.0.0/com.ibm.apic.toolkit.doc/task_APIonPrem_createapi.html Finally you can create a default product directly from the APIs: Products are just packaging for APIs and Plans. Step 2: Define operation mapping You have two options here: taking the generic approach where all the defined APIs from the swagger are used as is in the new Product and the API gateway is just a proxy do some specific mappings, select a sub set of APIs and may be do some interface change. API management is using its own vocabulary for such implementation: Policies are the building blocks of assembly flows, and they provide the means to configure capability, such as security, logging, routing of requests to target services, and transformation of data from one format to another. Logic constructs behave in a similar way to policies, but they affect how and which parts of the assembly are implemented without modifying the data flow of the assembly. Proxy policy To implement a generic proxy approach, open the Assembly editor and create a new assembly: between the two circles drag and drop a Proxy policy: Then in the URL specify the end point of the virtual host name of the deployed micro service within ICP. The hostname was specified within the deployment configuration of the service when deploying it to ICP within the Ingress construct. yaml ingress: enabled: true # Used to create Ingress record (should used with service.type: ClusterIP). hosts: - customer.green.case The URL includes the webcontext and URL mapping as defined in the web.xml file of the java webapp. http://customer.green.case/caseserv/api/v1$(request.path) The variable request.path is the one defined in the Paths, for example can be /customers/{id} Attention!: there is something wrong here!. How API gateway that will run this product implementation will know how to map the customer.green.case host to the ICP proxy server responsible to route the traffic to the pod running the backend service? As this name is only known by ICP internal kubernetes DNS service. The solution is to define static host alias inside the API Gateway, or use a DNS server inside the same network. Our API datapower gateway is on the same IP network as our ICP cluster: 172.16. . . To add a host alias, connect to the API Gateway console using the port 9090. (https://172.16...:9090/console) as an admin user and using the Default domain so you can access the global settings. Then select network > interface > host alias to define a new entry. Once the configuration is saved you can use the ping remote host feature: Before going to the unit test let see the other approach: Map some operations For each operation, define the mapping to the back end operation. So go to the Assembly editor , and drag and drop the Operation switch logic operator: Add one case per operation you want to do a mapping to. We start by the three read only operations we need for the user interface to call: get all customer, get a customer given its id, and giving its email. The following diagram illustrates an Invoke policy to call the back end: Specify the URL as before. There is way to add scripts, mapping ... before and after the invoke if needed. Step 3: unit test To unit test, you need first to republish the product Then select the operation to trigger, like /customers/{id}, set the parameter, id=3 and invoke the service You should get the data coming from the customer database. Step 4: define and deploy product Some important concepts to keep in mind: products provide a method by which you can group APIs into a package that is intended for a particular use or channel. Products contain Plans, which can be used to differentiate between different offerings. To make an API available to an application developer, it must be included in a Plan. Products are then published in a Catalog. A lifecycle manager can then control the availability and visibility of APIs and Plans through the API Manager. The application developer can only subscribe to one Plan from a specific Product. In the product view, select the upload button on the right side to stage the product to a specific environment. Our catalog is called sandbox . You can see from the figure above we select a default plan. In production you will define a plan according to business requirements. Publish the product to the catalog, you should be able to see the new URL for the API endpoint: (e.g. https://172.16.50.8/csplab/sb/customers) Once deployed developers can access the API Portal to define the application the developer will implement and that subscribed to this product. Step 5: Define application for developer Using the portal at a URL like: https://172.16..../csplab/sb you can access to the API portal for developers. Developers have an account, and need to register their application before using an API. When you register an application, the application is assigned a unique client ID and client secret. You must use the client ID when you call an API that requires you to identify your application by using a client ID. Create an application In App menu, use Create an application button. Specify a name Then browse existing API products and select the customer API, then subscribe: Now you should have a clientID to be used in future request from the application. This client id needs to be part of the HTTP header for any request to the api. You should be able to do an integration test to validate the path API product deployed on gateway -> Java REST micro service on ICP -> DB2, using a tool like postman or SOAP UI. The following figure illustrates a GET request to the API URL to get a customer given its id, with the setting of the X-IBM-Client-Id in the HTTP header. Cool! we get the good answer. Modify consumer The webapp we developed for the demonstration, needs to be update to integrate the HTTP header as below: var buildOptions=function(met,aPath,config){ return { url: config.customerAPI.url+aPath, method: met, rejectUnauthorized: true, headers: { accept: 'application/json', 'Content-Type': 'application/json', 'X-IBM-Client-Id': config.customerAPI.xibmclientid, Host: config.customerAPI.host, } } } getCustomerByEmail : function(config,req,res){ var opts = buildOptions('GET','/customers/email/'+req.params.email,config); opts.headers['Content-Type']='multipart/form-data'; processRequest(res,opts); } and in the configuration file, we need to change the URL and add the new parameter: \"customerAPI\":{ \"url\":\"https://172.16.50.8:443/csplab/sb\", \"host\":\"customer.green.case\", \"xibmclientid\": \"d6ef6d26-f017-42e6-b703-1d8aecfe1834\" } Remark: the host attribute is still needed if you do not have DNS resolution in the network where ICP, API gateway are running. Further readings Tutorial on the garage method https://cloudcontent.mybluemix.net/cloud/garage/demo/try-api-connect Brown compute specific API management best practices: https://github.com/ibm-cloud-architecture/refarch-integration-api#implementation-details","title":"API management"},{"location":"apim/#defining-customer-api-product","text":"We are detailing how the customer API product was developed to integrate with the REST api back end. This is not a tutorial, this is more for education purpose.","title":"Defining customer API product"},{"location":"apim/#pre-requisite","text":"You need to have access to IBM API Connect management product. We have one instance within our on-premise server, soon we will use one on ICP. You need to access the swagger file from the service project .","title":"Pre-requisite"},{"location":"apim/#development-steps-done","text":"","title":"Development steps done"},{"location":"apim/#step-1-create-product-from-swagger","text":"Connect to the API manager, our url is 172.16.40.88/apim. From the main page click Add and select Import API from file . Select the swagger src/main/webapp/META-INF/stub/swagger.yaml from the integation-service project: this should create a project customer-management-micro-service-api. The Paths section of the definition includes the CRUD operation on the customers resource: You do not need to use all the operations of the imported swagger, and it is possible to delete operations that we do not want to expose to external client applications. Instead of using an already defined swagger you can develop the API definition manually in the API Designer tool and the following product documentation gives you some how to do it. https://www.ibm.com/support/knowledgecenter/en/SSMNED_5.0.0/com.ibm.apic.toolkit.doc/task_APIonPrem_createapi.html Finally you can create a default product directly from the APIs: Products are just packaging for APIs and Plans.","title":"Step 1: Create product from swagger"},{"location":"apim/#step-2-define-operation-mapping","text":"You have two options here: taking the generic approach where all the defined APIs from the swagger are used as is in the new Product and the API gateway is just a proxy do some specific mappings, select a sub set of APIs and may be do some interface change. API management is using its own vocabulary for such implementation: Policies are the building blocks of assembly flows, and they provide the means to configure capability, such as security, logging, routing of requests to target services, and transformation of data from one format to another. Logic constructs behave in a similar way to policies, but they affect how and which parts of the assembly are implemented without modifying the data flow of the assembly. Proxy policy To implement a generic proxy approach, open the Assembly editor and create a new assembly: between the two circles drag and drop a Proxy policy: Then in the URL specify the end point of the virtual host name of the deployed micro service within ICP. The hostname was specified within the deployment configuration of the service when deploying it to ICP within the Ingress construct. yaml ingress: enabled: true # Used to create Ingress record (should used with service.type: ClusterIP). hosts: - customer.green.case The URL includes the webcontext and URL mapping as defined in the web.xml file of the java webapp. http://customer.green.case/caseserv/api/v1$(request.path) The variable request.path is the one defined in the Paths, for example can be /customers/{id} Attention!: there is something wrong here!. How API gateway that will run this product implementation will know how to map the customer.green.case host to the ICP proxy server responsible to route the traffic to the pod running the backend service? As this name is only known by ICP internal kubernetes DNS service. The solution is to define static host alias inside the API Gateway, or use a DNS server inside the same network. Our API datapower gateway is on the same IP network as our ICP cluster: 172.16. . . To add a host alias, connect to the API Gateway console using the port 9090. (https://172.16...:9090/console) as an admin user and using the Default domain so you can access the global settings. Then select network > interface > host alias to define a new entry. Once the configuration is saved you can use the ping remote host feature: Before going to the unit test let see the other approach: Map some operations For each operation, define the mapping to the back end operation. So go to the Assembly editor , and drag and drop the Operation switch logic operator: Add one case per operation you want to do a mapping to. We start by the three read only operations we need for the user interface to call: get all customer, get a customer given its id, and giving its email. The following diagram illustrates an Invoke policy to call the back end: Specify the URL as before. There is way to add scripts, mapping ... before and after the invoke if needed.","title":"Step 2: Define operation mapping"},{"location":"apim/#step-3-unit-test","text":"To unit test, you need first to republish the product Then select the operation to trigger, like /customers/{id}, set the parameter, id=3 and invoke the service You should get the data coming from the customer database.","title":"Step 3: unit test"},{"location":"apim/#step-4-define-and-deploy-product","text":"Some important concepts to keep in mind: products provide a method by which you can group APIs into a package that is intended for a particular use or channel. Products contain Plans, which can be used to differentiate between different offerings. To make an API available to an application developer, it must be included in a Plan. Products are then published in a Catalog. A lifecycle manager can then control the availability and visibility of APIs and Plans through the API Manager. The application developer can only subscribe to one Plan from a specific Product. In the product view, select the upload button on the right side to stage the product to a specific environment. Our catalog is called sandbox . You can see from the figure above we select a default plan. In production you will define a plan according to business requirements. Publish the product to the catalog, you should be able to see the new URL for the API endpoint: (e.g. https://172.16.50.8/csplab/sb/customers) Once deployed developers can access the API Portal to define the application the developer will implement and that subscribed to this product.","title":"Step 4: define and deploy product"},{"location":"apim/#step-5-define-application-for-developer","text":"Using the portal at a URL like: https://172.16..../csplab/sb you can access to the API portal for developers. Developers have an account, and need to register their application before using an API. When you register an application, the application is assigned a unique client ID and client secret. You must use the client ID when you call an API that requires you to identify your application by using a client ID.","title":"Step 5: Define application for developer"},{"location":"apim/#create-an-application","text":"In App menu, use Create an application button. Specify a name Then browse existing API products and select the customer API, then subscribe: Now you should have a clientID to be used in future request from the application. This client id needs to be part of the HTTP header for any request to the api. You should be able to do an integration test to validate the path API product deployed on gateway -> Java REST micro service on ICP -> DB2, using a tool like postman or SOAP UI. The following figure illustrates a GET request to the API URL to get a customer given its id, with the setting of the X-IBM-Client-Id in the HTTP header. Cool! we get the good answer.","title":"Create an application"},{"location":"apim/#modify-consumer","text":"The webapp we developed for the demonstration, needs to be update to integrate the HTTP header as below: var buildOptions=function(met,aPath,config){ return { url: config.customerAPI.url+aPath, method: met, rejectUnauthorized: true, headers: { accept: 'application/json', 'Content-Type': 'application/json', 'X-IBM-Client-Id': config.customerAPI.xibmclientid, Host: config.customerAPI.host, } } } getCustomerByEmail : function(config,req,res){ var opts = buildOptions('GET','/customers/email/'+req.params.email,config); opts.headers['Content-Type']='multipart/form-data'; processRequest(res,opts); } and in the configuration file, we need to change the URL and add the new parameter: \"customerAPI\":{ \"url\":\"https://172.16.50.8:443/csplab/sb\", \"host\":\"customer.green.case\", \"xibmclientid\": \"d6ef6d26-f017-42e6-b703-1d8aecfe1834\" } Remark: the host attribute is still needed if you do not have DNS resolution in the network where ICP, API gateway are running.","title":"Modify consumer"},{"location":"apim/#further-readings","text":"Tutorial on the garage method https://cloudcontent.mybluemix.net/cloud/garage/demo/try-api-connect Brown compute specific API management best practices: https://github.com/ibm-cloud-architecture/refarch-integration-api#implementation-details","title":"Further readings"},{"location":"data/","text":"Data Ingestion For data ingestion we have multiple choices to move data from DB2 to Db2 warehouse, Cloudant to Db2 warehouse. The appraoch will depend of the number of records to load, the frequency of data movement, and the overall size. This data movement is motivated by the Data Scientist work to select the best algorithm to classify customers into churn or not. For this solution we have used the following approach churn settings were done by marketing group on existing 3000 customer records. To develop in parallel the analytics model and database, microservice,... we used csv files to share the data, but in real life the data are in the DB2 CUSTDB instance, We stage the data to Db2 warehouse on ICP, and let the Data science uses this data source from his Jupyter notebook within DSX When the random forest model is done we assume the marketing will ask to do model evaluation on new added records every month. To move data we used the Db2 remote table feature to load the same data from Db2 and then do data preparation to customeraccount unique table used by the ML training and testing Loading the data using remote table Connect to the DB2 warehouse running on ICP. Normally is by using the proxy server with the mapped port number. For example our deployment is https://172.16.40.131:31107/console. (If you want to know more on how we configure Db2 warehouse helm release see this note )). Recall that to get the port number you can execute the command: kubectl get --namespace db2-warehouse services db2-gc-ibm-db2warehouse-prod Go to the console and choose Administer > remote tables . Since nothing had been defined yet it popped up and asked if you want to create a remote server. If something was already defined you would have to select \"Manage Servers\". Then you can \"Create\" and enter all the connection information. If you go back to the \"Nickname Explorer\" tab or \"Administer / Remote Tables\" then you can select \"Create\" to add a reference to a remote table. Select the data source type (Db2) and the data source they come from (UDB_1, as named previously). Optionally you can choose to \"test server\" if you want to make sure the connection works. Click \"Add Nicknames\" and select the items from the list. Check the boxes and select \"Update Local Schema\" to change the schema name (in case there might be collisions - we chose BROWN). Now those three tables are available for local questions as BROWN.ACCOUNTS, BROWN.CUSTOMERS, and BROWN.PRODUCTS You can now create a view, called BROWN.CUSTOMERACCOUNT which has no data in it but uses the remote tables created earlier. CREATE VIEW BROWN.CUSTOMERACCOUNT AS ( select id, name, firstname, lastname, emailaddress, age, gender, type, status, children, estimatedincome, carowner, profession, churn, churnrisk, zipcode, maritalstatus, mostdominanttone, accountnumber, longdistance, longdistancebilltype, international, local, balance, usage, dropped, paymentmethod, localbilltype, rateplan from brown.customers c, brown.accounts a where c.account_accountnumber = a.accountnumber ); From this view create the new table that will be used by the data scientist and a Spark jobs. CREATE TABLE BLUADMIN.CUSTOMERACCOUNT LIKE BROWN.CUSTOMERACCOUNT; INSERT INTO BLUADMIN.CUSTOMERACCOUNT select id, name, firstname, lastname, emailaddress, age, gender, type, status, children, estimatedincome, carowner, profession, churn, churnrisk, zipcode, maritalstatus, mostdominanttone, accountnumber, longdistance, longdistancebilltype, international, local, balance, usage, dropped, paymentmethod, localbilltype, rateplan from brown.customers c, brown.accounts a where c.account_accountnumber = a.accountnumber ; Please note that if a large number of rows was involved this insert could easily be too big to do like this. Compendium As we used remote table in this solution you can deep dive into federated systems from the product documentation: https://www.ibm.com/support/knowledgecenter/SS6NHC/com.ibm.data.fluidquery.doc/topics/cfpint01.html","title":"Data ingestion"},{"location":"data/#data-ingestion","text":"For data ingestion we have multiple choices to move data from DB2 to Db2 warehouse, Cloudant to Db2 warehouse. The appraoch will depend of the number of records to load, the frequency of data movement, and the overall size. This data movement is motivated by the Data Scientist work to select the best algorithm to classify customers into churn or not. For this solution we have used the following approach churn settings were done by marketing group on existing 3000 customer records. To develop in parallel the analytics model and database, microservice,... we used csv files to share the data, but in real life the data are in the DB2 CUSTDB instance, We stage the data to Db2 warehouse on ICP, and let the Data science uses this data source from his Jupyter notebook within DSX When the random forest model is done we assume the marketing will ask to do model evaluation on new added records every month. To move data we used the Db2 remote table feature to load the same data from Db2 and then do data preparation to customeraccount unique table used by the ML training and testing","title":"Data Ingestion"},{"location":"data/#loading-the-data-using-remote-table","text":"Connect to the DB2 warehouse running on ICP. Normally is by using the proxy server with the mapped port number. For example our deployment is https://172.16.40.131:31107/console. (If you want to know more on how we configure Db2 warehouse helm release see this note )). Recall that to get the port number you can execute the command: kubectl get --namespace db2-warehouse services db2-gc-ibm-db2warehouse-prod Go to the console and choose Administer > remote tables . Since nothing had been defined yet it popped up and asked if you want to create a remote server. If something was already defined you would have to select \"Manage Servers\". Then you can \"Create\" and enter all the connection information. If you go back to the \"Nickname Explorer\" tab or \"Administer / Remote Tables\" then you can select \"Create\" to add a reference to a remote table. Select the data source type (Db2) and the data source they come from (UDB_1, as named previously). Optionally you can choose to \"test server\" if you want to make sure the connection works. Click \"Add Nicknames\" and select the items from the list. Check the boxes and select \"Update Local Schema\" to change the schema name (in case there might be collisions - we chose BROWN). Now those three tables are available for local questions as BROWN.ACCOUNTS, BROWN.CUSTOMERS, and BROWN.PRODUCTS You can now create a view, called BROWN.CUSTOMERACCOUNT which has no data in it but uses the remote tables created earlier. CREATE VIEW BROWN.CUSTOMERACCOUNT AS ( select id, name, firstname, lastname, emailaddress, age, gender, type, status, children, estimatedincome, carowner, profession, churn, churnrisk, zipcode, maritalstatus, mostdominanttone, accountnumber, longdistance, longdistancebilltype, international, local, balance, usage, dropped, paymentmethod, localbilltype, rateplan from brown.customers c, brown.accounts a where c.account_accountnumber = a.accountnumber ); From this view create the new table that will be used by the data scientist and a Spark jobs. CREATE TABLE BLUADMIN.CUSTOMERACCOUNT LIKE BROWN.CUSTOMERACCOUNT; INSERT INTO BLUADMIN.CUSTOMERACCOUNT select id, name, firstname, lastname, emailaddress, age, gender, type, status, children, estimatedincome, carowner, profession, churn, churnrisk, zipcode, maritalstatus, mostdominanttone, accountnumber, longdistance, longdistancebilltype, international, local, balance, usage, dropped, paymentmethod, localbilltype, rateplan from brown.customers c, brown.accounts a where c.account_accountnumber = a.accountnumber ; Please note that if a large number of rows was involved this insert could easily be too big to do like this.","title":"Loading the data using remote table"},{"location":"data/#compendium","text":"As we used remote table in this solution you can deep dive into federated systems from the product documentation: https://www.ibm.com/support/knowledgecenter/SS6NHC/com.ibm.data.fluidquery.doc/topics/cfpint01.html","title":"Compendium"},{"location":"deploy/icp/","text":"Deploy the solution for IBM Cloud Private","title":"Private ICP"},{"location":"deploy/icp/#deploy-the-solution-for-ibm-cloud-private","text":"","title":"Deploy the solution for IBM Cloud Private"},{"location":"deploy/iks/","text":"Deploy the solution for IBM Cloud Kubernetes Service","title":"Public IKS"},{"location":"deploy/iks/#deploy-the-solution-for-ibm-cloud-kubernetes-service","text":"","title":"Deploy the solution for IBM Cloud Kubernetes Service"},{"location":"deploy/local/","text":"Build and running local This chapter addresses how to build and run the different component of the application. The approach is to use the cognitive services on IBM Cloud and docker-compose to run the difference service on your local laptop: pre-requisites For this application you need to have nodejs installed on your computer with the npm installer tool. You need a local docker engine You need docker-compose Configure DB2 customer tables Build You can run a unique command to build all the components of the solution, and build their docker image, or you can build one service at a time. Build the web app You need to install Angular command line interface if you do not have it yet: see the cli.angular.io website with the following command sudo npm install -g @angular/cli If you want to tune the server code, you need to install nodemon to support server code change without stopping the server. The installation is done with npm : sudo npm install -g nodemon In this project run the command: ./scripts/build.sh Build the customer service In the project refarch-integration-services use scripts/build.sh Run The solution runs with docker compose. Under the docker folder in this project do the following command: docker-compose up &","title":"Local"},{"location":"deploy/local/#build-and-running-local","text":"This chapter addresses how to build and run the different component of the application. The approach is to use the cognitive services on IBM Cloud and docker-compose to run the difference service on your local laptop:","title":"Build and running local"},{"location":"deploy/local/#pre-requisites","text":"For this application you need to have nodejs installed on your computer with the npm installer tool. You need a local docker engine You need docker-compose","title":"pre-requisites"},{"location":"deploy/local/#configure-db2-customer-tables","text":"","title":"Configure DB2 customer tables"},{"location":"deploy/local/#build","text":"You can run a unique command to build all the components of the solution, and build their docker image, or you can build one service at a time.","title":"Build"},{"location":"deploy/local/#build-the-web-app","text":"You need to install Angular command line interface if you do not have it yet: see the cli.angular.io website with the following command sudo npm install -g @angular/cli If you want to tune the server code, you need to install nodemon to support server code change without stopping the server. The installation is done with npm : sudo npm install -g nodemon In this project run the command: ./scripts/build.sh","title":"Build the web app"},{"location":"deploy/local/#build-the-customer-service","text":"In the project refarch-integration-services use scripts/build.sh","title":"Build the customer service"},{"location":"deploy/local/#run","text":"The solution runs with docker compose. Under the docker folder in this project do the following command: docker-compose up &","title":"Run"},{"location":"design/code/","text":"Implementation details In this chapter we are addressing the Angular 5 implementation and the Backend For Frontend as a nodejs app used to implement authentication, service orchestration, data mapping and to serve the Angular App. The BFF is also using Hystrix JS for circuit breaker and fault tolerance. Update 08/2108 - Author Jerome Boyer Web Application Code explanation Most of the end user's interactions are supported by Angular 5 single page javascript library, with its router mechanism and the DOM rendering capabilities via directives and components. When there is a need to access data to the on-premise server for persistence, an AJAX call is done to server, and the server will respond asynchronously later on. The components involved are presented in the figure below in a generic way: From an implementation point of view we are interested by the router, the controller and the services. To clearly separate the codebase for front-end and back-end the src/client folder includes Angular 5 code while src/server folder includes the REST api and BFF implemented with expressjs. Angular app The application code follows the standard best practices for Angularjs development: unique index.html to support single page application use of modules to organize features use of component, html and css per feature page encapsulate calls to back end for front end server via service components. We recommend Angular beginners to follow the product \"tour of heroes\" tutorial . We also recommend to read our last work on Angular 5 app using a test driven development approach in this project . Main Components As traditional Angular 5 app, you need: a main.ts script to declare and bootstrap your application. a app.module.ts to declare all the components of the application and the URL routes declaration. Those routes are internal to the web browser. They are protected by a guard mechanism to avoid unlogged person to access some private pages. The following code declares four routes for the four main features of this application: display the main top navigation page, the customer page to access account, and the itSupport to access the chat bot user interface. The AuthGard component assesses if the user is known and logged, if not he/she is routed to the login page. const routes: Routes = [ { path: 'home', component: HomeComponent,canActivate: [AuthGuard]}, { path: 'log', component: LoginComponent }, //canActivate: [AuthGuard] { path: 'itSupport', component: ConversationComponent,canActivate: [AuthGuard]}, { path: 'customer', component: CustomersComponent,canActivate: [AuthGuard]}, // otherwise redirect to home { path: '**', redirectTo: 'home' } ] * an app.component to support the main page template where routing is done. This component has the header and footer of the HTML page and the placeholder directly to support sub page routing: <router-outlet></router-outlet> Home page The home page is just a front end to navigate to the different features. It persists the user information in a local storage and uses the Angular router capability to map widget button action to method and route. For example the following HTML page uses angular construct to link the button to the itSupport() method of the Home.component.ts <div class=\"col-md-6 roundRect\" style=\"box-shadow: 3px 3px 1px #05870b; border-color: #05870b;\"> <h2>Support Help</h2> <p>Get help</p> <p><button (click)=\"itSupport()\" class=\"btn btn-primary\">Ask me</button></p> </div> the method delegates to the angular router with the 'itSupport' url itSupport(){ this.router.navigate(['itSupport']); } Conversation bot For the conversation front end we are re-using the code approach of the conversation broker of the Cognitive reference architecture implementation . The same approach, service and component are used to control the user interface and to call the back end. The service does an HTTP POST of the newly entered message: export class ConversationService { private convUrl ='/api/c/conversation/'; constructor(private http: Http) { }; submitMessage(msg:string,ctx:any): Observable<any>{ let user = JSON.parse(sessionStorage.getItem('currentUser')); let bodyString = JSON.stringify( { text:msg,context:ctx,user:user }); let headers = new Headers({ 'Content-Type': 'application/json' }); let options = new RequestOptions({ headers: headers }) return this.http.post(this.convUrl,bodyString,options) .map((res:Response) => res.json()) } } So it is interesting to see the message as the watson conversation context and the user basic information. Account component When the user selects to access the account information, the routing is going to the account component in client/app/account folder use a service to call the nodejs / expressjs REST services as illustrated in the code below: export class CustomerService { private invUrl ='/api/c'; constructor(private http: Http) { }; getItems(): Observable<any>{ return this.http.get(this.invUrl+'/customer') .map((res:Response) => res.json()) } } The http component is injected at service creation, and the promise returned object is map so the response can be processed as json document. An example of code using those service is the account.component.ts , which loads the account during component initialization phase. export class AccountComponent implements OnInit { constructor(private router: Router, private cService : CustomerService){ } // Uses in init to load data and not the constructor. ngOnInit(): void { this.user = JSON.parse(localStorage.getItem('currentUser')); if(this.user && 'email' in this.user) { cService.getCustomerByEmail(this.user.email).subscribe( data => { this.customer=data; }, error => { console.log(error); }); } } } Server code The application is using nodejs and expressjs standard code structure. The code is under server folder. Conversation back end The script is in server/route/features/chatBot.js and uses the Watson developer cloud library to connect to the remote service. This library encapsulates HTTP calls and simplifies the interactions with the public service. The only thing that needs to be done for each chat bot is to add the logic to process the response, for example to get data from a backend, presents user choices in a form of buttons, or call remote service like a rule engine / decision service. This module exports one function to be called by the API used by the front end. This API is defined in api.js as: app.post('/api/c/conversation',isLoggedIn,(req,res) => { chatBot.chat(config,req,res) }); The chatBot.chat() method gets the message and connection parameters and uses the Watson API to transfer the call. The set of if statements are used to perform actions, call services, using the variables set in the Watson Conversation Context. One example is to use the Operational Decision Management rule engine to compute the best product for a given customer situation. chat : function(config,req,res){ req.body.context.predefinedResponses=\"\"; console.log(\"text \"+req.body.text+\".\") if (req.body.context.toneAnalyzer && req.body.text !== \"\" ) { analyzeTone(config,req,res) } if (req.body.context.action === \"search\" && req.body.context.item ===\"UserRequests\") { getSupportTicket(config,req,res); } if (req.body.context.action === \"recommend\") { odmclient.recommend(config,req.body.context,res, function(contextWithRecommendation){ req.body.context = contextWithRecommendation; sendToWCSAndBackToUser(config,req,res); }); } if (req.body.context.action === \"transfer\") { console.log(\"Transfer to \"+ req.body.context.item) } if (req.body.context.action === undefined) { sendToWCSAndBackToUser(config,req,res); } } // chat The send message uses the Watson developer library: conversation = watson.conversation({ username: config.conversation.username, password: config.conversation.password, version: config.conversation.version, version_date: config.conversation.versionDate}); conversation.message( { workspace_id: wkid, input: {'text': message.text}, context: message.context }, function(err, response) { // add logic here to process the conversation response } ) It uses content of the conversation context to drive some of the routing mechanism. This code supports the following sequencing: As the user is enquiring about an existing ticket support, the conversation set the action variable to \"search\", and return a message in \"A\" that the system is searching for existing records. The web interface send back an empty message on behave of the user so the flow can continue. If the conversation context has a variable action set to \"search\", it calls the corresponding backend to get other data. Like a ticket management app. We did not implement the ticket management app, but just a mockup. javascript if (req.body.context.action === \"search\" && req.body.context.item ===\"UserRequests\") { ticketing.getUserTicket(config,req.body.user.email,function(ticket){ if (config.debug) { console.log('Ticket response: ' + JSON.stringify(ticket)); } req.body.context[\"Ticket\"]=ticket sendToWCSAndBackToUser(config,req,res); })} The ticket information is returned to the conversation directly and the message response is built there. if the action is \"recommend\", the code can call a decision service deployed on IBM Cloud and execute business rules to compute the best recommendations/ actions. See example of such approach in the project \"ODM and Watson conversation\" If in the conversation context the boolean toneAnalyzer is set to true, then any new sentence sent by the end user will be sent to Watson Tone Analyzer. if (req.body.context.toneAnalyzer && req.body.text !== \"\" ) { analyzeTone(config,req,res) } When the result to the tone analyzer returns a tone as Sad or Frustrated then a call to a churn scoring service is performed. function analyzeTone(config,req,res){ toneAnalyzer.analyzeSentence(config,req.body.text).then(function(toneArep) { if (config.debug) {console.log('Tone Analyzer '+ JSON.stringify(toneArep));} req.body.context[\"ToneAnalysisResponse\"]=toneArep.utterances_tone[0].tones[0]; if (req.body.context[\"ToneAnalysisResponse\"].tone_name === \"Frustrated\") { churnScoring.scoreCustomer(config,req,function(score){ req.body.context[\"ChurnScore\"]=score; sendToWCSAndBackToUser(config,req,res); }) } }).catch(function(error){ console.error(error); res.status(500).send({'msg':error.Error}); }); } // analyzeTone when the churn score is greater than a value the call is routed to a human. This is done in the conversation dialog and the context action is set to Transfer if (req.body.context.action === \"transfer\") { console.log(\"Transfer to \"+ req.body.context.item) } See also how the IBM Watson conversation is built to support this logic, in this note. Finally this code can persist the conversation to a remote document oriented database. The code is in persist.js and a complete detailed explanation to setup this service is in this note. Customer back end The customer API is defined in the server/routes/feature folder and uses the request and hystrix libraries to perform the call to the customer micro service API. The config.json file specifies the end point URL. The Hystrixjs is interesting to use to protect the remote call with timeout, circuit breaker, fails quickly.... modern pattern to support resiliency and fault tolerance. var run = function(config,email){ return new Promise(function(resolve, reject){ var opts = buildOptions('GET','/customers/email/'+email,config); opts.headers['Content-Type']='multipart/form-data'; request(opts,function (error, response, body) { if (error) {reject(error)} resolve(body); }); }); } // times out calls that take longer, than the configured threshold. var serviceCommand =CommandsFactory.getOrCreate(\"getCustomerDetail\") .run(run) .timeout(5000) .requestVolumeRejectionThreshold(2) .build(); getCustomerDetail : function(config,email) { return serviceCommand.execute(config,email); } Churn risk scoring The scoring is done by deploying a trained model as a service. We have two clients, one for Watson Data Platform and one for Spark cluster on ICP. The interface is the same so it is easy to change implementation. Helm chart We created a new helm chart for this application with helm create greencompute-telco-app . Then we update the following: Add a configMap.yaml file under the templates to defined the parameters used to configure the service end point metadata used by the BFF code. Add a volume in the deployment.yaml to use the parameters from the configMap. and mount this volume into the file. And modify the port mapping spec: volumes: - name: config configMap: name: {{ template \"greencompute-telco-app.fullname\" . }} containers: - name: {{ .Chart.Name }} image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\" imagePullPolicy: {{ .Values.image.pullPolicy }} ports: - containerPort: {{ .Values.service.internalPort }} volumeMounts: - name: config mountPath: /greenapp/server/config/config.json subPath: config.json In values.yaml set the docker image name Modify the value to use service.port to be externalPort and set it to match the one exposed in dockerfile: service: type: ClusterIP externalPort: 3001 internalPort: 3001 enable ingress and set a hostname (telcoapp.green.case) In the ingress use the servicePort to map the externalPort. {{- $servicePort := .Values.service.externalPort -}} In the service.yaml be sure to define the ports well using the one set in values.yaml ports: - port: {{ .Values.service.externalPort }} targetPort: {{ .Values.service.internalPort }} protocol: TCP name: {{ .Values.service.name }} ICP deployment For this web application we are following the same steps introduced within the Brown Case Web app application and can be summarized as: Compile the app: ng build Create docker images: docker build -t ibmcase/greencompute-telco-app . We are now using public dockerhub so just we are doing a docker push ibmcase/greencompute-telco-app When deploying to a private registry as the one internal to ICP, tag the image with the docker repository name, version: docker tag ibmcase/greencompute-telco-app greencluster.icp:8500/greencompute/greencompute-telco-app , then push the docker images to the docker repository running on the Master node of ICP: $ docker login greencluster.icp:8500 $ docker push greencluster.icp:8500/greencompute/greenapp:v0.0.2 Be sure to be connected to the kubernetes server with commands like: bx pr login -u admin -a https://greencluster.icp:8443 --skip-ssl-validation bx pr cluster-config greencluster.icp Install the Helm release with the greencompute namespace: helm install greencompute-telco-app/ --name green-telco-app --namespace greencompute Be sure you have name resolution from the hostname you set in values.yaml and IP address of the ICP proxy. Use your local '/etc/hosts' file for that. In production, set your local DNS with this name resolution. Test by accessing the URL: http://http://greenapp.green.case/ Customer Microservice The back end customer management function is a micro service in its separate repository, and the code implementation explanation can be read here. More readings Angular io Hystrixjs the latency an fault tolerance library Javascript Promise chaining article [Case Portal app using Angular 5 app using a test driven development approach in this project","title":"Implementation details"},{"location":"design/code/#implementation-details","text":"In this chapter we are addressing the Angular 5 implementation and the Backend For Frontend as a nodejs app used to implement authentication, service orchestration, data mapping and to serve the Angular App. The BFF is also using Hystrix JS for circuit breaker and fault tolerance. Update 08/2108 - Author Jerome Boyer","title":"Implementation details"},{"location":"design/code/#web-application","text":"","title":"Web Application"},{"location":"design/code/#code-explanation","text":"Most of the end user's interactions are supported by Angular 5 single page javascript library, with its router mechanism and the DOM rendering capabilities via directives and components. When there is a need to access data to the on-premise server for persistence, an AJAX call is done to server, and the server will respond asynchronously later on. The components involved are presented in the figure below in a generic way: From an implementation point of view we are interested by the router, the controller and the services. To clearly separate the codebase for front-end and back-end the src/client folder includes Angular 5 code while src/server folder includes the REST api and BFF implemented with expressjs.","title":"Code explanation"},{"location":"design/code/#angular-app","text":"The application code follows the standard best practices for Angularjs development: unique index.html to support single page application use of modules to organize features use of component, html and css per feature page encapsulate calls to back end for front end server via service components. We recommend Angular beginners to follow the product \"tour of heroes\" tutorial . We also recommend to read our last work on Angular 5 app using a test driven development approach in this project .","title":"Angular app"},{"location":"design/code/#main-components","text":"As traditional Angular 5 app, you need: a main.ts script to declare and bootstrap your application. a app.module.ts to declare all the components of the application and the URL routes declaration. Those routes are internal to the web browser. They are protected by a guard mechanism to avoid unlogged person to access some private pages. The following code declares four routes for the four main features of this application: display the main top navigation page, the customer page to access account, and the itSupport to access the chat bot user interface. The AuthGard component assesses if the user is known and logged, if not he/she is routed to the login page. const routes: Routes = [ { path: 'home', component: HomeComponent,canActivate: [AuthGuard]}, { path: 'log', component: LoginComponent }, //canActivate: [AuthGuard] { path: 'itSupport', component: ConversationComponent,canActivate: [AuthGuard]}, { path: 'customer', component: CustomersComponent,canActivate: [AuthGuard]}, // otherwise redirect to home { path: '**', redirectTo: 'home' } ] * an app.component to support the main page template where routing is done. This component has the header and footer of the HTML page and the placeholder directly to support sub page routing: <router-outlet></router-outlet>","title":"Main Components"},{"location":"design/code/#home-page","text":"The home page is just a front end to navigate to the different features. It persists the user information in a local storage and uses the Angular router capability to map widget button action to method and route. For example the following HTML page uses angular construct to link the button to the itSupport() method of the Home.component.ts <div class=\"col-md-6 roundRect\" style=\"box-shadow: 3px 3px 1px #05870b; border-color: #05870b;\"> <h2>Support Help</h2> <p>Get help</p> <p><button (click)=\"itSupport()\" class=\"btn btn-primary\">Ask me</button></p> </div> the method delegates to the angular router with the 'itSupport' url itSupport(){ this.router.navigate(['itSupport']); }","title":"Home page"},{"location":"design/code/#conversation-bot","text":"For the conversation front end we are re-using the code approach of the conversation broker of the Cognitive reference architecture implementation . The same approach, service and component are used to control the user interface and to call the back end. The service does an HTTP POST of the newly entered message: export class ConversationService { private convUrl ='/api/c/conversation/'; constructor(private http: Http) { }; submitMessage(msg:string,ctx:any): Observable<any>{ let user = JSON.parse(sessionStorage.getItem('currentUser')); let bodyString = JSON.stringify( { text:msg,context:ctx,user:user }); let headers = new Headers({ 'Content-Type': 'application/json' }); let options = new RequestOptions({ headers: headers }) return this.http.post(this.convUrl,bodyString,options) .map((res:Response) => res.json()) } } So it is interesting to see the message as the watson conversation context and the user basic information.","title":"Conversation bot"},{"location":"design/code/#account-component","text":"When the user selects to access the account information, the routing is going to the account component in client/app/account folder use a service to call the nodejs / expressjs REST services as illustrated in the code below: export class CustomerService { private invUrl ='/api/c'; constructor(private http: Http) { }; getItems(): Observable<any>{ return this.http.get(this.invUrl+'/customer') .map((res:Response) => res.json()) } } The http component is injected at service creation, and the promise returned object is map so the response can be processed as json document. An example of code using those service is the account.component.ts , which loads the account during component initialization phase. export class AccountComponent implements OnInit { constructor(private router: Router, private cService : CustomerService){ } // Uses in init to load data and not the constructor. ngOnInit(): void { this.user = JSON.parse(localStorage.getItem('currentUser')); if(this.user && 'email' in this.user) { cService.getCustomerByEmail(this.user.email).subscribe( data => { this.customer=data; }, error => { console.log(error); }); } } }","title":"Account component"},{"location":"design/code/#server-code","text":"The application is using nodejs and expressjs standard code structure. The code is under server folder.","title":"Server code"},{"location":"design/code/#conversation-back-end","text":"The script is in server/route/features/chatBot.js and uses the Watson developer cloud library to connect to the remote service. This library encapsulates HTTP calls and simplifies the interactions with the public service. The only thing that needs to be done for each chat bot is to add the logic to process the response, for example to get data from a backend, presents user choices in a form of buttons, or call remote service like a rule engine / decision service. This module exports one function to be called by the API used by the front end. This API is defined in api.js as: app.post('/api/c/conversation',isLoggedIn,(req,res) => { chatBot.chat(config,req,res) }); The chatBot.chat() method gets the message and connection parameters and uses the Watson API to transfer the call. The set of if statements are used to perform actions, call services, using the variables set in the Watson Conversation Context. One example is to use the Operational Decision Management rule engine to compute the best product for a given customer situation. chat : function(config,req,res){ req.body.context.predefinedResponses=\"\"; console.log(\"text \"+req.body.text+\".\") if (req.body.context.toneAnalyzer && req.body.text !== \"\" ) { analyzeTone(config,req,res) } if (req.body.context.action === \"search\" && req.body.context.item ===\"UserRequests\") { getSupportTicket(config,req,res); } if (req.body.context.action === \"recommend\") { odmclient.recommend(config,req.body.context,res, function(contextWithRecommendation){ req.body.context = contextWithRecommendation; sendToWCSAndBackToUser(config,req,res); }); } if (req.body.context.action === \"transfer\") { console.log(\"Transfer to \"+ req.body.context.item) } if (req.body.context.action === undefined) { sendToWCSAndBackToUser(config,req,res); } } // chat The send message uses the Watson developer library: conversation = watson.conversation({ username: config.conversation.username, password: config.conversation.password, version: config.conversation.version, version_date: config.conversation.versionDate}); conversation.message( { workspace_id: wkid, input: {'text': message.text}, context: message.context }, function(err, response) { // add logic here to process the conversation response } ) It uses content of the conversation context to drive some of the routing mechanism. This code supports the following sequencing: As the user is enquiring about an existing ticket support, the conversation set the action variable to \"search\", and return a message in \"A\" that the system is searching for existing records. The web interface send back an empty message on behave of the user so the flow can continue. If the conversation context has a variable action set to \"search\", it calls the corresponding backend to get other data. Like a ticket management app. We did not implement the ticket management app, but just a mockup. javascript if (req.body.context.action === \"search\" && req.body.context.item ===\"UserRequests\") { ticketing.getUserTicket(config,req.body.user.email,function(ticket){ if (config.debug) { console.log('Ticket response: ' + JSON.stringify(ticket)); } req.body.context[\"Ticket\"]=ticket sendToWCSAndBackToUser(config,req,res); })} The ticket information is returned to the conversation directly and the message response is built there. if the action is \"recommend\", the code can call a decision service deployed on IBM Cloud and execute business rules to compute the best recommendations/ actions. See example of such approach in the project \"ODM and Watson conversation\" If in the conversation context the boolean toneAnalyzer is set to true, then any new sentence sent by the end user will be sent to Watson Tone Analyzer. if (req.body.context.toneAnalyzer && req.body.text !== \"\" ) { analyzeTone(config,req,res) } When the result to the tone analyzer returns a tone as Sad or Frustrated then a call to a churn scoring service is performed. function analyzeTone(config,req,res){ toneAnalyzer.analyzeSentence(config,req.body.text).then(function(toneArep) { if (config.debug) {console.log('Tone Analyzer '+ JSON.stringify(toneArep));} req.body.context[\"ToneAnalysisResponse\"]=toneArep.utterances_tone[0].tones[0]; if (req.body.context[\"ToneAnalysisResponse\"].tone_name === \"Frustrated\") { churnScoring.scoreCustomer(config,req,function(score){ req.body.context[\"ChurnScore\"]=score; sendToWCSAndBackToUser(config,req,res); }) } }).catch(function(error){ console.error(error); res.status(500).send({'msg':error.Error}); }); } // analyzeTone when the churn score is greater than a value the call is routed to a human. This is done in the conversation dialog and the context action is set to Transfer if (req.body.context.action === \"transfer\") { console.log(\"Transfer to \"+ req.body.context.item) } See also how the IBM Watson conversation is built to support this logic, in this note. Finally this code can persist the conversation to a remote document oriented database. The code is in persist.js and a complete detailed explanation to setup this service is in this note.","title":"Conversation back end"},{"location":"design/code/#customer-back-end","text":"The customer API is defined in the server/routes/feature folder and uses the request and hystrix libraries to perform the call to the customer micro service API. The config.json file specifies the end point URL. The Hystrixjs is interesting to use to protect the remote call with timeout, circuit breaker, fails quickly.... modern pattern to support resiliency and fault tolerance. var run = function(config,email){ return new Promise(function(resolve, reject){ var opts = buildOptions('GET','/customers/email/'+email,config); opts.headers['Content-Type']='multipart/form-data'; request(opts,function (error, response, body) { if (error) {reject(error)} resolve(body); }); }); } // times out calls that take longer, than the configured threshold. var serviceCommand =CommandsFactory.getOrCreate(\"getCustomerDetail\") .run(run) .timeout(5000) .requestVolumeRejectionThreshold(2) .build(); getCustomerDetail : function(config,email) { return serviceCommand.execute(config,email); }","title":"Customer back end"},{"location":"design/code/#churn-risk-scoring","text":"The scoring is done by deploying a trained model as a service. We have two clients, one for Watson Data Platform and one for Spark cluster on ICP. The interface is the same so it is easy to change implementation.","title":"Churn risk scoring"},{"location":"design/code/#helm-chart","text":"We created a new helm chart for this application with helm create greencompute-telco-app . Then we update the following: Add a configMap.yaml file under the templates to defined the parameters used to configure the service end point metadata used by the BFF code. Add a volume in the deployment.yaml to use the parameters from the configMap. and mount this volume into the file. And modify the port mapping spec: volumes: - name: config configMap: name: {{ template \"greencompute-telco-app.fullname\" . }} containers: - name: {{ .Chart.Name }} image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\" imagePullPolicy: {{ .Values.image.pullPolicy }} ports: - containerPort: {{ .Values.service.internalPort }} volumeMounts: - name: config mountPath: /greenapp/server/config/config.json subPath: config.json In values.yaml set the docker image name Modify the value to use service.port to be externalPort and set it to match the one exposed in dockerfile: service: type: ClusterIP externalPort: 3001 internalPort: 3001 enable ingress and set a hostname (telcoapp.green.case) In the ingress use the servicePort to map the externalPort. {{- $servicePort := .Values.service.externalPort -}} In the service.yaml be sure to define the ports well using the one set in values.yaml ports: - port: {{ .Values.service.externalPort }} targetPort: {{ .Values.service.internalPort }} protocol: TCP name: {{ .Values.service.name }}","title":"Helm chart"},{"location":"design/code/#icp-deployment","text":"For this web application we are following the same steps introduced within the Brown Case Web app application and can be summarized as: Compile the app: ng build Create docker images: docker build -t ibmcase/greencompute-telco-app . We are now using public dockerhub so just we are doing a docker push ibmcase/greencompute-telco-app When deploying to a private registry as the one internal to ICP, tag the image with the docker repository name, version: docker tag ibmcase/greencompute-telco-app greencluster.icp:8500/greencompute/greencompute-telco-app , then push the docker images to the docker repository running on the Master node of ICP: $ docker login greencluster.icp:8500 $ docker push greencluster.icp:8500/greencompute/greenapp:v0.0.2 Be sure to be connected to the kubernetes server with commands like: bx pr login -u admin -a https://greencluster.icp:8443 --skip-ssl-validation bx pr cluster-config greencluster.icp Install the Helm release with the greencompute namespace: helm install greencompute-telco-app/ --name green-telco-app --namespace greencompute Be sure you have name resolution from the hostname you set in values.yaml and IP address of the ICP proxy. Use your local '/etc/hosts' file for that. In production, set your local DNS with this name resolution. Test by accessing the URL: http://http://greenapp.green.case/","title":"ICP deployment"},{"location":"design/code/#customer-microservice","text":"The back end customer management function is a micro service in its separate repository, and the code implementation explanation can be read here.","title":"Customer Microservice"},{"location":"design/code/#more-readings","text":"Angular io Hystrixjs the latency an fault tolerance library Javascript Promise chaining article [Case Portal app using Angular 5 app using a test driven development approach in this project","title":"More readings"},{"location":"design/readme/","text":"Design considerations System Context diagram The following diagram illustrates the system context of the application, including analytics model preparation and run time execution. This repository presents best practices to deploy such solution on public and private cloud, implements the webapp deployable in public or private cloud, and deliver example of data sets. Components From above figure left to right the components involved are: Web application to offer a set of services for the end user to access: from this user interface the end user, customer of Green Telco, can access his account, pays his bill (not implemented) and uses the chat bot user interface to get support help. This note presents the implementation details and how to deploy it on ICP. The account informations are loaded from the back end systems (7) via a customer management micro service (8) and an API product defined (9) in API Connect. The call in (2) is RESTful API and documented in this note The chat bot is implemented with Watson Assistant . The workspace is delivered for you to upload to your own Watson Assistant cloud service you may have created, this note go into the implementation detail. Conversation sentences may be analyzed for tone analysis , and natural language understanding, those data are used by the scoring service. The service creation in IBM Cloud and the integration into the application flow is explained in this note The conversation transcripts are persisted in a document oriented database. We discuss about its implementation with Cloudant service on IBM Cloud in this technical note. A scoring service to assess current risk of churn for the customer interacting with Green Telco services. This is a runtime analytics service using customer data and results from the Tone Analysis. This note goes over the detail of the deployment and implementation of this machine learning based service. It can be deployed on IBM Cloud public Watson Machine learning or IBM Cloud Private. The customer data are persisted in on-premise server with relational database. We are using DB2 on-premise server for that. To read how the database was created see the note in this repository Customer data are exposed via a micro service approach. The implementation is done in a separate repository: the Customer management micro-services . It supports the JAXRS implementation deployed in Liberty as Docker image and the DB2 schema for DB2 data base. API product can be defined on top of the customer management service to monitor API usage and perform API governance. The implementation is supported by IBM API Connect. Some explanation of the product development in this note At this stage the other components are more used at design time with the involvement of knowledge engineers, data analysts and data scientists. Data scientists use machine learning library and Jupiter notebook, R Studio or Zeppelin on top of Apache Spark in IBM Data Science Experience (DSX) to discover the model. We are documenting two different approaches: One based on Watson Data Platform running on IBM Cloud and described in this note with this jupyter notebook . One based on Private cloud using DSX and Db2 warehouse and another notebook . The data used by data scientists are persisted in Db2 warehouse. This note goes over the creation of the Db2 warehouse release within IBM Cloud private. Ingestion mechanism can move data, for chat transcripts and customer records to the DB2 warehouse. This process can run on demand when Data Scientists need new data to tune the model. It can be implemented with an ETL, with Java program, or using the Db2 Federation capability. This note explains what was done to move DB2 customer data to Db2 warehouse. Product recommendations based on Operational Decision Management rules The natural language understanding service is added to support most advanced language processing from the text entered but the end user: entity extraction, relationships, taxonomy, etc. Those elements could be used for scoring services. The language understanding can be fine-tuned by using terms and model defined in Watson Knowledge Studio . The following sequence diagram explains how the components interact with each others.","title":"Design considerations"},{"location":"design/readme/#design-considerations","text":"","title":"Design considerations"},{"location":"design/readme/#system-context-diagram","text":"The following diagram illustrates the system context of the application, including analytics model preparation and run time execution. This repository presents best practices to deploy such solution on public and private cloud, implements the webapp deployable in public or private cloud, and deliver example of data sets.","title":"System Context diagram"},{"location":"design/readme/#components","text":"From above figure left to right the components involved are: Web application to offer a set of services for the end user to access: from this user interface the end user, customer of Green Telco, can access his account, pays his bill (not implemented) and uses the chat bot user interface to get support help. This note presents the implementation details and how to deploy it on ICP. The account informations are loaded from the back end systems (7) via a customer management micro service (8) and an API product defined (9) in API Connect. The call in (2) is RESTful API and documented in this note The chat bot is implemented with Watson Assistant . The workspace is delivered for you to upload to your own Watson Assistant cloud service you may have created, this note go into the implementation detail. Conversation sentences may be analyzed for tone analysis , and natural language understanding, those data are used by the scoring service. The service creation in IBM Cloud and the integration into the application flow is explained in this note The conversation transcripts are persisted in a document oriented database. We discuss about its implementation with Cloudant service on IBM Cloud in this technical note. A scoring service to assess current risk of churn for the customer interacting with Green Telco services. This is a runtime analytics service using customer data and results from the Tone Analysis. This note goes over the detail of the deployment and implementation of this machine learning based service. It can be deployed on IBM Cloud public Watson Machine learning or IBM Cloud Private. The customer data are persisted in on-premise server with relational database. We are using DB2 on-premise server for that. To read how the database was created see the note in this repository Customer data are exposed via a micro service approach. The implementation is done in a separate repository: the Customer management micro-services . It supports the JAXRS implementation deployed in Liberty as Docker image and the DB2 schema for DB2 data base. API product can be defined on top of the customer management service to monitor API usage and perform API governance. The implementation is supported by IBM API Connect. Some explanation of the product development in this note At this stage the other components are more used at design time with the involvement of knowledge engineers, data analysts and data scientists. Data scientists use machine learning library and Jupiter notebook, R Studio or Zeppelin on top of Apache Spark in IBM Data Science Experience (DSX) to discover the model. We are documenting two different approaches: One based on Watson Data Platform running on IBM Cloud and described in this note with this jupyter notebook . One based on Private cloud using DSX and Db2 warehouse and another notebook . The data used by data scientists are persisted in Db2 warehouse. This note goes over the creation of the Db2 warehouse release within IBM Cloud private. Ingestion mechanism can move data, for chat transcripts and customer records to the DB2 warehouse. This process can run on demand when Data Scientists need new data to tune the model. It can be implemented with an ETL, with Java program, or using the Db2 Federation capability. This note explains what was done to move DB2 customer data to Db2 warehouse. Product recommendations based on Operational Decision Management rules The natural language understanding service is added to support most advanced language processing from the text entered but the end user: entity extraction, relationships, taxonomy, etc. Those elements could be used for scoring services. The language understanding can be fine-tuned by using terms and model defined in Watson Knowledge Studio . The following sequence diagram explains how the components interact with each others.","title":"Components"},{"location":"design/scoring-serv/","text":"Churn Scoring Service We have tried two deployment approaches: Use Watson Machine Learning Service to deploy the model defined in Data Science Experiences, trained with Apache Sparks. This note goes over how to use Watson Data Platform to develop the model and publish it. Use Sparks in ICP and deploy it as a service Publishing the model In the Jupyter notebook it is possible to persist the model. The service URL, username and password are the one for the Watson Machine Learning service on IBM Cloud: from repository.mlrepositoryclient import MLRepositoryClient from repository.mlrepositoryartifact import MLRepositoryArtifact ml_repository_client = MLRepositoryClient(service_url) ml_repository_client.authorize(username, password) model_artifact = MLRepositoryArtifact(model, training_data=train, name=\"Customer Churn Prediction\") saved_model = ml_repository_client.models.save(model_artifact) Once deployed the service can be accessed via API. The code is in server/routes/features/WMLChurnServiceClient.js. The approach is to: Get an authorization token Prepare the data for the service: customer record, tone analysis results... Do a POST on the deployed scoring service. The parameters come from a config file or when deployed in kubernetes cluster from a configMap. const scoring_url = config.scoringService.baseUrl+config.scoringService.instance; request({url:scoring_url, method:\"POST\", headers: { Accept: 'application/json', 'Content-Type': 'application/json;charset=UTF-8', Authorization: 'Bearer '+token }, body: JSON.stringify(payload) ... }) ICP For the ICP DSX work, the Data Scientists is using the data centralized in Db2 Warehouse. The notebook is under src/dsx folder, named CustomerChurnAnalysisDSXICP.ipynb . Loading data from DB2 tables, you need the core utils module, and then using the Spark Session to load data. import dsx_core_utils dataSet = dsx_core_utils.get_remote_data_set_info('CUSTOMER') dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource']) print(dataSource) dbTableOrQuery = dataSet['schema'] + '.' + dataSet['table'] print(dbTableOrQuery) sparkSession = SparkSession(sc).builder.getOrCreate() df_customer_transactions = sparkSession.read.format(\"jdbc\").option(\"url\", dataSource['URL']).option(\"dbtable\",dbTableOrQuery).option(\"user\",'BLUADMIN').option(\"password\",\"changemeplease\").load() df_customer_transactions.show(5) df_customer_transactions.printSchema() Then the process is the same for doing data cleansing, visualization... To persist the mode the client is the same, but the URL does not need to be specified as it will be saved to the ml repository inside DSX running in ICP. from repository.mlrepositoryclient import MLRepositoryClient from repository.mlrepositoryartifact import MLRepositoryArtifact service_path = 'https://internal-nginx-svc.ibm-private-cloud.svc.cluster.local:12443' ml_repository_client = MLRepositoryClient() model_artifact = MLRepositoryArtifact(model, training_data=train, name=\"Customer Churn Prediction - db2\") model_artifact.meta.add(\"authorName\", \"Data Scientist 007\"); saved_model = ml_repository_client.models.save(model_artifact)","title":"Churn risk scoring"},{"location":"design/scoring-serv/#churn-scoring-service","text":"We have tried two deployment approaches: Use Watson Machine Learning Service to deploy the model defined in Data Science Experiences, trained with Apache Sparks. This note goes over how to use Watson Data Platform to develop the model and publish it. Use Sparks in ICP and deploy it as a service","title":"Churn Scoring Service"},{"location":"design/scoring-serv/#publishing-the-model","text":"In the Jupyter notebook it is possible to persist the model. The service URL, username and password are the one for the Watson Machine Learning service on IBM Cloud: from repository.mlrepositoryclient import MLRepositoryClient from repository.mlrepositoryartifact import MLRepositoryArtifact ml_repository_client = MLRepositoryClient(service_url) ml_repository_client.authorize(username, password) model_artifact = MLRepositoryArtifact(model, training_data=train, name=\"Customer Churn Prediction\") saved_model = ml_repository_client.models.save(model_artifact) Once deployed the service can be accessed via API. The code is in server/routes/features/WMLChurnServiceClient.js. The approach is to: Get an authorization token Prepare the data for the service: customer record, tone analysis results... Do a POST on the deployed scoring service. The parameters come from a config file or when deployed in kubernetes cluster from a configMap. const scoring_url = config.scoringService.baseUrl+config.scoringService.instance; request({url:scoring_url, method:\"POST\", headers: { Accept: 'application/json', 'Content-Type': 'application/json;charset=UTF-8', Authorization: 'Bearer '+token }, body: JSON.stringify(payload) ... })","title":"Publishing the model"},{"location":"design/scoring-serv/#icp","text":"For the ICP DSX work, the Data Scientists is using the data centralized in Db2 Warehouse. The notebook is under src/dsx folder, named CustomerChurnAnalysisDSXICP.ipynb . Loading data from DB2 tables, you need the core utils module, and then using the Spark Session to load data. import dsx_core_utils dataSet = dsx_core_utils.get_remote_data_set_info('CUSTOMER') dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource']) print(dataSource) dbTableOrQuery = dataSet['schema'] + '.' + dataSet['table'] print(dbTableOrQuery) sparkSession = SparkSession(sc).builder.getOrCreate() df_customer_transactions = sparkSession.read.format(\"jdbc\").option(\"url\", dataSource['URL']).option(\"dbtable\",dbTableOrQuery).option(\"user\",'BLUADMIN').option(\"password\",\"changemeplease\").load() df_customer_transactions.show(5) df_customer_transactions.printSchema() Then the process is the same for doing data cleansing, visualization... To persist the mode the client is the same, but the URL does not need to be specified as it will be saved to the ml repository inside DSX running in ICP. from repository.mlrepositoryclient import MLRepositoryClient from repository.mlrepositoryartifact import MLRepositoryArtifact service_path = 'https://internal-nginx-svc.ibm-private-cloud.svc.cluster.local:12443' ml_repository_client = MLRepositoryClient() model_artifact = MLRepositoryArtifact(model, training_data=train, name=\"Customer Churn Prediction - db2\") model_artifact.meta.add(\"authorName\", \"Data Scientist 007\"); saved_model = ml_repository_client.models.save(model_artifact)","title":"ICP"},{"location":"design/w-tone-analyzer/","text":"Watson Tone Analyzer integration For integrating Watson Tone Analyzer inside your application flow, you need to create a Watson Tone Analyzer service, get the credential and develop a client code to connect to your application flow. 1- Create the Watson Tone Analyzer service To create the service, login to IBM Cloud and navigate the catalog to Watson category, under Platform , select Tone Analyzer, select the region, organization and space to attach the service to, and then create 2- Access the credentials Select your newly created service and go to the Service credentials to define new configuration for remote connect to the service: Copy those credential into the src/server/config/config.json file and into the chart/values.yaml file. \"toneAnalyzer\":{ \"url\": \"https://gateway.watsonplatform.net/tone-analyzer/api\", \"versionDate\": \"2017-09-21\", \"username\": \"3\", \"password\": \"z\" }, 3- Client code The client code is already coded and can be seen in src/server/routes/features/toneAnalyzer.js it basically uses the Watson developer client modules to access the service. The only thing to consider is what are the utterances. In our case the last sentence entered by the end user is the unique utterance. The alternate choice is to accumulate the last n sentences and add them in the array of utterances. So the service will use more content to assess the global tone of the user. var ToneAnalyzerV3=require('watson-developer-cloud/tone-analyzer/v3'); //... analyzeSentence : function(config,message,res){ return new Promise(function(resolve, reject){ var tone_analyzer = new ToneAnalyzerV3(buildOptions(config)); var params = { utterances: [{\"text\":message}] }; //... })} Integration As explained in the code explanation note when the designer of the conversation dialog flow decides a speciic conversation subject / intent is becoming sensitive, he may set parameters in the conversation context to trigger calls to tone analyzer for any sub sequent interactions. The conversation context boolean toneAnalyzer is used for that, and set to true, so any new sentence sent by the end user will be routed to Watson Tone Analyzer. if (req.body.context.toneAnalyzer && req.body.text !== \"\" ) { toneAnalyzer.analyzeSentence(config,req.body.text).then(function(toneArep) { // ... }) } The returned json object is added to the context too, so it can be used for any other call to back end services, like the churn risk scoring. Here is an example of outcome: {\"utterances_tone\": [{\"utterance_id\":0, \"utterance_text\":\"what? the sale rep told me it will be free, this is a scandal very frustrating, what can be done?\", \"tones\":[ {\"score\":0.651769, \"tone_id\":\"frustrated\", \"tone_name\":\"Frustrated\"}] }] }","title":"Tone analyzer inegration"},{"location":"design/w-tone-analyzer/#watson-tone-analyzer-integration","text":"For integrating Watson Tone Analyzer inside your application flow, you need to create a Watson Tone Analyzer service, get the credential and develop a client code to connect to your application flow.","title":"Watson Tone Analyzer integration"},{"location":"design/w-tone-analyzer/#1-create-the-watson-tone-analyzer-service","text":"To create the service, login to IBM Cloud and navigate the catalog to Watson category, under Platform , select Tone Analyzer, select the region, organization and space to attach the service to, and then create","title":"1- Create the Watson Tone Analyzer service"},{"location":"design/w-tone-analyzer/#2-access-the-credentials","text":"Select your newly created service and go to the Service credentials to define new configuration for remote connect to the service: Copy those credential into the src/server/config/config.json file and into the chart/values.yaml file. \"toneAnalyzer\":{ \"url\": \"https://gateway.watsonplatform.net/tone-analyzer/api\", \"versionDate\": \"2017-09-21\", \"username\": \"3\", \"password\": \"z\" },","title":"2- Access the credentials"},{"location":"design/w-tone-analyzer/#3-client-code","text":"The client code is already coded and can be seen in src/server/routes/features/toneAnalyzer.js it basically uses the Watson developer client modules to access the service. The only thing to consider is what are the utterances. In our case the last sentence entered by the end user is the unique utterance. The alternate choice is to accumulate the last n sentences and add them in the array of utterances. So the service will use more content to assess the global tone of the user. var ToneAnalyzerV3=require('watson-developer-cloud/tone-analyzer/v3'); //... analyzeSentence : function(config,message,res){ return new Promise(function(resolve, reject){ var tone_analyzer = new ToneAnalyzerV3(buildOptions(config)); var params = { utterances: [{\"text\":message}] }; //... })}","title":"3- Client code"},{"location":"design/w-tone-analyzer/#integration","text":"As explained in the code explanation note when the designer of the conversation dialog flow decides a speciic conversation subject / intent is becoming sensitive, he may set parameters in the conversation context to trigger calls to tone analyzer for any sub sequent interactions. The conversation context boolean toneAnalyzer is used for that, and set to true, so any new sentence sent by the end user will be routed to Watson Tone Analyzer. if (req.body.context.toneAnalyzer && req.body.text !== \"\" ) { toneAnalyzer.analyzeSentence(config,req.body.text).then(function(toneArep) { // ... }) } The returned json object is added to the context too, so it can be used for any other call to back end services, like the churn risk scoring. Here is an example of outcome: {\"utterances_tone\": [{\"utterance_id\":0, \"utterance_text\":\"what? the sale rep told me it will be free, this is a scandal very frustrating, what can be done?\", \"tones\":[ {\"score\":0.651769, \"tone_id\":\"frustrated\", \"tone_name\":\"Frustrated\"}] }] }","title":"Integration"},{"location":"env/private/","text":"IBM Cloud Private environment If you want setting up the environment on your own you will need at least seven VMs: One ICP cluster with at the bare minimum of three VMs: one master-proxy node, two worker nodes. This is for the run time. For the machine learning part, we propose to use a dedicated IBM Cloud Pak for data, with four nodes: ne master-proxy node, three worker nodes. One VM for DB2 community edition and liberty server In production, you will use a high availability clusters with 3 masters, 3 managements, 3 proxy, at least 4 worker nodes per ICP clusters. Installing IBM Cloud Private You can study this website and reuse the Terraform file to configure your environment. Installing IBM Cloud Pack for Data We do not plan to add yet another step by step instruction for installing ICP4D. The product installation instructions are at https://docs-icpdata.mybluemix.net/install/com.ibm.icpdata.doc/zen/install/standovu.html and to prepare VMs see this note The ICP run time clustering The following diagram illustrates the ICP cluster topology with some of the major ICP components: The dashed lines highlight the deployment concept of k8s. The Db2 warehouse is using external Glusterfs cluster for persisting data via the persistent volumes and persistent volume claim. The spark cluster, master, spawner... are deployments inside ICP and installed via DSX Local.","title":"Private"},{"location":"env/private/#ibm-cloud-private-environment","text":"If you want setting up the environment on your own you will need at least seven VMs: One ICP cluster with at the bare minimum of three VMs: one master-proxy node, two worker nodes. This is for the run time. For the machine learning part, we propose to use a dedicated IBM Cloud Pak for data, with four nodes: ne master-proxy node, three worker nodes. One VM for DB2 community edition and liberty server In production, you will use a high availability clusters with 3 masters, 3 managements, 3 proxy, at least 4 worker nodes per ICP clusters.","title":"IBM Cloud Private environment"},{"location":"env/private/#installing-ibm-cloud-private","text":"You can study this website and reuse the Terraform file to configure your environment.","title":"Installing IBM Cloud Private"},{"location":"env/private/#installing-ibm-cloud-pack-for-data","text":"We do not plan to add yet another step by step instruction for installing ICP4D. The product installation instructions are at https://docs-icpdata.mybluemix.net/install/com.ibm.icpdata.doc/zen/install/standovu.html and to prepare VMs see this note","title":"Installing IBM Cloud Pack for Data"},{"location":"env/private/#the-icp-run-time-clustering","text":"The following diagram illustrates the ICP cluster topology with some of the major ICP components: The dashed lines highlight the deployment concept of k8s. The Db2 warehouse is using external Glusterfs cluster for persisting data via the persistent volumes and persistent volume claim. The spark cluster, master, spawner... are deployments inside ICP and installed via DSX Local.","title":"The ICP run time clustering"},{"location":"env/public/","text":"IBM Public Cloud environment In this article we are detailing how to prepare the needed services.","title":"Public"},{"location":"env/public/#ibm-public-cloud-environment","text":"In this article we are detailing how to prepare the needed services.","title":"IBM Public Cloud environment"},{"location":"flow/","text":"Demonstration Script There are two paths to demonstrate, the model creating, focusing on analytics work from at-rest data, and the end user view of the application. Training the Machine Learning Model Customer Information, that is persisted in DB2 running on Z or Db2 server, contains customer\u2019s personal information like age, gender, profession, family, income, and account information like service usage, rate plan and device owned. This is item [7] in diagram below: The data model can be seen here . Customer Information is currently planned to run and be maintained on system Z. An Extract Transform Load job, or other script running on-premise ingests customer information from DB2 running on mainframe into the DB2 Warehouse running on IBM cloud private on Z. Items [12] in diagram above. (Alternatively, IBM Information Server products can be used to move data). We have also used the Db2 Warehouse 'remote tables' features to remote connect to the DB2 table (see this note for details). The current implementation is using a Java based implementation of the customer service, using a micro service approach, see the explanation in this project IBM Watson Studio running on ICP uses the data persisted in DB2 Warehouse (Item [11]) to train a model to predict customer churn (Item [10]). The machine learning / jupyter notebook explanations are in this note . In addition, past customer voice records maintained in a file system is cleansed by a cloud native application running on IBM Cloud Private. The cleansed information is then transcribed using IBM Watson Speech to Text running on IBM Public cloud. The transcribed information is persisted on IBM Cloudant database (Item [3]) running on IBM public cloud. IBM Knowledge Studio is used to create custom models to understand the content that is part of the voice chat records. The transcribed information is then used by Watson NLU running on public cloud that uses the custom model from Knowledge Studio to gain understanding of the content. In parallel Watson Tone Analyzer uses the tone to understand the tones from past historical transcripts. The use of Watson NLU and the Tone Analyzer is orchestrated by a cloud native app running in IBM cloud private. The output of NLU and Tone Analyzer is combined and stored in Cloudant DB running on public cloud. IBM DSX running on IBM Watson Data Platform in IBM public cloud catalogs the content from IBM Cloudant (IBM NLU and Tone Analyzer information extracted) and the DB2 Warehouse information and builds a new ML model. This model is deployed on IBM Watson ML running on ICP. Now we have trained model to predict customer churn Predicting Customer Churn In this scenario, the following items can be demonstrated: * Eddie logs on to the Green Telco portal application built as a Microservice and running on IBM Cloud Private. The userid is eddie@email.com . Note that the screen capture comes from the Case portal App. The current project has another version of the user interface Using the customer id (eddie@email.com) from the login, the app invokes the Get Customer Detail API from the back end to load customer and account data. There are two implementations for this API. One using direct access to a Java based service, and another using API Connect and Z OS Connect to retrieve the customer information. The loaded information is collected by the App in memory. The figure below highlights the flow: From the home page, user can access his/her account or the chat bot: The account information comes from the backend database: Eddie complains about the poor quality of data and chat service while he was overseas. He enters the sentence: \"I was oversea and the data was poor\" Eddie also asking about the status of his reimbursement request, with the phrase: \"Well what about the status of my request for reimbursement\" The bot returns the status of the last ticket: it is rejected. Eddie is not very happy and complains about it: \"\" The chat transcript from Eddie is sent to Tone Analyzer and NLU which then determines the sentiment and tone. Using the sentiment and the tone it is determined that Eddie is \"Frustrated\". The chatbot application then invokes the customer churn service which uses the ML model to determine that the customer is not a happy customer and has been with the Telco for 2 years. The figure below illustrates this new flow: Eddie has a genuine problem and deserves a senior call center rep or a supervisor. Eddie is the type of customer the Telco does not want to loose. The application then asks Eddie for a number and the supervisor calls Eddie and clarifies and resolves Eddies issues. Eddie wants to move The conversation is running differently now, as Eddie wants to reallocate. He may have an IPTV and ADSL products at home, and the bot is asking for his new zip code to compute a new product recommendation and take into account his existing products, the new area coverage and the churn risk score to apply a discount. The dialog flow looks like below:","title":"Demonstration"},{"location":"flow/#demonstration-script","text":"There are two paths to demonstrate, the model creating, focusing on analytics work from at-rest data, and the end user view of the application.","title":"Demonstration Script"},{"location":"flow/#training-the-machine-learning-model","text":"Customer Information, that is persisted in DB2 running on Z or Db2 server, contains customer\u2019s personal information like age, gender, profession, family, income, and account information like service usage, rate plan and device owned. This is item [7] in diagram below: The data model can be seen here . Customer Information is currently planned to run and be maintained on system Z. An Extract Transform Load job, or other script running on-premise ingests customer information from DB2 running on mainframe into the DB2 Warehouse running on IBM cloud private on Z. Items [12] in diagram above. (Alternatively, IBM Information Server products can be used to move data). We have also used the Db2 Warehouse 'remote tables' features to remote connect to the DB2 table (see this note for details). The current implementation is using a Java based implementation of the customer service, using a micro service approach, see the explanation in this project IBM Watson Studio running on ICP uses the data persisted in DB2 Warehouse (Item [11]) to train a model to predict customer churn (Item [10]). The machine learning / jupyter notebook explanations are in this note . In addition, past customer voice records maintained in a file system is cleansed by a cloud native application running on IBM Cloud Private. The cleansed information is then transcribed using IBM Watson Speech to Text running on IBM Public cloud. The transcribed information is persisted on IBM Cloudant database (Item [3]) running on IBM public cloud. IBM Knowledge Studio is used to create custom models to understand the content that is part of the voice chat records. The transcribed information is then used by Watson NLU running on public cloud that uses the custom model from Knowledge Studio to gain understanding of the content. In parallel Watson Tone Analyzer uses the tone to understand the tones from past historical transcripts. The use of Watson NLU and the Tone Analyzer is orchestrated by a cloud native app running in IBM cloud private. The output of NLU and Tone Analyzer is combined and stored in Cloudant DB running on public cloud. IBM DSX running on IBM Watson Data Platform in IBM public cloud catalogs the content from IBM Cloudant (IBM NLU and Tone Analyzer information extracted) and the DB2 Warehouse information and builds a new ML model. This model is deployed on IBM Watson ML running on ICP. Now we have trained model to predict customer churn","title":"Training the Machine Learning Model"},{"location":"flow/#predicting-customer-churn","text":"In this scenario, the following items can be demonstrated: * Eddie logs on to the Green Telco portal application built as a Microservice and running on IBM Cloud Private. The userid is eddie@email.com . Note that the screen capture comes from the Case portal App. The current project has another version of the user interface Using the customer id (eddie@email.com) from the login, the app invokes the Get Customer Detail API from the back end to load customer and account data. There are two implementations for this API. One using direct access to a Java based service, and another using API Connect and Z OS Connect to retrieve the customer information. The loaded information is collected by the App in memory. The figure below highlights the flow: From the home page, user can access his/her account or the chat bot: The account information comes from the backend database: Eddie complains about the poor quality of data and chat service while he was overseas. He enters the sentence: \"I was oversea and the data was poor\" Eddie also asking about the status of his reimbursement request, with the phrase: \"Well what about the status of my request for reimbursement\" The bot returns the status of the last ticket: it is rejected. Eddie is not very happy and complains about it: \"\" The chat transcript from Eddie is sent to Tone Analyzer and NLU which then determines the sentiment and tone. Using the sentiment and the tone it is determined that Eddie is \"Frustrated\". The chatbot application then invokes the customer churn service which uses the ML model to determine that the customer is not a happy customer and has been with the Telco for 2 years. The figure below illustrates this new flow: Eddie has a genuine problem and deserves a senior call center rep or a supervisor. Eddie is the type of customer the Telco does not want to loose. The application then asks Eddie for a number and the supervisor calls Eddie and clarifies and resolves Eddies issues.","title":"Predicting Customer Churn"},{"location":"flow/#eddie-wants-to-move","text":"The conversation is running differently now, as Eddie wants to reallocate. He may have an IPTV and ADSL products at home, and the bot is asking for his new zip code to compute a new product recommendation and take into account his existing products, the new area coverage and the churn risk score to apply a discount. The dialog flow looks like below:","title":"Eddie wants to move"},{"location":"method/readme/","text":"Methodology The following diagram illustrates the artificial intelligence / cognitive capabilities developers can integrate in their business application. Data scientists can leverage to develop their analytics models, and the data tasks that need to be perform on private, public or licensed dataset. Micro service development From a pure software engineering implementation, we adopt agile, iterative implementation: the following lower level tasks are done: Develop the data model for the customer, accounts, device as java classes... Develop the Java REST API with resources for customer, account and products, and JAXRS annotations Unit test the API Add JPA annotation to entities, develop Data Transfer Object for service interface, refactoring the API. Implement Data Access Object classes with respective unit tests. In fact, we started by defining the tests. We used a embedded derby, which has a very similar SQL support as DB2, for the unit tests. Develop compile and build script using gradle to be ready for CI/CD Package the webapp as war, deploy to a liberty, tune the liberty settings Dockerize the service with liberty server and the webapp Build helm chart to deploy to IBM Cloud private Build DB2 scripts to prepare the external database, create instance to test and product DB2 instances, add simple data for testing and demo Develop integration tests to validate at the API level the end to end senario Develop the WebApp to support the demonstration using nodejs and angular 4 and the BFF pattern Integrate the webApp with the customer manager micro service running on icp Define / extract the swagger using Liberty api Discovery which is able to introspect the JAXRS annotations and generates the swagger Share the swagger with Z OS team so they can implement the service with z Connect bases on the same interface definition Add API product using the swagger, publish the API to the Data Power gateway, test with Postman the integration API -> REST service on ICP -> DB2 on on-premise server. Change the URL of the webapp to point to the API end point, redeploy to ICP. Analytics specifics The following note explains in detail what need to be done to prepare the data, train the model and test its validity.","title":"Methodology"},{"location":"method/readme/#methodology","text":"The following diagram illustrates the artificial intelligence / cognitive capabilities developers can integrate in their business application. Data scientists can leverage to develop their analytics models, and the data tasks that need to be perform on private, public or licensed dataset.","title":"Methodology"},{"location":"method/readme/#micro-service-development","text":"From a pure software engineering implementation, we adopt agile, iterative implementation: the following lower level tasks are done: Develop the data model for the customer, accounts, device as java classes... Develop the Java REST API with resources for customer, account and products, and JAXRS annotations Unit test the API Add JPA annotation to entities, develop Data Transfer Object for service interface, refactoring the API. Implement Data Access Object classes with respective unit tests. In fact, we started by defining the tests. We used a embedded derby, which has a very similar SQL support as DB2, for the unit tests. Develop compile and build script using gradle to be ready for CI/CD Package the webapp as war, deploy to a liberty, tune the liberty settings Dockerize the service with liberty server and the webapp Build helm chart to deploy to IBM Cloud private Build DB2 scripts to prepare the external database, create instance to test and product DB2 instances, add simple data for testing and demo Develop integration tests to validate at the API level the end to end senario Develop the WebApp to support the demonstration using nodejs and angular 4 and the BFF pattern Integrate the webApp with the customer manager micro service running on icp Define / extract the swagger using Liberty api Discovery which is able to introspect the JAXRS annotations and generates the swagger Share the swagger with Z OS team so they can implement the service with z Connect bases on the same interface definition Add API product using the swagger, publish the API to the Data Power gateway, test with Postman the integration API -> REST service on ICP -> DB2 on on-premise server. Change the URL of the webapp to point to the API end point, redeploy to ICP.","title":"Micro service development"},{"location":"method/readme/#analytics-specifics","text":"The following note explains in detail what need to be done to prepare the data, train the model and test its validity.","title":"Analytics specifics"},{"location":"ml/","text":"Customer Churn Analytics using IBM Data Science Experience and Watson Data Platform Introduction and Background This section outlines the steps to build and deploy an analytical model for predicting customer churn, using the combination of tooling in Watson Data Platform (WDP), and IBM Data Science Experience (DSX). The steps involve gathering data from various sources, aggregating them using Jupyter notebooks, building and training an analytical model, and finally deploying it on Watson Data Platform. The overall system context for Watson Data Platform part of the solution is outlined in the following picture. For a pure DSX and Db2 Warehouse and Spark cluster inside ICP approach the high level view looks like: For the purposes of this exercise, this section shows similar operations (like filling missing values in the dataset) both in WDP and DSX. The reason is to illustrate the capabilities and the art of the possible. The solution involves three datasets from the analytics standpoint, as outlined below: Customer Transaction Data : This data contains the customer's personal information and demographics (age, marital status, income etc.), and the subscription related details (such as phone model, plans, bill type etc.). This information is stored in a DB2 Warehouse database on cloud. Customer campaign results : This dataset includes the results of a marketing campaign conducted by a third part marketing firm for the Telco customers. This is intended to capture the customer's preferred device or feature (such as large display, Android phone etc.) and usage related aspects such as if the customer has multiple phone numbers, and the number of SMS text messages the customer exchanges with his contacts. The notion is, the usage model and preferences, could have a strong influence in any given customer's preference to stay with the provider or leave. As per the scenario in this solution, this data is stored in the data servers of the marketing firm, which is on a third party cloud. The marketing firm makes this dataset available on Amazon S3. Call center notes : This dataset contains a short, summary style description of customer's complaint, two or three important keywords showing the device, feature, and the problem, and the sentiment shown by the customer. In some cases customers simply ask questions such as where the nearest store is. These kinds of entries may have no value for sentiments. These need to be fixed before one can perform analytics. This dataset resides in Cloudant on IBM cloud. Since each customer complaint is a service ticket, which essentially is a document, this data is stored in Cloudant. The steps will be mostly aligned with CRISP-DM methodology, focusing on the data preparation and analytics. Prerequisites Before beginning the exercise, make sure you have accounts in the following systems. IBM Cloud Watson Data Platform IBM Data Science Experience - If you register for access in Watson Data Platform, you will get access to IBM data science experience. As an alternate use the DSX on IBM Cloud Private instance. The datasets used in this exercise are shown in the data subfolder of this github repository. It is assumed you know how to create instances of services on IBM Cloud (Cloudant, DB2 Warehouse), and Amazon S3. Those steps are not outlined in the following sections, and for guidance with those services, the user is advised to check the documentation. Chrome and Firefox are tested for this exercise and for the examples shown here. Other browsers may or may not work. It is left to the user to check and make sure. Data Import Ingestion of data from different data sources requires creating connection to the data sources, and importing the datasets. Preparation of data involves operations inside Data Refinery using the shaper tool. Part of the data preparations are also shown inside DSX. To begin with, open IBM Watson Data Platform (henceforth in this document referred to as WDP) using a browser. The main screen should show something like the following: Provide name for the project and select the appropriate cloud object storage and the spark instance from your IBM Cloud workspace. Once you have finished the steps, the resulting screen would look like the following. Click on \"Create\" button on the lower right bottom of the screen. This will create a new WDP project for us to work further. If you have not created an instance of cloud object storage and spark service, create them first before proceeding with this step. What you just created is a WDP project, in which you will be creating artifacts such as connections to datasources, datasets, notebooks, catalogs, analytical models etc. After creating the project, you should now be looking at the main screen where you see a list of projects. Click on the newly created project, and you should be seeing a page like the following. From the tool bar on the right, click on the drop down box \"Add to project\" and then click on the menu \"connection\". This is illustrated in the following picture. It should take you to a screen where you can see all the datsources supported by WDP. For this example, you will be creating three datasources - cloudant, DB2 Warehouse, and Amazon S3. Each one of these sources contain a particular dataset as described in the section at the introduction section. Note that if you have your data in different systems, choose those systems. Click on Cloudant database and add it as a connection. The following two screenshots show this step. If you have not created a cloudant instance on IBM Cloud, you should do that first before resuming from this step. In the screenshot you see all the service instances that are part of IBM Cloud. You can click on \"IBM Cloudant\" option as highlighted, and proceed to fill in the credentials. Your screen should look like the following after clicking \"IBM Cloudant\". Fill in the credentials and click the button \"Create\" on the lower right corner of the page. To get the credentials for accessing the cloudant instance, you should open your cloudant instance on IBM Cloud. Likewise, create two additional connections for DB2Warehouse and Amazon S3. The screen for DB2 Warehouse is shown below. Note that there is a checkbox for secure gateway that you can enable if you are accessing the DB2 Warehouse instance behind a firewall. Once you have created all the three connections, you should be back to the main screen of your project, where you can see all the three connections listed. The next step is to import the data from these three datasources, using the connection we just created. Click on the toolbar drop down menu \"Add to Project\" > \"Connected Data\" as shown in the following picture. In the resulting screen as shown below, provide a name for the data asset, and click on \"Select Source\". That should bring up a page with all the datasaources created in the previous steps. The following screenshot illustrates this step. Click on the cloudant based datasource. That would bring up the databases you created on cloudant. Pick the right database, and click on the button \"Select\" on the lower right corner. The following screenshot shows this part of the operation. That should bring you back to the original screen from where you selected the datasource. Click on the button \"Create\" on the lower right corner. That should complete the steps to import the data asset from cloudant. Likewise repeat the steps for the other two data sources to pull the two data assets we need for analytics. Data Preparation / Refinement In the previous steps we created datasources, and then imported data assets from each one of them. In this section, we will perform some visualizations to explore the data, and perform simple operations to prepare the data. These steps will be performed using the tool Data Refinery, which is part of the Watson Data Platform. See note of the jupyter notebook exported as markdown file. Analytical Model Appendix 1: Data Catalogs Appendix 2: Performance Monitoring & Fine Tuning","title":"Analytics implementation"},{"location":"ml/#customer-churn-analytics-using-ibm-data-science-experience-and-watson-data-platform","text":"","title":"Customer Churn Analytics using IBM Data Science Experience and Watson Data Platform"},{"location":"ml/#introduction-and-background","text":"This section outlines the steps to build and deploy an analytical model for predicting customer churn, using the combination of tooling in Watson Data Platform (WDP), and IBM Data Science Experience (DSX). The steps involve gathering data from various sources, aggregating them using Jupyter notebooks, building and training an analytical model, and finally deploying it on Watson Data Platform. The overall system context for Watson Data Platform part of the solution is outlined in the following picture. For a pure DSX and Db2 Warehouse and Spark cluster inside ICP approach the high level view looks like: For the purposes of this exercise, this section shows similar operations (like filling missing values in the dataset) both in WDP and DSX. The reason is to illustrate the capabilities and the art of the possible. The solution involves three datasets from the analytics standpoint, as outlined below: Customer Transaction Data : This data contains the customer's personal information and demographics (age, marital status, income etc.), and the subscription related details (such as phone model, plans, bill type etc.). This information is stored in a DB2 Warehouse database on cloud. Customer campaign results : This dataset includes the results of a marketing campaign conducted by a third part marketing firm for the Telco customers. This is intended to capture the customer's preferred device or feature (such as large display, Android phone etc.) and usage related aspects such as if the customer has multiple phone numbers, and the number of SMS text messages the customer exchanges with his contacts. The notion is, the usage model and preferences, could have a strong influence in any given customer's preference to stay with the provider or leave. As per the scenario in this solution, this data is stored in the data servers of the marketing firm, which is on a third party cloud. The marketing firm makes this dataset available on Amazon S3. Call center notes : This dataset contains a short, summary style description of customer's complaint, two or three important keywords showing the device, feature, and the problem, and the sentiment shown by the customer. In some cases customers simply ask questions such as where the nearest store is. These kinds of entries may have no value for sentiments. These need to be fixed before one can perform analytics. This dataset resides in Cloudant on IBM cloud. Since each customer complaint is a service ticket, which essentially is a document, this data is stored in Cloudant. The steps will be mostly aligned with CRISP-DM methodology, focusing on the data preparation and analytics.","title":"Introduction and Background"},{"location":"ml/#prerequisites","text":"Before beginning the exercise, make sure you have accounts in the following systems. IBM Cloud Watson Data Platform IBM Data Science Experience - If you register for access in Watson Data Platform, you will get access to IBM data science experience. As an alternate use the DSX on IBM Cloud Private instance. The datasets used in this exercise are shown in the data subfolder of this github repository. It is assumed you know how to create instances of services on IBM Cloud (Cloudant, DB2 Warehouse), and Amazon S3. Those steps are not outlined in the following sections, and for guidance with those services, the user is advised to check the documentation. Chrome and Firefox are tested for this exercise and for the examples shown here. Other browsers may or may not work. It is left to the user to check and make sure.","title":"Prerequisites"},{"location":"ml/#data-import","text":"Ingestion of data from different data sources requires creating connection to the data sources, and importing the datasets. Preparation of data involves operations inside Data Refinery using the shaper tool. Part of the data preparations are also shown inside DSX. To begin with, open IBM Watson Data Platform (henceforth in this document referred to as WDP) using a browser. The main screen should show something like the following: Provide name for the project and select the appropriate cloud object storage and the spark instance from your IBM Cloud workspace. Once you have finished the steps, the resulting screen would look like the following. Click on \"Create\" button on the lower right bottom of the screen. This will create a new WDP project for us to work further. If you have not created an instance of cloud object storage and spark service, create them first before proceeding with this step. What you just created is a WDP project, in which you will be creating artifacts such as connections to datasources, datasets, notebooks, catalogs, analytical models etc. After creating the project, you should now be looking at the main screen where you see a list of projects. Click on the newly created project, and you should be seeing a page like the following. From the tool bar on the right, click on the drop down box \"Add to project\" and then click on the menu \"connection\". This is illustrated in the following picture. It should take you to a screen where you can see all the datsources supported by WDP. For this example, you will be creating three datasources - cloudant, DB2 Warehouse, and Amazon S3. Each one of these sources contain a particular dataset as described in the section at the introduction section. Note that if you have your data in different systems, choose those systems. Click on Cloudant database and add it as a connection. The following two screenshots show this step. If you have not created a cloudant instance on IBM Cloud, you should do that first before resuming from this step. In the screenshot you see all the service instances that are part of IBM Cloud. You can click on \"IBM Cloudant\" option as highlighted, and proceed to fill in the credentials. Your screen should look like the following after clicking \"IBM Cloudant\". Fill in the credentials and click the button \"Create\" on the lower right corner of the page. To get the credentials for accessing the cloudant instance, you should open your cloudant instance on IBM Cloud. Likewise, create two additional connections for DB2Warehouse and Amazon S3. The screen for DB2 Warehouse is shown below. Note that there is a checkbox for secure gateway that you can enable if you are accessing the DB2 Warehouse instance behind a firewall. Once you have created all the three connections, you should be back to the main screen of your project, where you can see all the three connections listed. The next step is to import the data from these three datasources, using the connection we just created. Click on the toolbar drop down menu \"Add to Project\" > \"Connected Data\" as shown in the following picture. In the resulting screen as shown below, provide a name for the data asset, and click on \"Select Source\". That should bring up a page with all the datasaources created in the previous steps. The following screenshot illustrates this step. Click on the cloudant based datasource. That would bring up the databases you created on cloudant. Pick the right database, and click on the button \"Select\" on the lower right corner. The following screenshot shows this part of the operation. That should bring you back to the original screen from where you selected the datasource. Click on the button \"Create\" on the lower right corner. That should complete the steps to import the data asset from cloudant. Likewise repeat the steps for the other two data sources to pull the two data assets we need for analytics.","title":"Data Import"},{"location":"ml/#data-preparation-refinement","text":"In the previous steps we created datasources, and then imported data assets from each one of them. In this section, we will perform some visualizations to explore the data, and perform simple operations to prepare the data. These steps will be performed using the tool Data Refinery, which is part of the Watson Data Platform. See note of the jupyter notebook exported as markdown file.","title":"Data Preparation / Refinement"},{"location":"ml/#analytical-model","text":"","title":"Analytical Model"},{"location":"ml/#appendix-1-data-catalogs","text":"","title":"Appendix 1: Data Catalogs"},{"location":"ml/#appendix-2-performance-monitoring-fine-tuning","text":"","title":"Appendix 2: Performance Monitoring &amp; Fine Tuning"},{"location":"ml/CustomerChurnAnalysisCl-bpull/","text":"Evaluate and predict customer churn This notebook is an adaptation from the work done by Sidney Phoon and Eleva Lowery with the following modifications: Use datasets persisted in DB2 Warehouse running on ICP Use additional datasets from multiple datasources such as Cloudant on public cloud, and Amazon S3 Use of Watson Data Platform for data ingestion and preparation (using Data Refinery) Deploy and run the notebook on DSX enterprise running on IBM Cloud Private (ICP) Run spark Machine learning job on ICP as part of the worker nodes. Document some actions for a beginner data scienctist / developer who wants to understand what's going on. The web application was separated in another git project The goal is to demonstrate how to build a predictive model with Spark machine learning API (SparkML) to predict customer churn, and deploy it for scoring in Machine Learning (ML) running on ICP or within IBM public Cloud, Watson Machine Learning service. Scope A lot of industries have the issue of customers moving to competitors when the product differentiation is not that important, or there is some customer support issues. One industry illustrating this problem is the telecom industry with mobile, internet and IP TV product offerings. Note book explanations The notebook aims to follow the classical data science modeling steps: load the data prepare the data analyze the data (iterate on those two activities) build a model validate the accuracy of the model deploy the model consume the model as a service This jupyter notebook uses Apache Spark to run the machine learning jobs to build decision trees and random forest classifier to assess when a customer is at risk to move to competitor. Apache Spark offers a Python module called pyspark to operate on data and use ML constructs. Start by all imports As a best practices for notebook implementation is to do the import at the top of the notebook. Spark SQLContext a spark module to process structured data spark conf to access Spark cluster configuration and then be able to execute queries jaydebeapi is used to connect to the DB 2 warehouse where customer data are persisted. We assume they are loaded. ibmdbpy interface for data manipulation and access to in-database algorithms in IBM dashDB and IBM DB2. pandas Python super library for data analysis brunel API and tool to visualize data quickly. * pixiedust Visualize data inside Jupyter notebooks The first cell below is to execute some system commands to update the kernel with updated dependant library. # Needed to access data from IBM Cloud Object Storage !pip install --upgrade ibm-cos-sdk Requirement already up-to-date: ibm-cos-sdk in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages Requirement already up-to-date: ibm-cos-sdk-core==2.*,>=2.0.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from ibm-cos-sdk) Requirement already up-to-date: ibm-cos-sdk-s3transfer==2.*,>=2.0.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from ibm-cos-sdk) Requirement already up-to-date: python-dateutil<3.0.0,>=2.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk) Requirement already up-to-date: docutils>=0.10 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk) Requirement already up-to-date: jmespath<1.0.0,>=0.7.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk) Requirement already up-to-date: six>=1.5 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk) # Required for accessing data on IBM Cloud Object Storage !pip install --upgrade boto3 Requirement already up-to-date: boto3 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages Requirement already up-to-date: botocore<1.9.0,>=1.8.36 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from boto3) Requirement already up-to-date: s3transfer<0.2.0,>=0.1.10 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from boto3) Requirement already up-to-date: jmespath<1.0.0,>=0.7.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from boto3) Requirement already up-to-date: python-dateutil<3.0.0,>=2.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from botocore<1.9.0,>=1.8.36->boto3) Requirement already up-to-date: docutils>=0.10 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from botocore<1.9.0,>=1.8.36->boto3) Requirement already up-to-date: six>=1.5 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.9.0,>=1.8.36->boto3) # The following will be needed if you want to download datasets from outside the WDP environment for any reason. But we won't be needing this in the current sample exercise !pip install --upgrade wget Requirement already up-to-date: wget in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages # Library required for pixiedust - a visualization and dashboarding framework !pip install --user --upgrade pixiedust Requirement already up-to-date: pixiedust in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages Requirement already up-to-date: astunparse in /usr/local/src/conda3_runtime.v27/4.1.1/lib/python3.5/site-packages (from pixiedust) Requirement already up-to-date: lxml in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from pixiedust) Requirement already up-to-date: mpld3 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from pixiedust) Requirement already up-to-date: geojson in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from pixiedust) Requirement already up-to-date: markdown in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from pixiedust) Requirement already up-to-date: wheel<1.0,>=0.23.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from astunparse->pixiedust) Requirement already up-to-date: six<2.0,>=1.6.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from astunparse->pixiedust) # Needed to deploy the model on Watson Machine Learning Service !pip install --upgrade watson-machine-learning-client Requirement already up-to-date: watson-machine-learning-client in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages Requirement already up-to-date: lomond in /usr/local/src/conda3_runtime.v27/4.1.1/lib/python3.5/site-packages (from watson-machine-learning-client) Requirement already up-to-date: tabulate in /usr/local/src/conda3_runtime.v27/4.1.1/lib/python3.5/site-packages (from watson-machine-learning-client) Requirement already up-to-date: tqdm in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from watson-machine-learning-client) Requirement already up-to-date: requests in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from watson-machine-learning-client) Requirement already up-to-date: urllib3 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from watson-machine-learning-client) Requirement already up-to-date: certifi in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from watson-machine-learning-client) Requirement already up-to-date: pandas in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from watson-machine-learning-client) Requirement already up-to-date: six>=1.10.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from lomond->watson-machine-learning-client) Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from requests->watson-machine-learning-client) Requirement already up-to-date: idna<2.7,>=2.5 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from requests->watson-machine-learning-client) Requirement already up-to-date: python-dateutil>=2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from pandas->watson-machine-learning-client) Requirement already up-to-date: numpy>=1.9.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from pandas->watson-machine-learning-client) Requirement already up-to-date: pytz>=2011k in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from pandas->watson-machine-learning-client) import pyspark import pandas as pd import brunel import numpy as np from pyspark.sql import SQLContext from pyspark.conf import SparkConf from pyspark.sql import SparkSession from pyspark.sql.types import DoubleType from pyspark.sql.types import DecimalType from pyspark.sql.types import IntegerType from pyspark.sql.types import LongType from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString from pyspark.ml import Pipeline from pyspark.ml.feature import VectorAssembler from pyspark.ml.classification import RandomForestClassifier from pyspark.ml.evaluation import BinaryClassificationEvaluator from pixiedust.display import * import ibm_boto3 Load data using Watson Data Platform (WDP) Watson Data platform (WDP) can be used to bring in data from multiple sources including but not limited to, files, data stores on cloud as well as on premises. WDP includes features to connect to data sources, bring in the data, refine, and then perform analytics. In this sample are using WDP approach. We connect to Amazon S3, Cloudant on IBM public (or private) cloud, and DB2 Data Warehouse on IBM public or private cloud. Once we bring in the data, we refine / cleanse them using Data Refinery and export the result as a CSV file for training analytical model. These steps are already accomplished using WDP so that we can start loading that data for analytics in the following cells. # Load customer information along with churn status. We read this from the CSV file prepared for training purposes import ibmos2spark # @hidden_cell credentials = { 'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com', 'api_key': '', 'service_id': 'iam-ServiceId-de6a2704-3436-4927-8ceb-a2e3dfc3288e', 'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token'} configuration_name = 'os_d21bf0a6588f494d822173729799c934_configs' cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos') from pyspark.sql import SparkSession spark = SparkSession.builder.getOrCreate() df_customer_transactions = spark.read\\ .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\ .option('header', 'true')\\ .load(cos.url('data_asset/YTc2MDUyNjAtYzg5OC00MDk5LTgyZGItYThlZjI4ZDczNmZl_data_6283e5af-2e69-426e-b327-1a3bbc23ded3.csv', 'customerchurnproject86ce96f6a9384a669a14bd5dd9b3028e')) df_customer_transactions.take(5) df_customer_transactions.printSchema() root |-- ID: string (nullable = true) |-- Gender: string (nullable = true) |-- Status: string (nullable = true) |-- Children: string (nullable = true) |-- Est Income: string (nullable = true) |-- Car Owner: string (nullable = true) |-- Age: string (nullable = true) |-- Marital Status: string (nullable = true) |-- zipcode: string (nullable = true) |-- LongDistance: string (nullable = true) |-- International: string (nullable = true) |-- Local: string (nullable = true) |-- Dropped: string (nullable = true) |-- Paymethod: string (nullable = true) |-- LocalBilltype: string (nullable = true) |-- LongDistanceBilltype: string (nullable = true) |-- Usage: string (nullable = true) |-- RatePlan: string (nullable = true) |-- DeviceOwned: string (nullable = true) |-- CHURN: string (nullable = true) # Load the call notes dataset df_call_notes = spark.read\\ .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\ .option('header', 'true')\\ .load(cos.url('data_asset/MTIwYWQwZDctMmMyMC00NTkzLWI3YjItNDI4NGVjMzhlYjA5_data_0601330c-2331-46df-b9bc-df5ab0eb9a41.csv', 'customerchurnproject86ce96f6a9384a669a14bd5dd9b3028e')) df_call_notes.take(5) df_call_notes.describe() df_call_notes.show(25) +--------------------+---+------------+-------------+----------+ | Comments| ID| Keyword1| Keyword2|Sentiments| +--------------------+---+------------+-------------+----------+ |Asked about stora...| 1| sim| storage|analytical| |Asked about low-e...| 6|basic config| smartphone|analytical| |Dissatisfied with...| 8| promotion| rebate|frustrated| |Asked about low-e...| 11|basic config| smartphone|analytical| |Asked about low-e...| 14|basic config| smartphone|analytical| |Dissatisfied with...| 17| promotion| rebate|frustrated| |Asked about low-e...| 18|basic config| smartphone|analytical| |Asked about low-e...| 21|basic config| smartphone|analytical| |Upset about the d...| 22| data plan| speed|frustrated| |Asked about low-e...| 23|basic config| smartphone|analytical| |Asked how to inst...| 24| sd card| apps|analytical| |Asked how to inst...| 29| sd card| apps|analytical| |Said his battery ...| 35| battery| new phone|frustrated| |Said his battery ...| 36| battery| new phone|frustrated| |Said his battery ...| 37| battery| new phone|frustrated| |Said his battery ...| 38| battery| new phone|frustrated| |Said his battery ...| 40| battery| new phone|frustrated| |He charges it and...| 42| charging| battery|frustrated| |Asked how to inst...| 45| sd card| apps|analytical| |Asked how to inst...| 48| sd card| apps|analytical| |He expected signi...| 52| technical| support|frustrated| |He asked for a ne...| 53| new number|customer care|analytical| |He expected signi...| 54| technical| support|frustrated| |He expected signi...| 60| technical| support|frustrated| |transferred to th...| 61| supervisor| delegation| neutral| +--------------------+---+------------+-------------+----------+ only showing top 25 rows # Load customer campaign responses dataset df_campaign_responses = spark.read\\ .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\ .option('header', 'true')\\ .load(cos.url('data_asset/MmQwNjUxYmUtYjU3My00ZmViLTk0YWEtYTc5ZWExZmU3Mjg3_data_e26e9c54-7143-449c-bbf4-71998290d86a.csv', 'customerchurnproject86ce96f6a9384a669a14bd5dd9b3028e')) df_campaign_responses.take(5) [Row(CUSTOMERID='6', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1807', Owns Multiple Lines='0'), Row(CUSTOMERID='8', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1364', Owns Multiple Lines='0'), Row(CUSTOMERID='18', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1407', Owns Multiple Lines='0'), Row(CUSTOMERID='29', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1043', Owns Multiple Lines='0'), Row(CUSTOMERID='37', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='841', Owns Multiple Lines='0')] The next few steps involve a series of data preparation tasks such as filling the missing values, joining datasets etc. The following cell fills the null values for average SMS count. Note that this can be accomplished by using Data Refinery, but the following snippet is shown to iullustrate the API way of accomplishing the same. df_campaign_responses = df_campaign_responses.na.fill({'Ave Text Msgs':'0'}) In the following cell we join some of our data sources. Note that we could have done some of these using Data Refinery on Watson Data Platform using GUI support. data_joined_callnotes_churn = df_call_notes.join(df_customer_transactions,df_call_notes['ID']==df_customer_transactions['ID'],'inner').select(df_call_notes['Sentiments'],df_call_notes['Keyword1'],df_call_notes['Keyword2'],df_customer_transactions['*']) data_joined_callnotes_churn_campaign = df_campaign_responses.join(data_joined_callnotes_churn,df_campaign_responses['CUSTOMERID']==data_joined_callnotes_churn['ID'],'inner').select(data_joined_callnotes_churn['*'],df_campaign_responses['Preference'],df_campaign_responses['Owns Multiple Phone Numbers'],df_campaign_responses['Ave Text Msgs']) data_joined_callnotes_churn_campaign.take(10) [Row(Sentiments='analytical', Keyword1='sd card', Keyword2='apps', ID='24', Gender='F', Status='M', Children='2', Est Income='47902.00', Car Owner='N', Age='26.033333', Marital Status='Married', zipcode=None, LongDistance='17.44', International='4.94', Local='49.92', Dropped='1', Paymethod='Auto', LocalBilltype='FreeLocal', LongDistanceBilltype='Standard', Usage='72.31', RatePlan='2', DeviceOwned='moto', CHURN='F', Preference='more storage', Owns Multiple Phone Numbers='N', Ave Text Msgs='1864'), Row(Sentiments='frustrated', Keyword1='battery', Keyword2='new phone', ID='35', Gender='F', Status='S', Children='0', Est Income='78851.30', Car Owner='N', Age='48.373333', Marital Status='Single', zipcode=None, LongDistance='0.37', International='0.00', Local='28.66', Dropped='0', Paymethod='CC', LocalBilltype='FreeLocal', LongDistanceBilltype='Standard', Usage='29.04', RatePlan='4', DeviceOwned='ipho', CHURN='T', Preference='dual sim', Owns Multiple Phone Numbers='Y', Ave Text Msgs='1173'), Row(Sentiments='frustrated', Keyword1='battery', Keyword2='new phone', ID='37', Gender='F', Status='M', Children='0', Est Income='83891.90', Car Owner='Y', Age='61.020000', Marital Status='Married', zipcode=None, LongDistance='28.92', International='0.00', Local='45.47', Dropped='0', Paymethod='CH', LocalBilltype='Budget', LongDistanceBilltype='Standard', Usage='74.40', RatePlan='4', DeviceOwned='ipho', CHURN='T', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='841'), Row(Sentiments='frustrated', Keyword1='battery', Keyword2='new phone', ID='38', Gender='F', Status='M', Children='2', Est Income='28220.80', Car Owner='N', Age='38.766667', Marital Status='Married', zipcode=None, LongDistance='26.49', International='0.00', Local='12.46', Dropped='0', Paymethod='CC', LocalBilltype='FreeLocal', LongDistanceBilltype='Standard', Usage='38.95', RatePlan='4', DeviceOwned='ipho', CHURN='T', Preference='windows phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1779'), Row(Sentiments='frustrated', Keyword1='battery', Keyword2='new phone', ID='40', Gender='F', Status='S', Children='0', Est Income='28589.10', Car Owner='N', Age='15.600000', Marital Status='Single', zipcode=None, LongDistance='13.19', International='0.00', Local='87.09', Dropped='0', Paymethod='CC', LocalBilltype='FreeLocal', LongDistanceBilltype='Standard', Usage='100.28', RatePlan='4', DeviceOwned='ipho', CHURN='T', Preference='large display', Owns Multiple Phone Numbers='N', Ave Text Msgs='1720'), Row(Sentiments='analytical', Keyword1='sd card', Keyword2='apps', ID='45', Gender='M', Status='S', Children='2', Est Income='89459.90', Car Owner='N', Age='53.280000', Marital Status='Single', zipcode=None, LongDistance='11.54', International='1.61', Local='22.90', Dropped='0', Paymethod='CC', LocalBilltype='FreeLocal', LongDistanceBilltype='Standard', Usage='36.05', RatePlan='2', DeviceOwned='ipho', CHURN='T', Preference='more storage', Owns Multiple Phone Numbers='N', Ave Text Msgs='1886'), Row(Sentiments='frustrated', Keyword1='technical', Keyword2='support', ID='52', Gender='F', Status='M', Children='2', Est Income='67388.00', Car Owner='N', Age='53.120000', Marital Status='Married', zipcode=None, LongDistance='4.79', International='0.50', Local='91.04', Dropped='1', Paymethod='CC', LocalBilltype='Budget', LongDistanceBilltype='Standard', Usage='96.33', RatePlan='3', DeviceOwned='ipho', CHURN='T', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1590'), Row(Sentiments='analytical', Keyword1='new number', Keyword2='customer care', ID='53', Gender='F', Status='M', Children='1', Est Income='57063.00', Car Owner='Y', Age='52.333333', Marital Status='Married', zipcode=None, LongDistance='16.79', International='0.00', Local='81.30', Dropped='0', Paymethod='CH', LocalBilltype='Budget', LongDistanceBilltype='Standard', Usage='98.10', RatePlan='4', DeviceOwned='ipho', CHURN='F', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1205'), Row(Sentiments='frustrated', Keyword1='technical', Keyword2='support', ID='54', Gender='F', Status='M', Children='2', Est Income='84166.10', Car Owner='N', Age='54.013333', Marital Status='Married', zipcode=None, LongDistance='3.28', International='0.00', Local='11.74', Dropped='1', Paymethod='CC', LocalBilltype='Budget', LongDistanceBilltype='Standard', Usage='15.02', RatePlan='2', DeviceOwned='ipho', CHURN='T', Preference='dual sim', Owns Multiple Phone Numbers='Y', Ave Text Msgs='1625'), Row(Sentiments='neutral', Keyword1='supervisor', Keyword2='delegation', ID='61', Gender='M', Status='S', Children='2', Est Income='100020.00', Car Owner='N', Age='50.000000', Marital Status='Single', zipcode=None, LongDistance='21.37', International='0.00', Local='293.24', Dropped='0', Paymethod='CH', LocalBilltype='Budget', LongDistanceBilltype='Standard', Usage='314.62', RatePlan='4', DeviceOwned='ipho', CHURN='T', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1696')] The following code block is intended to get a feel for Spark DataFrame APIs. We attempt to fix some of the column titles to promote readability, and also remove a duplicate column (Status and Marital Status are the same). Finally convert the DataFrame to Python Pandas structure for visualization. Since all are string types from the CSV file, let us change some of them to other types # Change some column names data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"Sentiments\", \"Sentiment\").withColumnRenamed(\"Owns Multiple Phone Numbers\",\"OMPN\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"Keyword1\", \"Keyword_Component\").withColumnRenamed(\"Keyword2\",\"Keyword_Query\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"Ave Text Msgs\", \"SMSCount\").withColumnRenamed(\"Car Owner\",\"CarOwnership\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"Marital Status\", \"MaritalStatus\").withColumnRenamed(\"Est Income\",\"Income\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.drop('Status') # Change some of the data types data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"Children\", data_joined_callnotes_churn_campaign[\"Children\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"Income\", data_joined_callnotes_churn_campaign[\"Income\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"Age\", data_joined_callnotes_churn_campaign[\"Age\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"LongDistance\", data_joined_callnotes_churn_campaign[\"LongDistance\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"International\", data_joined_callnotes_churn_campaign[\"International\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"Local\", data_joined_callnotes_churn_campaign[\"Local\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"Dropped\", data_joined_callnotes_churn_campaign[\"Dropped\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"Usage\", data_joined_callnotes_churn_campaign[\"Usage\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"RatePlan\", data_joined_callnotes_churn_campaign[\"RatePlan\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"SMSCount\", data_joined_callnotes_churn_campaign[\"SMSCount\"].cast(IntegerType())) data_joined_callnotes_churn_campaign.show(10) data_joined_callnotes_churn_campaign.printSchema() pandas_df_callnotes_campaign_churn = data_joined_callnotes_churn_campaign.toPandas() pandas_df_callnotes_campaign_churn.head(12) +----------+-----------------+-------------+---+------+--------+------+------------+---+-------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----------+-----+-------------+----+--------+ | Sentiment|Keyword_Component|Keyword_Query| ID|Gender|Children|Income|CarOwnership|Age|MaritalStatus|zipcode|LongDistance|International|Local|Dropped|Paymethod|LocalBilltype|LongDistanceBilltype|Usage|RatePlan|DeviceOwned|CHURN| Preference|OMPN|SMSCount| +----------+-----------------+-------------+---+------+--------+------+------------+---+-------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----------+-----+-------------+----+--------+ |analytical| sd card| apps| 24| F| 2| 47902| N| 26| Married| null| 17| 5| 50| 1| Auto| FreeLocal| Standard| 72| 2| moto| F| more storage| N| 1864| |frustrated| battery| new phone| 35| F| 0| 78851| N| 48| Single| null| 0| 0| 29| 0| CC| FreeLocal| Standard| 29| 4| ipho| T| dual sim| Y| 1173| |frustrated| battery| new phone| 37| F| 0| 83892| Y| 61| Married| null| 29| 0| 45| 0| CH| Budget| Standard| 74| 4| ipho| T|android phone| N| 841| |frustrated| battery| new phone| 38| F| 2| 28221| N| 38| Married| null| 26| 0| 12| 0| CC| FreeLocal| Standard| 39| 4| ipho| T|windows phone| N| 1779| |frustrated| battery| new phone| 40| F| 0| 28589| N| 15| Single| null| 13| 0| 87| 0| CC| FreeLocal| Standard| 100| 4| ipho| T|large display| N| 1720| |analytical| sd card| apps| 45| M| 2| 89460| N| 53| Single| null| 12| 2| 23| 0| CC| FreeLocal| Standard| 36| 2| ipho| T| more storage| N| 1886| |frustrated| technical| support| 52| F| 2| 67388| N| 53| Married| null| 5| 1| 91| 1| CC| Budget| Standard| 96| 3| ipho| T|android phone| N| 1590| |analytical| new number|customer care| 53| F| 1| 57063| Y| 52| Married| null| 17| 0| 81| 0| CH| Budget| Standard| 98| 4| ipho| F|android phone| N| 1205| |frustrated| technical| support| 54| F| 2| 84166| N| 54| Married| null| 3| 0| 12| 1| CC| Budget| Standard| 15| 2| ipho| T| dual sim| Y| 1625| | neutral| supervisor| delegation| 61| M| 2|100020| N| 50| Single| null| 21| 0| 293| 0| CH| Budget| Standard| 315| 4| ipho| T|android phone| N| 1696| +----------+-----------------+-------------+---+------+--------+------+------------+---+-------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----------+-----+-------------+----+--------+ only showing top 10 rows root |-- Sentiment: string (nullable = true) |-- Keyword_Component: string (nullable = true) |-- Keyword_Query: string (nullable = true) |-- ID: string (nullable = true) |-- Gender: string (nullable = true) |-- Children: integer (nullable = true) |-- Income: decimal(10,0) (nullable = true) |-- CarOwnership: string (nullable = true) |-- Age: integer (nullable = true) |-- MaritalStatus: string (nullable = true) |-- zipcode: string (nullable = true) |-- LongDistance: decimal(10,0) (nullable = true) |-- International: decimal(10,0) (nullable = true) |-- Local: decimal(10,0) (nullable = true) |-- Dropped: integer (nullable = true) |-- Paymethod: string (nullable = true) |-- LocalBilltype: string (nullable = true) |-- LongDistanceBilltype: string (nullable = true) |-- Usage: decimal(10,0) (nullable = true) |-- RatePlan: integer (nullable = true) |-- DeviceOwned: string (nullable = true) |-- CHURN: string (nullable = true) |-- Preference: string (nullable = true) |-- OMPN: string (nullable = true) |-- SMSCount: integer (nullable = true) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sentiment Keyword_Component Keyword_Query ID Gender Children Income CarOwnership Age MaritalStatus ... Paymethod LocalBilltype LongDistanceBilltype Usage RatePlan DeviceOwned CHURN Preference OMPN SMSCount 0 analytical sd card apps 24 F 2 47902 N 26 Married ... Auto FreeLocal Standard 72 2 moto F more storage N 1864 1 frustrated battery new phone 35 F 0 78851 N 48 Single ... CC FreeLocal Standard 29 4 ipho T dual sim Y 1173 2 frustrated battery new phone 37 F 0 83892 Y 61 Married ... CH Budget Standard 74 4 ipho T android phone N 841 3 frustrated battery new phone 38 F 2 28221 N 38 Married ... CC FreeLocal Standard 39 4 ipho T windows phone N 1779 4 frustrated battery new phone 40 F 0 28589 N 15 Single ... CC FreeLocal Standard 100 4 ipho T large display N 1720 5 analytical sd card apps 45 M 2 89460 N 53 Single ... CC FreeLocal Standard 36 2 ipho T more storage N 1886 6 frustrated technical support 52 F 2 67388 N 53 Married ... CC Budget Standard 96 3 ipho T android phone N 1590 7 analytical new number customer care 53 F 1 57063 Y 52 Married ... CH Budget Standard 98 4 ipho F android phone N 1205 8 frustrated technical support 54 F 2 84166 N 54 Married ... CC Budget Standard 15 2 ipho T dual sim Y 1625 9 neutral supervisor delegation 61 M 2 100020 N 50 Single ... CH Budget Standard 315 4 ipho T android phone N 1696 10 frustrated car adapter battery 62 F 2 45288 Y 29 Married ... CC Budget Standard 3 3 ipho F dual sim N 1579 11 frustrated technical support 63 F 2 59613 N 34 Married ... CC Budget Intnl_discount 176 1 ipho T dual sim Y 1662 12 rows \u00d7 25 columns The following brunel based visualization can also be performed from Data Refinery. Shown here to get the feel for APIs %brunel data('pandas_df_callnotes_campaign_churn') bar y(#count) stack polar color(Sentiment) sort(#count) label(Sentiment, ' (', #count, '%)') tooltip(#all) percent(#count) legends(none) \u0002wzxhzdk:13\u0003 \u0002wzxhzdk:14\u0003 ##### The following cell shows an example of how pixiedust can be used to build interactive dashboards, and how it can be exported out \u0002wzxhzdk:15\u0003 .pd_warning{display:none;} Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter ##### Building RandomForest based classifier \u0002wzxhzdk:16\u0003 \u0002wzxhzdk:17\u0003 ##### Split the dataset into training and test using 70:30 split ratio and build the model \u0002wzxhzdk:18\u0003 ##### Testing the test dataset \u0002wzxhzdk:19\u0003 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID CHURN label predictedLabel prediction probability 0 1006 T 1.0 T 1.0 [0.15866010494609709, 0.8413398950539029] 1 1231 F 0.0 F 0.0 [0.8565502761039058, 0.14344972389609417] 2 1239 F 0.0 F 0.0 [0.5644293832170035, 0.43557061678299647] 3 1292 F 0.0 F 0.0 [0.7184336545774561, 0.281566345422544] 4 1293 F 0.0 F 0.0 [0.7324334755270526, 0.26756652447294743] 5 1299 F 0.0 F 0.0 [0.7425781475656279, 0.25742185243437216] ##### Model Evaluation \u0002wzxhzdk:20\u0003 Precision model1 = 0.88. Area under ROC curve = 0.86. ##### Deployment in Watson Machine Learning. Note that there are multiple ways of deploying a model in WML. Reading the credentials of your service from an external directory or at least a file, is better than hard coding in the code. First cell below loads the credentials, and the next cell instantiates the service. \u0002wzxhzdk:21\u0003 02f481c2-7717-4e79-b60f-3c1214b5f374 \u0002wzxhzdk:22\u0003 modelType: sparkml-model-2.1 creationTime: 2018-01-29 05:00:35.460000+00:00 modelVersionHref: https://ibm-watson-ml.mybluemix.net/v2/artifacts/models/a6d5cae3-c31a-46a6-b160-af82d11a6df9/versions/fefe0848-8426-463c-984f-0137e28ad56e label: CHURN","title":"Evaluate and predict customer churn"},{"location":"ml/CustomerChurnAnalysisCl-bpull/#evaluate-and-predict-customer-churn","text":"This notebook is an adaptation from the work done by Sidney Phoon and Eleva Lowery with the following modifications: Use datasets persisted in DB2 Warehouse running on ICP Use additional datasets from multiple datasources such as Cloudant on public cloud, and Amazon S3 Use of Watson Data Platform for data ingestion and preparation (using Data Refinery) Deploy and run the notebook on DSX enterprise running on IBM Cloud Private (ICP) Run spark Machine learning job on ICP as part of the worker nodes. Document some actions for a beginner data scienctist / developer who wants to understand what's going on. The web application was separated in another git project The goal is to demonstrate how to build a predictive model with Spark machine learning API (SparkML) to predict customer churn, and deploy it for scoring in Machine Learning (ML) running on ICP or within IBM public Cloud, Watson Machine Learning service.","title":"Evaluate and predict customer churn"},{"location":"ml/CustomerChurnAnalysisCl-bpull/#scope","text":"A lot of industries have the issue of customers moving to competitors when the product differentiation is not that important, or there is some customer support issues. One industry illustrating this problem is the telecom industry with mobile, internet and IP TV product offerings.","title":"Scope"},{"location":"ml/CustomerChurnAnalysisCl-bpull/#note-book-explanations","text":"The notebook aims to follow the classical data science modeling steps: load the data prepare the data analyze the data (iterate on those two activities) build a model validate the accuracy of the model deploy the model consume the model as a service This jupyter notebook uses Apache Spark to run the machine learning jobs to build decision trees and random forest classifier to assess when a customer is at risk to move to competitor. Apache Spark offers a Python module called pyspark to operate on data and use ML constructs.","title":"Note book explanations"},{"location":"ml/CustomerChurnAnalysisCl-bpull/#start-by-all-imports","text":"As a best practices for notebook implementation is to do the import at the top of the notebook. Spark SQLContext a spark module to process structured data spark conf to access Spark cluster configuration and then be able to execute queries jaydebeapi is used to connect to the DB 2 warehouse where customer data are persisted. We assume they are loaded. ibmdbpy interface for data manipulation and access to in-database algorithms in IBM dashDB and IBM DB2. pandas Python super library for data analysis brunel API and tool to visualize data quickly. * pixiedust Visualize data inside Jupyter notebooks The first cell below is to execute some system commands to update the kernel with updated dependant library. # Needed to access data from IBM Cloud Object Storage !pip install --upgrade ibm-cos-sdk Requirement already up-to-date: ibm-cos-sdk in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages Requirement already up-to-date: ibm-cos-sdk-core==2.*,>=2.0.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from ibm-cos-sdk) Requirement already up-to-date: ibm-cos-sdk-s3transfer==2.*,>=2.0.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from ibm-cos-sdk) Requirement already up-to-date: python-dateutil<3.0.0,>=2.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk) Requirement already up-to-date: docutils>=0.10 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk) Requirement already up-to-date: jmespath<1.0.0,>=0.7.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk) Requirement already up-to-date: six>=1.5 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk) # Required for accessing data on IBM Cloud Object Storage !pip install --upgrade boto3 Requirement already up-to-date: boto3 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages Requirement already up-to-date: botocore<1.9.0,>=1.8.36 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from boto3) Requirement already up-to-date: s3transfer<0.2.0,>=0.1.10 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from boto3) Requirement already up-to-date: jmespath<1.0.0,>=0.7.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from boto3) Requirement already up-to-date: python-dateutil<3.0.0,>=2.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from botocore<1.9.0,>=1.8.36->boto3) Requirement already up-to-date: docutils>=0.10 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from botocore<1.9.0,>=1.8.36->boto3) Requirement already up-to-date: six>=1.5 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.9.0,>=1.8.36->boto3) # The following will be needed if you want to download datasets from outside the WDP environment for any reason. But we won't be needing this in the current sample exercise !pip install --upgrade wget Requirement already up-to-date: wget in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages # Library required for pixiedust - a visualization and dashboarding framework !pip install --user --upgrade pixiedust Requirement already up-to-date: pixiedust in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages Requirement already up-to-date: astunparse in /usr/local/src/conda3_runtime.v27/4.1.1/lib/python3.5/site-packages (from pixiedust) Requirement already up-to-date: lxml in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from pixiedust) Requirement already up-to-date: mpld3 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from pixiedust) Requirement already up-to-date: geojson in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from pixiedust) Requirement already up-to-date: markdown in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from pixiedust) Requirement already up-to-date: wheel<1.0,>=0.23.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from astunparse->pixiedust) Requirement already up-to-date: six<2.0,>=1.6.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from astunparse->pixiedust) # Needed to deploy the model on Watson Machine Learning Service !pip install --upgrade watson-machine-learning-client Requirement already up-to-date: watson-machine-learning-client in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages Requirement already up-to-date: lomond in /usr/local/src/conda3_runtime.v27/4.1.1/lib/python3.5/site-packages (from watson-machine-learning-client) Requirement already up-to-date: tabulate in /usr/local/src/conda3_runtime.v27/4.1.1/lib/python3.5/site-packages (from watson-machine-learning-client) Requirement already up-to-date: tqdm in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from watson-machine-learning-client) Requirement already up-to-date: requests in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from watson-machine-learning-client) Requirement already up-to-date: urllib3 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from watson-machine-learning-client) Requirement already up-to-date: certifi in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from watson-machine-learning-client) Requirement already up-to-date: pandas in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from watson-machine-learning-client) Requirement already up-to-date: six>=1.10.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from lomond->watson-machine-learning-client) Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from requests->watson-machine-learning-client) Requirement already up-to-date: idna<2.7,>=2.5 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from requests->watson-machine-learning-client) Requirement already up-to-date: python-dateutil>=2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from pandas->watson-machine-learning-client) Requirement already up-to-date: numpy>=1.9.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from pandas->watson-machine-learning-client) Requirement already up-to-date: pytz>=2011k in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s796-0c7a8ec08c3a1c-8b16cafc4fd4/.local/lib/python3.5/site-packages (from pandas->watson-machine-learning-client) import pyspark import pandas as pd import brunel import numpy as np from pyspark.sql import SQLContext from pyspark.conf import SparkConf from pyspark.sql import SparkSession from pyspark.sql.types import DoubleType from pyspark.sql.types import DecimalType from pyspark.sql.types import IntegerType from pyspark.sql.types import LongType from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString from pyspark.ml import Pipeline from pyspark.ml.feature import VectorAssembler from pyspark.ml.classification import RandomForestClassifier from pyspark.ml.evaluation import BinaryClassificationEvaluator from pixiedust.display import * import ibm_boto3","title":"Start by all imports"},{"location":"ml/CustomerChurnAnalysisCl-bpull/#load-data-using-watson-data-platform-wdp","text":"Watson Data platform (WDP) can be used to bring in data from multiple sources including but not limited to, files, data stores on cloud as well as on premises. WDP includes features to connect to data sources, bring in the data, refine, and then perform analytics. In this sample are using WDP approach. We connect to Amazon S3, Cloudant on IBM public (or private) cloud, and DB2 Data Warehouse on IBM public or private cloud. Once we bring in the data, we refine / cleanse them using Data Refinery and export the result as a CSV file for training analytical model. These steps are already accomplished using WDP so that we can start loading that data for analytics in the following cells. # Load customer information along with churn status. We read this from the CSV file prepared for training purposes import ibmos2spark # @hidden_cell credentials = { 'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com', 'api_key': '0iYlgkldtINM5my_68CDOlxU5Ts0E7dn0wJkYCs6bqOp', 'service_id': 'iam-ServiceId-de6a2704-3436-4927-8ceb-a2e3dfc3288e', 'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token'} configuration_name = 'os_d21bf0a6588f494d822173729799c934_configs' cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos') from pyspark.sql import SparkSession spark = SparkSession.builder.getOrCreate() df_customer_transactions = spark.read\\ .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\ .option('header', 'true')\\ .load(cos.url('data_asset/YTc2MDUyNjAtYzg5OC00MDk5LTgyZGItYThlZjI4ZDczNmZl_data_6283e5af-2e69-426e-b327-1a3bbc23ded3.csv', 'customerchurnproject86ce96f6a9384a669a14bd5dd9b3028e')) df_customer_transactions.take(5) df_customer_transactions.printSchema() root |-- ID: string (nullable = true) |-- Gender: string (nullable = true) |-- Status: string (nullable = true) |-- Children: string (nullable = true) |-- Est Income: string (nullable = true) |-- Car Owner: string (nullable = true) |-- Age: string (nullable = true) |-- Marital Status: string (nullable = true) |-- zipcode: string (nullable = true) |-- LongDistance: string (nullable = true) |-- International: string (nullable = true) |-- Local: string (nullable = true) |-- Dropped: string (nullable = true) |-- Paymethod: string (nullable = true) |-- LocalBilltype: string (nullable = true) |-- LongDistanceBilltype: string (nullable = true) |-- Usage: string (nullable = true) |-- RatePlan: string (nullable = true) |-- DeviceOwned: string (nullable = true) |-- CHURN: string (nullable = true) # Load the call notes dataset df_call_notes = spark.read\\ .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\ .option('header', 'true')\\ .load(cos.url('data_asset/MTIwYWQwZDctMmMyMC00NTkzLWI3YjItNDI4NGVjMzhlYjA5_data_0601330c-2331-46df-b9bc-df5ab0eb9a41.csv', 'customerchurnproject86ce96f6a9384a669a14bd5dd9b3028e')) df_call_notes.take(5) df_call_notes.describe() df_call_notes.show(25) +--------------------+---+------------+-------------+----------+ | Comments| ID| Keyword1| Keyword2|Sentiments| +--------------------+---+------------+-------------+----------+ |Asked about stora...| 1| sim| storage|analytical| |Asked about low-e...| 6|basic config| smartphone|analytical| |Dissatisfied with...| 8| promotion| rebate|frustrated| |Asked about low-e...| 11|basic config| smartphone|analytical| |Asked about low-e...| 14|basic config| smartphone|analytical| |Dissatisfied with...| 17| promotion| rebate|frustrated| |Asked about low-e...| 18|basic config| smartphone|analytical| |Asked about low-e...| 21|basic config| smartphone|analytical| |Upset about the d...| 22| data plan| speed|frustrated| |Asked about low-e...| 23|basic config| smartphone|analytical| |Asked how to inst...| 24| sd card| apps|analytical| |Asked how to inst...| 29| sd card| apps|analytical| |Said his battery ...| 35| battery| new phone|frustrated| |Said his battery ...| 36| battery| new phone|frustrated| |Said his battery ...| 37| battery| new phone|frustrated| |Said his battery ...| 38| battery| new phone|frustrated| |Said his battery ...| 40| battery| new phone|frustrated| |He charges it and...| 42| charging| battery|frustrated| |Asked how to inst...| 45| sd card| apps|analytical| |Asked how to inst...| 48| sd card| apps|analytical| |He expected signi...| 52| technical| support|frustrated| |He asked for a ne...| 53| new number|customer care|analytical| |He expected signi...| 54| technical| support|frustrated| |He expected signi...| 60| technical| support|frustrated| |transferred to th...| 61| supervisor| delegation| neutral| +--------------------+---+------------+-------------+----------+ only showing top 25 rows # Load customer campaign responses dataset df_campaign_responses = spark.read\\ .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\ .option('header', 'true')\\ .load(cos.url('data_asset/MmQwNjUxYmUtYjU3My00ZmViLTk0YWEtYTc5ZWExZmU3Mjg3_data_e26e9c54-7143-449c-bbf4-71998290d86a.csv', 'customerchurnproject86ce96f6a9384a669a14bd5dd9b3028e')) df_campaign_responses.take(5) [Row(CUSTOMERID='6', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1807', Owns Multiple Lines='0'), Row(CUSTOMERID='8', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1364', Owns Multiple Lines='0'), Row(CUSTOMERID='18', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1407', Owns Multiple Lines='0'), Row(CUSTOMERID='29', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1043', Owns Multiple Lines='0'), Row(CUSTOMERID='37', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='841', Owns Multiple Lines='0')]","title":"Load data using Watson Data Platform (WDP)"},{"location":"ml/CustomerChurnAnalysisCl-bpull/#the-next-few-steps-involve-a-series-of-data-preparation-tasks-such-as-filling-the-missing-values-joining-datasets-etc-the-following-cell-fills-the-null-values-for-average-sms-count-note-that-this-can-be-accomplished-by-using-data-refinery-but-the-following-snippet-is-shown-to-iullustrate-the-api-way-of-accomplishing-the-same","text":"df_campaign_responses = df_campaign_responses.na.fill({'Ave Text Msgs':'0'})","title":"The next few steps involve a series of data preparation tasks such as filling the missing values, joining datasets etc. The following cell fills the null values for average SMS count. Note that this can be accomplished by using Data Refinery, but the following snippet is shown to iullustrate the API way of accomplishing the same."},{"location":"ml/CustomerChurnAnalysisCl-bpull/#in-the-following-cell-we-join-some-of-our-data-sources-note-that-we-could-have-done-some-of-these-using-data-refinery-on-watson-data-platform-using-gui-support","text":"data_joined_callnotes_churn = df_call_notes.join(df_customer_transactions,df_call_notes['ID']==df_customer_transactions['ID'],'inner').select(df_call_notes['Sentiments'],df_call_notes['Keyword1'],df_call_notes['Keyword2'],df_customer_transactions['*']) data_joined_callnotes_churn_campaign = df_campaign_responses.join(data_joined_callnotes_churn,df_campaign_responses['CUSTOMERID']==data_joined_callnotes_churn['ID'],'inner').select(data_joined_callnotes_churn['*'],df_campaign_responses['Preference'],df_campaign_responses['Owns Multiple Phone Numbers'],df_campaign_responses['Ave Text Msgs']) data_joined_callnotes_churn_campaign.take(10) [Row(Sentiments='analytical', Keyword1='sd card', Keyword2='apps', ID='24', Gender='F', Status='M', Children='2', Est Income='47902.00', Car Owner='N', Age='26.033333', Marital Status='Married', zipcode=None, LongDistance='17.44', International='4.94', Local='49.92', Dropped='1', Paymethod='Auto', LocalBilltype='FreeLocal', LongDistanceBilltype='Standard', Usage='72.31', RatePlan='2', DeviceOwned='moto', CHURN='F', Preference='more storage', Owns Multiple Phone Numbers='N', Ave Text Msgs='1864'), Row(Sentiments='frustrated', Keyword1='battery', Keyword2='new phone', ID='35', Gender='F', Status='S', Children='0', Est Income='78851.30', Car Owner='N', Age='48.373333', Marital Status='Single', zipcode=None, LongDistance='0.37', International='0.00', Local='28.66', Dropped='0', Paymethod='CC', LocalBilltype='FreeLocal', LongDistanceBilltype='Standard', Usage='29.04', RatePlan='4', DeviceOwned='ipho', CHURN='T', Preference='dual sim', Owns Multiple Phone Numbers='Y', Ave Text Msgs='1173'), Row(Sentiments='frustrated', Keyword1='battery', Keyword2='new phone', ID='37', Gender='F', Status='M', Children='0', Est Income='83891.90', Car Owner='Y', Age='61.020000', Marital Status='Married', zipcode=None, LongDistance='28.92', International='0.00', Local='45.47', Dropped='0', Paymethod='CH', LocalBilltype='Budget', LongDistanceBilltype='Standard', Usage='74.40', RatePlan='4', DeviceOwned='ipho', CHURN='T', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='841'), Row(Sentiments='frustrated', Keyword1='battery', Keyword2='new phone', ID='38', Gender='F', Status='M', Children='2', Est Income='28220.80', Car Owner='N', Age='38.766667', Marital Status='Married', zipcode=None, LongDistance='26.49', International='0.00', Local='12.46', Dropped='0', Paymethod='CC', LocalBilltype='FreeLocal', LongDistanceBilltype='Standard', Usage='38.95', RatePlan='4', DeviceOwned='ipho', CHURN='T', Preference='windows phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1779'), Row(Sentiments='frustrated', Keyword1='battery', Keyword2='new phone', ID='40', Gender='F', Status='S', Children='0', Est Income='28589.10', Car Owner='N', Age='15.600000', Marital Status='Single', zipcode=None, LongDistance='13.19', International='0.00', Local='87.09', Dropped='0', Paymethod='CC', LocalBilltype='FreeLocal', LongDistanceBilltype='Standard', Usage='100.28', RatePlan='4', DeviceOwned='ipho', CHURN='T', Preference='large display', Owns Multiple Phone Numbers='N', Ave Text Msgs='1720'), Row(Sentiments='analytical', Keyword1='sd card', Keyword2='apps', ID='45', Gender='M', Status='S', Children='2', Est Income='89459.90', Car Owner='N', Age='53.280000', Marital Status='Single', zipcode=None, LongDistance='11.54', International='1.61', Local='22.90', Dropped='0', Paymethod='CC', LocalBilltype='FreeLocal', LongDistanceBilltype='Standard', Usage='36.05', RatePlan='2', DeviceOwned='ipho', CHURN='T', Preference='more storage', Owns Multiple Phone Numbers='N', Ave Text Msgs='1886'), Row(Sentiments='frustrated', Keyword1='technical', Keyword2='support', ID='52', Gender='F', Status='M', Children='2', Est Income='67388.00', Car Owner='N', Age='53.120000', Marital Status='Married', zipcode=None, LongDistance='4.79', International='0.50', Local='91.04', Dropped='1', Paymethod='CC', LocalBilltype='Budget', LongDistanceBilltype='Standard', Usage='96.33', RatePlan='3', DeviceOwned='ipho', CHURN='T', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1590'), Row(Sentiments='analytical', Keyword1='new number', Keyword2='customer care', ID='53', Gender='F', Status='M', Children='1', Est Income='57063.00', Car Owner='Y', Age='52.333333', Marital Status='Married', zipcode=None, LongDistance='16.79', International='0.00', Local='81.30', Dropped='0', Paymethod='CH', LocalBilltype='Budget', LongDistanceBilltype='Standard', Usage='98.10', RatePlan='4', DeviceOwned='ipho', CHURN='F', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1205'), Row(Sentiments='frustrated', Keyword1='technical', Keyword2='support', ID='54', Gender='F', Status='M', Children='2', Est Income='84166.10', Car Owner='N', Age='54.013333', Marital Status='Married', zipcode=None, LongDistance='3.28', International='0.00', Local='11.74', Dropped='1', Paymethod='CC', LocalBilltype='Budget', LongDistanceBilltype='Standard', Usage='15.02', RatePlan='2', DeviceOwned='ipho', CHURN='T', Preference='dual sim', Owns Multiple Phone Numbers='Y', Ave Text Msgs='1625'), Row(Sentiments='neutral', Keyword1='supervisor', Keyword2='delegation', ID='61', Gender='M', Status='S', Children='2', Est Income='100020.00', Car Owner='N', Age='50.000000', Marital Status='Single', zipcode=None, LongDistance='21.37', International='0.00', Local='293.24', Dropped='0', Paymethod='CH', LocalBilltype='Budget', LongDistanceBilltype='Standard', Usage='314.62', RatePlan='4', DeviceOwned='ipho', CHURN='T', Preference='android phone', Owns Multiple Phone Numbers='N', Ave Text Msgs='1696')]","title":"In the following cell we join some of our data sources. Note that we could have done some of these using Data Refinery on Watson Data Platform using GUI support."},{"location":"ml/CustomerChurnAnalysisCl-bpull/#the-following-code-block-is-intended-to-get-a-feel-for-spark-dataframe-apis-we-attempt-to-fix-some-of-the-column-titles-to-promote-readability-and-also-remove-a-duplicate-column-status-and-marital-status-are-the-same-finally-convert-the-dataframe-to-python-pandas-structure-for-visualization-since-all-are-string-types-from-the-csv-file-let-us-change-some-of-them-to-other-types","text":"# Change some column names data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"Sentiments\", \"Sentiment\").withColumnRenamed(\"Owns Multiple Phone Numbers\",\"OMPN\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"Keyword1\", \"Keyword_Component\").withColumnRenamed(\"Keyword2\",\"Keyword_Query\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"Ave Text Msgs\", \"SMSCount\").withColumnRenamed(\"Car Owner\",\"CarOwnership\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"Marital Status\", \"MaritalStatus\").withColumnRenamed(\"Est Income\",\"Income\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.drop('Status') # Change some of the data types data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"Children\", data_joined_callnotes_churn_campaign[\"Children\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"Income\", data_joined_callnotes_churn_campaign[\"Income\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"Age\", data_joined_callnotes_churn_campaign[\"Age\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"LongDistance\", data_joined_callnotes_churn_campaign[\"LongDistance\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"International\", data_joined_callnotes_churn_campaign[\"International\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"Local\", data_joined_callnotes_churn_campaign[\"Local\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"Dropped\", data_joined_callnotes_churn_campaign[\"Dropped\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"Usage\", data_joined_callnotes_churn_campaign[\"Usage\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"RatePlan\", data_joined_callnotes_churn_campaign[\"RatePlan\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"SMSCount\", data_joined_callnotes_churn_campaign[\"SMSCount\"].cast(IntegerType())) data_joined_callnotes_churn_campaign.show(10) data_joined_callnotes_churn_campaign.printSchema() pandas_df_callnotes_campaign_churn = data_joined_callnotes_churn_campaign.toPandas() pandas_df_callnotes_campaign_churn.head(12) +----------+-----------------+-------------+---+------+--------+------+------------+---+-------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----------+-----+-------------+----+--------+ | Sentiment|Keyword_Component|Keyword_Query| ID|Gender|Children|Income|CarOwnership|Age|MaritalStatus|zipcode|LongDistance|International|Local|Dropped|Paymethod|LocalBilltype|LongDistanceBilltype|Usage|RatePlan|DeviceOwned|CHURN| Preference|OMPN|SMSCount| +----------+-----------------+-------------+---+------+--------+------+------------+---+-------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----------+-----+-------------+----+--------+ |analytical| sd card| apps| 24| F| 2| 47902| N| 26| Married| null| 17| 5| 50| 1| Auto| FreeLocal| Standard| 72| 2| moto| F| more storage| N| 1864| |frustrated| battery| new phone| 35| F| 0| 78851| N| 48| Single| null| 0| 0| 29| 0| CC| FreeLocal| Standard| 29| 4| ipho| T| dual sim| Y| 1173| |frustrated| battery| new phone| 37| F| 0| 83892| Y| 61| Married| null| 29| 0| 45| 0| CH| Budget| Standard| 74| 4| ipho| T|android phone| N| 841| |frustrated| battery| new phone| 38| F| 2| 28221| N| 38| Married| null| 26| 0| 12| 0| CC| FreeLocal| Standard| 39| 4| ipho| T|windows phone| N| 1779| |frustrated| battery| new phone| 40| F| 0| 28589| N| 15| Single| null| 13| 0| 87| 0| CC| FreeLocal| Standard| 100| 4| ipho| T|large display| N| 1720| |analytical| sd card| apps| 45| M| 2| 89460| N| 53| Single| null| 12| 2| 23| 0| CC| FreeLocal| Standard| 36| 2| ipho| T| more storage| N| 1886| |frustrated| technical| support| 52| F| 2| 67388| N| 53| Married| null| 5| 1| 91| 1| CC| Budget| Standard| 96| 3| ipho| T|android phone| N| 1590| |analytical| new number|customer care| 53| F| 1| 57063| Y| 52| Married| null| 17| 0| 81| 0| CH| Budget| Standard| 98| 4| ipho| F|android phone| N| 1205| |frustrated| technical| support| 54| F| 2| 84166| N| 54| Married| null| 3| 0| 12| 1| CC| Budget| Standard| 15| 2| ipho| T| dual sim| Y| 1625| | neutral| supervisor| delegation| 61| M| 2|100020| N| 50| Single| null| 21| 0| 293| 0| CH| Budget| Standard| 315| 4| ipho| T|android phone| N| 1696| +----------+-----------------+-------------+---+------+--------+------+------------+---+-------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----------+-----+-------------+----+--------+ only showing top 10 rows root |-- Sentiment: string (nullable = true) |-- Keyword_Component: string (nullable = true) |-- Keyword_Query: string (nullable = true) |-- ID: string (nullable = true) |-- Gender: string (nullable = true) |-- Children: integer (nullable = true) |-- Income: decimal(10,0) (nullable = true) |-- CarOwnership: string (nullable = true) |-- Age: integer (nullable = true) |-- MaritalStatus: string (nullable = true) |-- zipcode: string (nullable = true) |-- LongDistance: decimal(10,0) (nullable = true) |-- International: decimal(10,0) (nullable = true) |-- Local: decimal(10,0) (nullable = true) |-- Dropped: integer (nullable = true) |-- Paymethod: string (nullable = true) |-- LocalBilltype: string (nullable = true) |-- LongDistanceBilltype: string (nullable = true) |-- Usage: decimal(10,0) (nullable = true) |-- RatePlan: integer (nullable = true) |-- DeviceOwned: string (nullable = true) |-- CHURN: string (nullable = true) |-- Preference: string (nullable = true) |-- OMPN: string (nullable = true) |-- SMSCount: integer (nullable = true) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sentiment Keyword_Component Keyword_Query ID Gender Children Income CarOwnership Age MaritalStatus ... Paymethod LocalBilltype LongDistanceBilltype Usage RatePlan DeviceOwned CHURN Preference OMPN SMSCount 0 analytical sd card apps 24 F 2 47902 N 26 Married ... Auto FreeLocal Standard 72 2 moto F more storage N 1864 1 frustrated battery new phone 35 F 0 78851 N 48 Single ... CC FreeLocal Standard 29 4 ipho T dual sim Y 1173 2 frustrated battery new phone 37 F 0 83892 Y 61 Married ... CH Budget Standard 74 4 ipho T android phone N 841 3 frustrated battery new phone 38 F 2 28221 N 38 Married ... CC FreeLocal Standard 39 4 ipho T windows phone N 1779 4 frustrated battery new phone 40 F 0 28589 N 15 Single ... CC FreeLocal Standard 100 4 ipho T large display N 1720 5 analytical sd card apps 45 M 2 89460 N 53 Single ... CC FreeLocal Standard 36 2 ipho T more storage N 1886 6 frustrated technical support 52 F 2 67388 N 53 Married ... CC Budget Standard 96 3 ipho T android phone N 1590 7 analytical new number customer care 53 F 1 57063 Y 52 Married ... CH Budget Standard 98 4 ipho F android phone N 1205 8 frustrated technical support 54 F 2 84166 N 54 Married ... CC Budget Standard 15 2 ipho T dual sim Y 1625 9 neutral supervisor delegation 61 M 2 100020 N 50 Single ... CH Budget Standard 315 4 ipho T android phone N 1696 10 frustrated car adapter battery 62 F 2 45288 Y 29 Married ... CC Budget Standard 3 3 ipho F dual sim N 1579 11 frustrated technical support 63 F 2 59613 N 34 Married ... CC Budget Intnl_discount 176 1 ipho T dual sim Y 1662 12 rows \u00d7 25 columns","title":"The following code block is intended to get a feel for Spark DataFrame APIs. We attempt to fix some of the column titles to promote readability, and also remove a duplicate column (Status and Marital Status are the same).  Finally convert the DataFrame to Python Pandas structure for visualization. Since all are string types from the CSV file, let us change some of them to other types"},{"location":"ml/CustomerChurnAnalysisCl-bpull/#the-following-brunel-based-visualization-can-also-be-performed-from-data-refinery-shown-here-to-get-the-feel-for-apis","text":"%brunel data('pandas_df_callnotes_campaign_churn') bar y(#count) stack polar color(Sentiment) sort(#count) label(Sentiment, ' (', #count, '%)') tooltip(#all) percent(#count) legends(none) \u0002wzxhzdk:13\u0003 \u0002wzxhzdk:14\u0003 ##### The following cell shows an example of how pixiedust can be used to build interactive dashboards, and how it can be exported out \u0002wzxhzdk:15\u0003 .pd_warning{display:none;} Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter ##### Building RandomForest based classifier \u0002wzxhzdk:16\u0003 \u0002wzxhzdk:17\u0003 ##### Split the dataset into training and test using 70:30 split ratio and build the model \u0002wzxhzdk:18\u0003 ##### Testing the test dataset \u0002wzxhzdk:19\u0003 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID CHURN label predictedLabel prediction probability 0 1006 T 1.0 T 1.0 [0.15866010494609709, 0.8413398950539029] 1 1231 F 0.0 F 0.0 [0.8565502761039058, 0.14344972389609417] 2 1239 F 0.0 F 0.0 [0.5644293832170035, 0.43557061678299647] 3 1292 F 0.0 F 0.0 [0.7184336545774561, 0.281566345422544] 4 1293 F 0.0 F 0.0 [0.7324334755270526, 0.26756652447294743] 5 1299 F 0.0 F 0.0 [0.7425781475656279, 0.25742185243437216] ##### Model Evaluation \u0002wzxhzdk:20\u0003 Precision model1 = 0.88. Area under ROC curve = 0.86. ##### Deployment in Watson Machine Learning. Note that there are multiple ways of deploying a model in WML. Reading the credentials of your service from an external directory or at least a file, is better than hard coding in the code. First cell below loads the credentials, and the next cell instantiates the service. \u0002wzxhzdk:21\u0003 02f481c2-7717-4e79-b60f-3c1214b5f374 \u0002wzxhzdk:22\u0003 modelType: sparkml-model-2.1 creationTime: 2018-01-29 05:00:35.460000+00:00 modelVersionHref: https://ibm-watson-ml.mybluemix.net/v2/artifacts/models/a6d5cae3-c31a-46a6-b160-af82d11a6df9/versions/fefe0848-8426-463c-984f-0137e28ad56e label: CHURN","title":"The following brunel based visualization can also be performed from Data Refinery. Shown here to get the feel for APIs"},{"location":"ml/CustomerChurnAnalysisDSXICP/","text":"Evaluate and predict customer churn This notebook is an adaptation from the work done by Sidney Phoon and Eleva Lowery with the following modifications: Use datasets persisted in DB2 Warehouse running on ICP Deploy and run the notebook on DSX local running on IBM Cloud Private (ICP) Run spark Machine learning job on ICP as part of the worker nodes. Document some actions for a beginner data scientist / developer who wants to understand what's going on. The goal is to demonstrate how to build a predictive model with Spark machine learning API (SparkML) to predict customer churn, and deploy it for scoring in Machine Learning (ML) running on ICP. There is an equivalent notebook to run on Watson Data Platform and Watson Machine Learning. Scope A lot of industries have the issue of customers moving to competitors when the product differentiation is not that important, or there is some customer support issues. One industry illustrating this problem is the telecom industry with mobile, internet and IP TV product offerings. Note book explanations The notebook aims to follow the classical data science modeling steps: load the data prepare the data analyze the data (iterate on those two activities) build a model validate the accuracy of the model deploy the model consume the model as a service This jupyter notebook uses Apache Spark to run the machine learning jobs to build decision trees using random forest classifier to assess when a customer is at risk to move to competitor. Apache Spark offers a Python module called pyspark to operate on data and use ML constructs. Start by all imports As a best practices for notebook implementation is to do the import at the top of the notebook. Spark SQLContext a spark module to process structured data spark conf to access Spark cluster configuration and then be able to execute queries pandas Python super library for data analysis brunel API and tool to visualize data quickly. * pixiedust Visualize data inside Jupyter notebooks The first cell below is to execute some system commands to update the kernel with updated dependant library. # Library required for pixiedust - a visualization and dashboarding framework !pip install --user --upgrade pixiedust Requirement already up-to-date: pixiedust in /user-home/1002/.local/lib/python2.7/site-packages Requirement already up-to-date: astunparse in /user-home/1002/.local/lib/python2.7/site-packages (from pixiedust) Requirement already up-to-date: mpld3 in /opt/conda/lib/python2.7/site-packages (from pixiedust) Requirement already up-to-date: markdown in /user-home/1002/.local/lib/python2.7/site-packages (from pixiedust) Requirement already up-to-date: geojson in /user-home/1002/.local/lib/python2.7/site-packages (from pixiedust) Requirement already up-to-date: lxml in /user-home/1002/.local/lib/python2.7/site-packages (from pixiedust) Requirement already up-to-date: six<2.0,>=1.6.1 in /user-home/1002/.local/lib/python2.7/site-packages (from astunparse->pixiedust) Requirement already up-to-date: wheel<1.0,>=0.23.0 in /user-home/1002/.local/lib/python2.7/site-packages (from astunparse->pixiedust) import pyspark import pandas as pd import brunel import numpy as np from pyspark.sql import SQLContext from pyspark.conf import SparkConf from pyspark.sql import SparkSession from pyspark.sql.types import DoubleType from pyspark.sql.types import DecimalType from pyspark.sql.types import IntegerType from pyspark.sql.types import LongType from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString from pyspark.ml import Pipeline from pyspark.ml.feature import VectorAssembler from pyspark.ml.classification import RandomForestClassifier from pyspark.ml.evaluation import BinaryClassificationEvaluator from pixiedust.display import * Pixiedust database opened successfully <div style=\"margin:10px\"> <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\"> <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/> </a> <span>Pixiedust version 1.1.7.1</span> </div> Load data from DB2 DSX ICP can be used to bring in data from multiple sources including but not limited to, files, datastores on cloud as well as on premises. DSX ICP includes features to connect to data sources, bring in the data, refine, and then perform analytics. In this sample we connect to DB2 Data Warehouse deployed on ICP and bring data about customer, call notes and marketing campaign in. import dsx_core_utils dataSet = dsx_core_utils.get_remote_data_set_info('CUSTOMER') dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource']) print(dataSource) dbTableOrQuery = dataSet['schema'] + '.' + dataSet['table'] print(dbTableOrQuery) sparkSession = SparkSession(sc).builder.getOrCreate() df_customer_transactions = sparkSession.read.format(\"jdbc\").option(\"url\", dataSource['URL']).option(\"dbtable\",dbTableOrQuery).option(\"user\",'BLUADMIN').option(\"password\",\"changemeplease\").load() df_customer_transactions.show(5) df_customer_transactions.printSchema() {u'description': u'', u'URL': u'jdbc:db2://172.16.40.131:32166/BLUDB', 'driver_class': 'com.ibm.db2.jcc.DB2Driver', u'dsx_artifact_type': u'datasource', u'shared': True, u'type': u'DB2', u'name': u'CUSTOMER'} BLUADMIN.CUSTOMER +----+------+------+--------+----------+---------+---------+--------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+------+--------+-----------+-----+ | ID|GENDER|STATUS|CHILDREN|EST_INCOME|CAR_OWNER| AGE|MARITAL_STATUS|ZIPCODE|LONGDISTANCE|INTERNATIONAL|LOCAL|DROPPED|PAYMETHOD|LOCALBILLTYPE|LONGDISTANCEBILLTYPE| USAGE|RATEPLAN|DEVICEOWNED|CHURN| +----+------+------+--------+----------+---------+---------+--------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+------+--------+-----------+-----+ |1311| F| M| 0| 53432.20| Y|57.000000| Married| null| 12.56| 0|83.48| 0| CH| FreeLocal| Standard| 96.04| 1| ipho| F| |1312| M| M| 0| 78894.20| N|52.740000| Married| null| 2.00| 1|43.23| 0| CH| Budget| Intnl_discount| 46.65| 3| ipho| T| |1313| M| S| 1| 16432.10| Y|20.920000| Single| null| 20.41| 8|80.47| 0| CH| Budget| Intnl_discount|109.78| 3| ipho| T| |1314| M| M| 0| 62797.90| N|54.446667| Married| null| 8.32| 4|55.48| 0| CH| Budget| Intnl_discount| 68.09| 3| ipho| T| |1315| M| S| 1| 71329.90| Y|42.233333| Single| null| 14.27| 3|45.92| 0| CC| Budget| Standard| 63.75| 1| ipho| T| +----+------+------+--------+----------+---------+---------+--------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+------+--------+-----------+-----+ only showing top 5 rows root |-- ID: integer (nullable = true) |-- GENDER: string (nullable = true) |-- STATUS: string (nullable = true) |-- CHILDREN: integer (nullable = true) |-- EST_INCOME: decimal(8,2) (nullable = true) |-- CAR_OWNER: string (nullable = true) |-- AGE: decimal(14,6) (nullable = true) |-- MARITAL_STATUS: string (nullable = true) |-- ZIPCODE: integer (nullable = true) |-- LONGDISTANCE: decimal(6,2) (nullable = true) |-- INTERNATIONAL: integer (nullable = true) |-- LOCAL: decimal(7,2) (nullable = true) |-- DROPPED: integer (nullable = true) |-- PAYMETHOD: string (nullable = true) |-- LOCALBILLTYPE: string (nullable = true) |-- LONGDISTANCEBILLTYPE: string (nullable = true) |-- USAGE: decimal(7,2) (nullable = true) |-- RATEPLAN: integer (nullable = true) |-- DEVICEOWNED: string (nullable = true) |-- CHURN: string (nullable = true) dataSet = dsx_core_utils.get_remote_data_set_info('CALLNOTES') dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource']) print(dataSource) dbTableOrQuery = dataSet['schema'] + '.' + dataSet['table'] print(dbTableOrQuery) sparkSession = SparkSession(sc).builder.getOrCreate() df_call_notes = sparkSession.read.format(\"jdbc\").option(\"url\", dataSource['URL']).option(\"dbtable\",dbTableOrQuery).option(\"user\",'BLUADMIN').option(\"password\",\"changemeplease\").load() df_call_notes.show(5) df_call_notes.printSchema() {u'description': u'', u'URL': u'jdbc:db2://172.16.40.131:32166/BLUDB', 'driver_class': 'com.ibm.db2.jcc.DB2Driver', u'dsx_artifact_type': u'datasource', u'shared': True, u'type': u'DB2', u'name': u'CUSTOMER'} BLUADMIN.CALLNOTES +----+--------------------+----------+--------------+--------+ | ID| COMMENTS|SENTIMENTS| KEYWORD1|KEYWORD2| +----+--------------------+----------+--------------+--------+ |2253|Wants to change a...| null|update records| address| |2254|Wants to change a...| null|update records| address| |2255|Wants to change a...| null|update records| address| |2256|Wants to change a...| null|update records| address| |2257|Needed help figur...|analytical| billing| charges| +----+--------------------+----------+--------------+--------+ only showing top 5 rows root |-- ID: integer (nullable = true) |-- COMMENTS: string (nullable = true) |-- SENTIMENTS: string (nullable = true) |-- KEYWORD1: string (nullable = true) |-- KEYWORD2: string (nullable = true) dataSet = dsx_core_utils.get_remote_data_set_info('CAMPAIGNRESPONSES_EXPANDED') dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource']) print(dataSource) dbTableOrQuery = dataSet['schema'] + '.' + dataSet['table'] print(dbTableOrQuery) sparkSession = SparkSession(sc).builder.getOrCreate() df_campaign_responses = sparkSession.read.format(\"jdbc\").option(\"url\", dataSource['URL']).option(\"dbtable\",dbTableOrQuery).option(\"user\",'BLUADMIN').option(\"password\",\"changemeplease\").load() df_campaign_responses.show(5) df_campaign_responses.printSchema() {u'description': u'', u'URL': u'jdbc:db2://172.16.40.131:32166/BLUDB', 'driver_class': 'com.ibm.db2.jcc.DB2Driver', u'dsx_artifact_type': u'datasource', u'shared': True, u'type': u'DB2', u'name': u'CUSTOMER'} BLUADMIN.CAMPAIGNRESPONSES_EXPANDED +----+------------------+---------------------------+-------------------------------------+ | ID|RESPONDED_CAMPAIGN|OWNS_MULTIPLE_PHONE_NUMBERS|AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_| +----+------------------+---------------------------+-------------------------------------+ |3064| Kids Tablet| Y| 1561| |3077| Kids Tablet| Y| 1225| |3105| Kids Tablet| Y| 1661| |3106| Kids Tablet| N| 2498| |3108| Kids Tablet| N| 1118| +----+------------------+---------------------------+-------------------------------------+ only showing top 5 rows root |-- ID: integer (nullable = true) |-- RESPONDED_CAMPAIGN: string (nullable = true) |-- OWNS_MULTIPLE_PHONE_NUMBERS: string (nullable = true) |-- AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_: integer (nullable = true) Data Preparation The next few steps involve a series of data preparation tasks such as filling the missing values, joining datasets etc. The following cell fills the null values for average SMS count and replaces Nulls with spaces for other fields. df_campaign_responses = df_campaign_responses.na.fill({'AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_':'0'}) df_call_notes = df_call_notes.na.fill({'SENTIMENTS':' '}) df_call_notes = df_call_notes.na.fill({'KEYWORD1':' '}) df_call_notes = df_call_notes.na.fill({'KEYWORD2':' '}) In the following cell we join some of the customer and call note data sources using the ID field. This ID field is the one coming from the CUSTOMER DB2 transactional database. data_joined_callnotes_churn = df_call_notes.join(df_customer_transactions,df_call_notes['ID']==df_customer_transactions['ID'],'inner').select(df_call_notes['SENTIMENTS'],df_call_notes['KEYWORD1'],df_call_notes['KEYWORD2'],df_customer_transactions['*']) data_joined_callnotes_churn_campaign = df_campaign_responses.join(data_joined_callnotes_churn,df_campaign_responses['ID']==data_joined_callnotes_churn['ID'],'inner').select(data_joined_callnotes_churn['*'],df_campaign_responses['RESPONDED_CAMPAIGN'],df_campaign_responses['OWNS_MULTIPLE_PHONE_NUMBERS'],df_campaign_responses['AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_']) data_joined_callnotes_churn_campaign.take(5) [Row(SENTIMENTS=u' ', KEYWORD1=u'help', KEYWORD2=u'support', ID=148, GENDER=u'M', STATUS=u'M', CHILDREN=2, EST_INCOME=Decimal('91272.20'), CAR_OWNER=u'Y', AGE=Decimal('25.033333'), MARITAL_STATUS=u'Married', ZIPCODE=None, LONGDISTANCE=Decimal('26.99'), INTERNATIONAL=0, LOCAL=Decimal('13.01'), DROPPED=0, PAYMETHOD=u'CC', LOCALBILLTYPE=u'FreeLocal', LONGDISTANCEBILLTYPE=u'Standard', USAGE=Decimal('40.00'), RATEPLAN=3, DEVICEOWNED=u'ipho', CHURN=u'F', RESPONDED_CAMPAIGN=u'Android Phone', OWNS_MULTIPLE_PHONE_NUMBERS=u'N', AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_=1900), Row(SENTIMENTS=u' ', KEYWORD1=u'update records', KEYWORD2=u'address', ID=463, GENDER=u'M', STATUS=u'M', CHILDREN=0, EST_INCOME=Decimal('69168.40'), CAR_OWNER=u'Y', AGE=Decimal('62.426667'), MARITAL_STATUS=u'Married', ZIPCODE=None, LONGDISTANCE=Decimal('14.16'), INTERNATIONAL=6, LOCAL=Decimal('214.73'), DROPPED=0, PAYMETHOD=u'CC', LOCALBILLTYPE=u'Budget', LONGDISTANCEBILLTYPE=u'Standard', USAGE=Decimal('234.91'), RATEPLAN=2, DEVICEOWNED=u'sam', CHURN=u'T', RESPONDED_CAMPAIGN=u'Dual SIM', OWNS_MULTIPLE_PHONE_NUMBERS=u'Y', AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_=1586), Row(SENTIMENTS=u'analytical', KEYWORD1=u'billing', KEYWORD2=u'charges', ID=471, GENDER=u'M', STATUS=u'M', CHILDREN=2, EST_INCOME=Decimal('90103.70'), CAR_OWNER=u'N', AGE=Decimal('34.946667'), MARITAL_STATUS=u'Married', ZIPCODE=None, LONGDISTANCE=Decimal('12.23'), INTERNATIONAL=8, LOCAL=Decimal('45.34'), DROPPED=0, PAYMETHOD=u'CC', LOCALBILLTYPE=u'Budget', LONGDISTANCEBILLTYPE=u'Intnl_discount', USAGE=Decimal('66.45'), RATEPLAN=3, DEVICEOWNED=u'sam', CHURN=u'F', RESPONDED_CAMPAIGN=u'More Storage', OWNS_MULTIPLE_PHONE_NUMBERS=u'N', AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_=1114), Row(SENTIMENTS=u'satisfied', KEYWORD1=u'battery', KEYWORD2=u'unpredictability', ID=1238, GENDER=u'F', STATUS=u'M', CHILDREN=2, EST_INCOME=Decimal('3193.60'), CAR_OWNER=u'N', AGE=Decimal('54.046667'), MARITAL_STATUS=u'Married', ZIPCODE=None, LONGDISTANCE=Decimal('4.19'), INTERNATIONAL=0, LOCAL=Decimal('114.62'), DROPPED=1, PAYMETHOD=u'CH', LOCALBILLTYPE=u'Budget', LONGDISTANCEBILLTYPE=u'Standard', USAGE=Decimal('118.82'), RATEPLAN=3, DEVICEOWNED=u'ipho', CHURN=u'F', RESPONDED_CAMPAIGN=u'Large Display', OWNS_MULTIPLE_PHONE_NUMBERS=u'N', AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_=1697), Row(SENTIMENTS=u'frustrated', KEYWORD1=u'charger', KEYWORD2=u'switch carrier', ID=1342, GENDER=u'M', STATUS=u'S', CHILDREN=0, EST_INCOME=Decimal('94928.30'), CAR_OWNER=u'N', AGE=Decimal('40.180000'), MARITAL_STATUS=u'Single', ZIPCODE=None, LONGDISTANCE=Decimal('14.42'), INTERNATIONAL=5, LOCAL=Decimal('73.74'), DROPPED=0, PAYMETHOD=u'CC', LOCALBILLTYPE=u'FreeLocal', LONGDISTANCEBILLTYPE=u'Standard', USAGE=Decimal('93.78'), RATEPLAN=1, DEVICEOWNED=u'ipho', CHURN=u'T', RESPONDED_CAMPAIGN=u'Dual SIM', OWNS_MULTIPLE_PHONE_NUMBERS=u'Y', AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_=1540)] The following code block is intended to get a feel for Spark DataFrame APIs. We attempt to fix some of the column titles to promote readability, and also remove a duplicate column (Status and Marital Status are the same). Finally convert the DataFrame to Python Pandas structure for visualization. Since some fields are string types from DB2 tables, let us change some of them to other types # Change some column names data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"SENTIMENTS\", \"SENTIMENT\").withColumnRenamed(\"OWNS_MULTIPLE_PHONE_NUMBERS\",\"OMPN\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"KEYWORD1\", \"KEYWORD_COMPONENT\").withColumnRenamed(\"KEYWORD2\",\"KEYWORD_QUERY\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_\", \"SMSCOUNT\").withColumnRenamed(\"CAR_OWNER\",\"CAROWNERSHIP\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"MARITAL_STATUS\", \"MARITALSTATUS\").withColumnRenamed(\"EST_INCOME\",\"INCOME\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.drop('Status') # Change some of the data types data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"CHILDREN\", data_joined_callnotes_churn_campaign[\"CHILDREN\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"INCOME\", data_joined_callnotes_churn_campaign[\"INCOME\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"AGE\", data_joined_callnotes_churn_campaign[\"AGE\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"LONGDISTANCE\", data_joined_callnotes_churn_campaign[\"LONGDISTANCE\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"INTERNATIONAL\", data_joined_callnotes_churn_campaign[\"INTERNATIONAL\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"LOCAL\", data_joined_callnotes_churn_campaign[\"LOCAL\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"DROPPED\", data_joined_callnotes_churn_campaign[\"DROPPED\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"USAGE\", data_joined_callnotes_churn_campaign[\"USAGE\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"RATEPLAN\", data_joined_callnotes_churn_campaign[\"RATEPLAN\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"SMSCOUNT\", data_joined_callnotes_churn_campaign[\"SMSCOUNT\"].cast(IntegerType())) data_joined_callnotes_churn_campaign.show(10) data_joined_callnotes_churn_campaign.printSchema() pandas_df_callnotes_campaign_churn = data_joined_callnotes_churn_campaign.toPandas() pandas_df_callnotes_campaign_churn.head(12) +----------+-----------------+----------------+----+------+--------+------+------------+---+-------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----------+-----+------------------+----+--------+ | SENTIMENT|KEYWORD_COMPONENT| KEYWORD_QUERY| ID|GENDER|CHILDREN|INCOME|CAROWNERSHIP|AGE|MARITALSTATUS|ZIPCODE|LONGDISTANCE|INTERNATIONAL|LOCAL|DROPPED|PAYMETHOD|LOCALBILLTYPE|LONGDISTANCEBILLTYPE|USAGE|RATEPLAN|DEVICEOWNED|CHURN|RESPONDED_CAMPAIGN|OMPN|SMSCOUNT| +----------+-----------------+----------------+----+------+--------+------+------------+---+-------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----------+-----+------------------+----+--------+ | | help| support| 148| M| 2| 91272| Y| 25| Married| null| 27| 0| 13| 0| CC| FreeLocal| Standard| 40| 3| ipho| F| Android Phone| N| 1900| | | update records| address| 463| M| 0| 69168| Y| 62| Married| null| 14| 6| 215| 0| CC| Budget| Standard| 235| 2| sam| T| Dual SIM| Y| 1586| |analytical| billing| charges| 471| M| 2| 90104| N| 34| Married| null| 12| 8| 45| 0| CC| Budget| Intnl_discount| 66| 3| sam| F| More Storage| N| 1114| | satisfied| battery|unpredictability|1238| F| 2| 3194| N| 54| Married| null| 4| 0| 115| 1| CH| Budget| Standard| 119| 3| ipho| F| Large Display| N| 1697| |frustrated| charger| switch carrier|1342| M| 0| 94928| N| 40| Single| null| 14| 5| 74| 0| CC| FreeLocal| Standard| 94| 1| ipho| T| Dual SIM| Y| 1540| |analytical| new number| customer care|1591| F| 0| 45613| N| 14| Single| null| 13| 0| 311| 0| CC| Budget| Standard| 324| 4| ipho| F| Android Phone| N| 1681| |frustrated| call forwarding| features|1645| M| 1| 92648| N| 56| Single| null| 16| 5| 10| 0| CC| Budget| Standard| 32| 4| ipho| T| Android Phone| N| 2291| |analytical| tablet| new offering|1959| F| 1| 13829| N| 19| Married| null| 42| 0| 160| 0| CC| FreeLocal| Standard| 177| 2| ipho| T| Android Phone| N| 1821| |analytical| rate plan| customer care|1959| F| 1| 13829| N| 19| Married| null| 42| 0| 160| 0| CC| FreeLocal| Standard| 177| 2| ipho| T| Android Phone| N| 1821| | | new number| service|2122| M| 2| 49911| Y| 51| Married| null| 27| 0| 24| 0| CC| Budget| Standard| 51| 1| ipho| F| Android Phone| N| 1487| +----------+-----------------+----------------+----+------+--------+------+------------+---+-------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----------+-----+------------------+----+--------+ only showing top 10 rows root |-- SENTIMENT: string (nullable = false) |-- KEYWORD_COMPONENT: string (nullable = false) |-- KEYWORD_QUERY: string (nullable = false) |-- ID: integer (nullable = true) |-- GENDER: string (nullable = true) |-- CHILDREN: integer (nullable = true) |-- INCOME: decimal(10,0) (nullable = true) |-- CAROWNERSHIP: string (nullable = true) |-- AGE: integer (nullable = true) |-- MARITALSTATUS: string (nullable = true) |-- ZIPCODE: integer (nullable = true) |-- LONGDISTANCE: decimal(10,0) (nullable = true) |-- INTERNATIONAL: decimal(10,0) (nullable = true) |-- LOCAL: decimal(10,0) (nullable = true) |-- DROPPED: integer (nullable = true) |-- PAYMETHOD: string (nullable = true) |-- LOCALBILLTYPE: string (nullable = true) |-- LONGDISTANCEBILLTYPE: string (nullable = true) |-- USAGE: decimal(10,0) (nullable = true) |-- RATEPLAN: integer (nullable = true) |-- DEVICEOWNED: string (nullable = true) |-- CHURN: string (nullable = true) |-- RESPONDED_CAMPAIGN: string (nullable = true) |-- OMPN: string (nullable = true) |-- SMSCOUNT: integer (nullable = true) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } SENTIMENT KEYWORD_COMPONENT KEYWORD_QUERY ID GENDER CHILDREN INCOME CAROWNERSHIP AGE MARITALSTATUS ... PAYMETHOD LOCALBILLTYPE LONGDISTANCEBILLTYPE USAGE RATEPLAN DEVICEOWNED CHURN RESPONDED_CAMPAIGN OMPN SMSCOUNT 0 help support 148 M 2 91272 Y 25 Married ... CC FreeLocal Standard 40 3 ipho F Android Phone N 1900 1 update records address 463 M 0 69168 Y 62 Married ... CC Budget Standard 235 2 sam T Dual SIM Y 1586 2 analytical billing charges 471 M 2 90104 N 34 Married ... CC Budget Intnl_discount 66 3 sam F More Storage N 1114 3 satisfied battery unpredictability 1238 F 2 3194 N 54 Married ... CH Budget Standard 119 3 ipho F Large Display N 1697 4 frustrated charger switch carrier 1342 M 0 94928 N 40 Single ... CC FreeLocal Standard 94 1 ipho T Dual SIM Y 1540 5 analytical new number customer care 1591 F 0 45613 N 14 Single ... CC Budget Standard 324 4 ipho F Android Phone N 1681 6 frustrated call forwarding features 1645 M 1 92648 N 56 Single ... CC Budget Standard 32 4 ipho T Android Phone N 2291 7 analytical tablet new offering 1959 F 1 13829 N 19 Married ... CC FreeLocal Standard 177 2 ipho T Android Phone N 1821 8 analytical rate plan customer care 1959 F 1 13829 N 19 Married ... CC FreeLocal Standard 177 2 ipho T Android Phone N 1821 9 new number service 2122 M 2 49911 Y 51 Married ... CC Budget Standard 51 1 ipho F Android Phone N 1487 10 lost phone service suspension 2366 M 2 1765 N 18 Married ... CC FreeLocal Standard 48 4 sam F More Storage Y 1865 11 frustrated data plan speed 2659 F 1 98680 Y 43 Married ... CH FreeLocal Intnl_discount 26 3 sam F Windows Phone N 1732 12 rows \u00d7 25 columns The following brunel based visualization can also be performed from Data Refinery. Shown here to get the feel for APIs %brunel data('pandas_df_callnotes_campaign_churn') bar y(#count) stack polar color(Sentiment) sort(#count) label(Sentiment, ' (', #count, '%)') tooltip(#all) percent(#count) legends(none) \u0002wzxhzdk:9\u0003 \u0002wzxhzdk:10\u0003 ##### The following cell shows an example of how pixiedust can be used to build interactive dashboards, and how it can be exported out \u0002wzxhzdk:11\u0003 .pd_warning{display:none;} Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter ##### Building RandomForest based classifier \u0002wzxhzdk:12\u0003 \u0002wzxhzdk:13\u0003 ##### Split the dataset into training and test using 70:30 split ratio and build the model \u0002wzxhzdk:14\u0003 ##### Testing the test dataset \u0002wzxhzdk:15\u0003 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } ID CHURN label predictedLabel prediction probability 0 148 F 0.0 F 0.0 [0.8728832555, 0.1271167445] 1 1959 T 1.0 T 1.0 [0.285629950796, 0.714370049204] 2 3749 T 1.0 T 1.0 [0.165697434112, 0.834302565888] 3 2659 F 0.0 F 0.0 [0.714051839779, 0.285948160221] 4 1238 F 0.0 F 0.0 [0.87563833506, 0.12436166494] 5 1460 F 0.0 F 0.0 [0.925838160212, 0.0741618397881] ##### Model Evaluation \u0002wzxhzdk:16\u0003 Precision model1 = 0.89. Area under ROC curve = 0.88. \u0002wzxhzdk:17\u0003 \u0002wzxhzdk:18\u0003 \u0002wzxhzdk:19\u0003 ##### Save pipeline and model artifacts to Machine Learning repository: \u0002wzxhzdk:20\u0003 \u0002wzxhzdk:21\u0003 modelType: sparkml-model-2.0 creationTime: 2018-02-19 22:18:53.157000+00:00 modelVersionHref: https://internal-nginx-svc.ibm-private-cloud.svc.cluster.local:12443/v2/artifacts/models/56af74d4-93f4-41bd-8069-850a5117475d/versions/4de851a3-4d05-4957-a27f-6701418e6ce4 label: CHURN","title":"Evaluate and predict customer churn"},{"location":"ml/CustomerChurnAnalysisDSXICP/#evaluate-and-predict-customer-churn","text":"This notebook is an adaptation from the work done by Sidney Phoon and Eleva Lowery with the following modifications: Use datasets persisted in DB2 Warehouse running on ICP Deploy and run the notebook on DSX local running on IBM Cloud Private (ICP) Run spark Machine learning job on ICP as part of the worker nodes. Document some actions for a beginner data scientist / developer who wants to understand what's going on. The goal is to demonstrate how to build a predictive model with Spark machine learning API (SparkML) to predict customer churn, and deploy it for scoring in Machine Learning (ML) running on ICP. There is an equivalent notebook to run on Watson Data Platform and Watson Machine Learning.","title":"Evaluate and predict customer churn"},{"location":"ml/CustomerChurnAnalysisDSXICP/#scope","text":"A lot of industries have the issue of customers moving to competitors when the product differentiation is not that important, or there is some customer support issues. One industry illustrating this problem is the telecom industry with mobile, internet and IP TV product offerings.","title":"Scope"},{"location":"ml/CustomerChurnAnalysisDSXICP/#note-book-explanations","text":"The notebook aims to follow the classical data science modeling steps: load the data prepare the data analyze the data (iterate on those two activities) build a model validate the accuracy of the model deploy the model consume the model as a service This jupyter notebook uses Apache Spark to run the machine learning jobs to build decision trees using random forest classifier to assess when a customer is at risk to move to competitor. Apache Spark offers a Python module called pyspark to operate on data and use ML constructs.","title":"Note book explanations"},{"location":"ml/CustomerChurnAnalysisDSXICP/#start-by-all-imports","text":"As a best practices for notebook implementation is to do the import at the top of the notebook. Spark SQLContext a spark module to process structured data spark conf to access Spark cluster configuration and then be able to execute queries pandas Python super library for data analysis brunel API and tool to visualize data quickly. * pixiedust Visualize data inside Jupyter notebooks The first cell below is to execute some system commands to update the kernel with updated dependant library. # Library required for pixiedust - a visualization and dashboarding framework !pip install --user --upgrade pixiedust Requirement already up-to-date: pixiedust in /user-home/1002/.local/lib/python2.7/site-packages Requirement already up-to-date: astunparse in /user-home/1002/.local/lib/python2.7/site-packages (from pixiedust) Requirement already up-to-date: mpld3 in /opt/conda/lib/python2.7/site-packages (from pixiedust) Requirement already up-to-date: markdown in /user-home/1002/.local/lib/python2.7/site-packages (from pixiedust) Requirement already up-to-date: geojson in /user-home/1002/.local/lib/python2.7/site-packages (from pixiedust) Requirement already up-to-date: lxml in /user-home/1002/.local/lib/python2.7/site-packages (from pixiedust) Requirement already up-to-date: six<2.0,>=1.6.1 in /user-home/1002/.local/lib/python2.7/site-packages (from astunparse->pixiedust) Requirement already up-to-date: wheel<1.0,>=0.23.0 in /user-home/1002/.local/lib/python2.7/site-packages (from astunparse->pixiedust) import pyspark import pandas as pd import brunel import numpy as np from pyspark.sql import SQLContext from pyspark.conf import SparkConf from pyspark.sql import SparkSession from pyspark.sql.types import DoubleType from pyspark.sql.types import DecimalType from pyspark.sql.types import IntegerType from pyspark.sql.types import LongType from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString from pyspark.ml import Pipeline from pyspark.ml.feature import VectorAssembler from pyspark.ml.classification import RandomForestClassifier from pyspark.ml.evaluation import BinaryClassificationEvaluator from pixiedust.display import * Pixiedust database opened successfully <div style=\"margin:10px\"> <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\"> <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/> </a> <span>Pixiedust version 1.1.7.1</span> </div>","title":"Start by all imports"},{"location":"ml/CustomerChurnAnalysisDSXICP/#load-data-from-db2","text":"DSX ICP can be used to bring in data from multiple sources including but not limited to, files, datastores on cloud as well as on premises. DSX ICP includes features to connect to data sources, bring in the data, refine, and then perform analytics. In this sample we connect to DB2 Data Warehouse deployed on ICP and bring data about customer, call notes and marketing campaign in. import dsx_core_utils dataSet = dsx_core_utils.get_remote_data_set_info('CUSTOMER') dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource']) print(dataSource) dbTableOrQuery = dataSet['schema'] + '.' + dataSet['table'] print(dbTableOrQuery) sparkSession = SparkSession(sc).builder.getOrCreate() df_customer_transactions = sparkSession.read.format(\"jdbc\").option(\"url\", dataSource['URL']).option(\"dbtable\",dbTableOrQuery).option(\"user\",'BLUADMIN').option(\"password\",\"changemeplease\").load() df_customer_transactions.show(5) df_customer_transactions.printSchema() {u'description': u'', u'URL': u'jdbc:db2://172.16.40.131:32166/BLUDB', 'driver_class': 'com.ibm.db2.jcc.DB2Driver', u'dsx_artifact_type': u'datasource', u'shared': True, u'type': u'DB2', u'name': u'CUSTOMER'} BLUADMIN.CUSTOMER +----+------+------+--------+----------+---------+---------+--------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+------+--------+-----------+-----+ | ID|GENDER|STATUS|CHILDREN|EST_INCOME|CAR_OWNER| AGE|MARITAL_STATUS|ZIPCODE|LONGDISTANCE|INTERNATIONAL|LOCAL|DROPPED|PAYMETHOD|LOCALBILLTYPE|LONGDISTANCEBILLTYPE| USAGE|RATEPLAN|DEVICEOWNED|CHURN| +----+------+------+--------+----------+---------+---------+--------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+------+--------+-----------+-----+ |1311| F| M| 0| 53432.20| Y|57.000000| Married| null| 12.56| 0|83.48| 0| CH| FreeLocal| Standard| 96.04| 1| ipho| F| |1312| M| M| 0| 78894.20| N|52.740000| Married| null| 2.00| 1|43.23| 0| CH| Budget| Intnl_discount| 46.65| 3| ipho| T| |1313| M| S| 1| 16432.10| Y|20.920000| Single| null| 20.41| 8|80.47| 0| CH| Budget| Intnl_discount|109.78| 3| ipho| T| |1314| M| M| 0| 62797.90| N|54.446667| Married| null| 8.32| 4|55.48| 0| CH| Budget| Intnl_discount| 68.09| 3| ipho| T| |1315| M| S| 1| 71329.90| Y|42.233333| Single| null| 14.27| 3|45.92| 0| CC| Budget| Standard| 63.75| 1| ipho| T| +----+------+------+--------+----------+---------+---------+--------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+------+--------+-----------+-----+ only showing top 5 rows root |-- ID: integer (nullable = true) |-- GENDER: string (nullable = true) |-- STATUS: string (nullable = true) |-- CHILDREN: integer (nullable = true) |-- EST_INCOME: decimal(8,2) (nullable = true) |-- CAR_OWNER: string (nullable = true) |-- AGE: decimal(14,6) (nullable = true) |-- MARITAL_STATUS: string (nullable = true) |-- ZIPCODE: integer (nullable = true) |-- LONGDISTANCE: decimal(6,2) (nullable = true) |-- INTERNATIONAL: integer (nullable = true) |-- LOCAL: decimal(7,2) (nullable = true) |-- DROPPED: integer (nullable = true) |-- PAYMETHOD: string (nullable = true) |-- LOCALBILLTYPE: string (nullable = true) |-- LONGDISTANCEBILLTYPE: string (nullable = true) |-- USAGE: decimal(7,2) (nullable = true) |-- RATEPLAN: integer (nullable = true) |-- DEVICEOWNED: string (nullable = true) |-- CHURN: string (nullable = true) dataSet = dsx_core_utils.get_remote_data_set_info('CALLNOTES') dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource']) print(dataSource) dbTableOrQuery = dataSet['schema'] + '.' + dataSet['table'] print(dbTableOrQuery) sparkSession = SparkSession(sc).builder.getOrCreate() df_call_notes = sparkSession.read.format(\"jdbc\").option(\"url\", dataSource['URL']).option(\"dbtable\",dbTableOrQuery).option(\"user\",'BLUADMIN').option(\"password\",\"changemeplease\").load() df_call_notes.show(5) df_call_notes.printSchema() {u'description': u'', u'URL': u'jdbc:db2://172.16.40.131:32166/BLUDB', 'driver_class': 'com.ibm.db2.jcc.DB2Driver', u'dsx_artifact_type': u'datasource', u'shared': True, u'type': u'DB2', u'name': u'CUSTOMER'} BLUADMIN.CALLNOTES +----+--------------------+----------+--------------+--------+ | ID| COMMENTS|SENTIMENTS| KEYWORD1|KEYWORD2| +----+--------------------+----------+--------------+--------+ |2253|Wants to change a...| null|update records| address| |2254|Wants to change a...| null|update records| address| |2255|Wants to change a...| null|update records| address| |2256|Wants to change a...| null|update records| address| |2257|Needed help figur...|analytical| billing| charges| +----+--------------------+----------+--------------+--------+ only showing top 5 rows root |-- ID: integer (nullable = true) |-- COMMENTS: string (nullable = true) |-- SENTIMENTS: string (nullable = true) |-- KEYWORD1: string (nullable = true) |-- KEYWORD2: string (nullable = true) dataSet = dsx_core_utils.get_remote_data_set_info('CAMPAIGNRESPONSES_EXPANDED') dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource']) print(dataSource) dbTableOrQuery = dataSet['schema'] + '.' + dataSet['table'] print(dbTableOrQuery) sparkSession = SparkSession(sc).builder.getOrCreate() df_campaign_responses = sparkSession.read.format(\"jdbc\").option(\"url\", dataSource['URL']).option(\"dbtable\",dbTableOrQuery).option(\"user\",'BLUADMIN').option(\"password\",\"changemeplease\").load() df_campaign_responses.show(5) df_campaign_responses.printSchema() {u'description': u'', u'URL': u'jdbc:db2://172.16.40.131:32166/BLUDB', 'driver_class': 'com.ibm.db2.jcc.DB2Driver', u'dsx_artifact_type': u'datasource', u'shared': True, u'type': u'DB2', u'name': u'CUSTOMER'} BLUADMIN.CAMPAIGNRESPONSES_EXPANDED +----+------------------+---------------------------+-------------------------------------+ | ID|RESPONDED_CAMPAIGN|OWNS_MULTIPLE_PHONE_NUMBERS|AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_| +----+------------------+---------------------------+-------------------------------------+ |3064| Kids Tablet| Y| 1561| |3077| Kids Tablet| Y| 1225| |3105| Kids Tablet| Y| 1661| |3106| Kids Tablet| N| 2498| |3108| Kids Tablet| N| 1118| +----+------------------+---------------------------+-------------------------------------+ only showing top 5 rows root |-- ID: integer (nullable = true) |-- RESPONDED_CAMPAIGN: string (nullable = true) |-- OWNS_MULTIPLE_PHONE_NUMBERS: string (nullable = true) |-- AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_: integer (nullable = true)","title":"Load data from DB2"},{"location":"ml/CustomerChurnAnalysisDSXICP/#data-preparation","text":"The next few steps involve a series of data preparation tasks such as filling the missing values, joining datasets etc. The following cell fills the null values for average SMS count and replaces Nulls with spaces for other fields. df_campaign_responses = df_campaign_responses.na.fill({'AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_':'0'}) df_call_notes = df_call_notes.na.fill({'SENTIMENTS':' '}) df_call_notes = df_call_notes.na.fill({'KEYWORD1':' '}) df_call_notes = df_call_notes.na.fill({'KEYWORD2':' '}) In the following cell we join some of the customer and call note data sources using the ID field. This ID field is the one coming from the CUSTOMER DB2 transactional database. data_joined_callnotes_churn = df_call_notes.join(df_customer_transactions,df_call_notes['ID']==df_customer_transactions['ID'],'inner').select(df_call_notes['SENTIMENTS'],df_call_notes['KEYWORD1'],df_call_notes['KEYWORD2'],df_customer_transactions['*']) data_joined_callnotes_churn_campaign = df_campaign_responses.join(data_joined_callnotes_churn,df_campaign_responses['ID']==data_joined_callnotes_churn['ID'],'inner').select(data_joined_callnotes_churn['*'],df_campaign_responses['RESPONDED_CAMPAIGN'],df_campaign_responses['OWNS_MULTIPLE_PHONE_NUMBERS'],df_campaign_responses['AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_']) data_joined_callnotes_churn_campaign.take(5) [Row(SENTIMENTS=u' ', KEYWORD1=u'help', KEYWORD2=u'support', ID=148, GENDER=u'M', STATUS=u'M', CHILDREN=2, EST_INCOME=Decimal('91272.20'), CAR_OWNER=u'Y', AGE=Decimal('25.033333'), MARITAL_STATUS=u'Married', ZIPCODE=None, LONGDISTANCE=Decimal('26.99'), INTERNATIONAL=0, LOCAL=Decimal('13.01'), DROPPED=0, PAYMETHOD=u'CC', LOCALBILLTYPE=u'FreeLocal', LONGDISTANCEBILLTYPE=u'Standard', USAGE=Decimal('40.00'), RATEPLAN=3, DEVICEOWNED=u'ipho', CHURN=u'F', RESPONDED_CAMPAIGN=u'Android Phone', OWNS_MULTIPLE_PHONE_NUMBERS=u'N', AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_=1900), Row(SENTIMENTS=u' ', KEYWORD1=u'update records', KEYWORD2=u'address', ID=463, GENDER=u'M', STATUS=u'M', CHILDREN=0, EST_INCOME=Decimal('69168.40'), CAR_OWNER=u'Y', AGE=Decimal('62.426667'), MARITAL_STATUS=u'Married', ZIPCODE=None, LONGDISTANCE=Decimal('14.16'), INTERNATIONAL=6, LOCAL=Decimal('214.73'), DROPPED=0, PAYMETHOD=u'CC', LOCALBILLTYPE=u'Budget', LONGDISTANCEBILLTYPE=u'Standard', USAGE=Decimal('234.91'), RATEPLAN=2, DEVICEOWNED=u'sam', CHURN=u'T', RESPONDED_CAMPAIGN=u'Dual SIM', OWNS_MULTIPLE_PHONE_NUMBERS=u'Y', AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_=1586), Row(SENTIMENTS=u'analytical', KEYWORD1=u'billing', KEYWORD2=u'charges', ID=471, GENDER=u'M', STATUS=u'M', CHILDREN=2, EST_INCOME=Decimal('90103.70'), CAR_OWNER=u'N', AGE=Decimal('34.946667'), MARITAL_STATUS=u'Married', ZIPCODE=None, LONGDISTANCE=Decimal('12.23'), INTERNATIONAL=8, LOCAL=Decimal('45.34'), DROPPED=0, PAYMETHOD=u'CC', LOCALBILLTYPE=u'Budget', LONGDISTANCEBILLTYPE=u'Intnl_discount', USAGE=Decimal('66.45'), RATEPLAN=3, DEVICEOWNED=u'sam', CHURN=u'F', RESPONDED_CAMPAIGN=u'More Storage', OWNS_MULTIPLE_PHONE_NUMBERS=u'N', AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_=1114), Row(SENTIMENTS=u'satisfied', KEYWORD1=u'battery', KEYWORD2=u'unpredictability', ID=1238, GENDER=u'F', STATUS=u'M', CHILDREN=2, EST_INCOME=Decimal('3193.60'), CAR_OWNER=u'N', AGE=Decimal('54.046667'), MARITAL_STATUS=u'Married', ZIPCODE=None, LONGDISTANCE=Decimal('4.19'), INTERNATIONAL=0, LOCAL=Decimal('114.62'), DROPPED=1, PAYMETHOD=u'CH', LOCALBILLTYPE=u'Budget', LONGDISTANCEBILLTYPE=u'Standard', USAGE=Decimal('118.82'), RATEPLAN=3, DEVICEOWNED=u'ipho', CHURN=u'F', RESPONDED_CAMPAIGN=u'Large Display', OWNS_MULTIPLE_PHONE_NUMBERS=u'N', AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_=1697), Row(SENTIMENTS=u'frustrated', KEYWORD1=u'charger', KEYWORD2=u'switch carrier', ID=1342, GENDER=u'M', STATUS=u'S', CHILDREN=0, EST_INCOME=Decimal('94928.30'), CAR_OWNER=u'N', AGE=Decimal('40.180000'), MARITAL_STATUS=u'Single', ZIPCODE=None, LONGDISTANCE=Decimal('14.42'), INTERNATIONAL=5, LOCAL=Decimal('73.74'), DROPPED=0, PAYMETHOD=u'CC', LOCALBILLTYPE=u'FreeLocal', LONGDISTANCEBILLTYPE=u'Standard', USAGE=Decimal('93.78'), RATEPLAN=1, DEVICEOWNED=u'ipho', CHURN=u'T', RESPONDED_CAMPAIGN=u'Dual SIM', OWNS_MULTIPLE_PHONE_NUMBERS=u'Y', AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_=1540)]","title":"Data Preparation"},{"location":"ml/CustomerChurnAnalysisDSXICP/#the-following-code-block-is-intended-to-get-a-feel-for-spark-dataframe-apis-we-attempt-to-fix-some-of-the-column-titles-to-promote-readability-and-also-remove-a-duplicate-column-status-and-marital-status-are-the-same-finally-convert-the-dataframe-to-python-pandas-structure-for-visualization-since-some-fields-are-string-types-from-db2-tables-let-us-change-some-of-them-to-other-types","text":"# Change some column names data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"SENTIMENTS\", \"SENTIMENT\").withColumnRenamed(\"OWNS_MULTIPLE_PHONE_NUMBERS\",\"OMPN\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"KEYWORD1\", \"KEYWORD_COMPONENT\").withColumnRenamed(\"KEYWORD2\",\"KEYWORD_QUERY\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"AVERAGE_TEXT_MESSAGES__90_DAY_PERIOD_\", \"SMSCOUNT\").withColumnRenamed(\"CAR_OWNER\",\"CAROWNERSHIP\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumnRenamed(\"MARITAL_STATUS\", \"MARITALSTATUS\").withColumnRenamed(\"EST_INCOME\",\"INCOME\") data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.drop('Status') # Change some of the data types data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"CHILDREN\", data_joined_callnotes_churn_campaign[\"CHILDREN\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"INCOME\", data_joined_callnotes_churn_campaign[\"INCOME\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"AGE\", data_joined_callnotes_churn_campaign[\"AGE\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"LONGDISTANCE\", data_joined_callnotes_churn_campaign[\"LONGDISTANCE\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"INTERNATIONAL\", data_joined_callnotes_churn_campaign[\"INTERNATIONAL\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"LOCAL\", data_joined_callnotes_churn_campaign[\"LOCAL\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"DROPPED\", data_joined_callnotes_churn_campaign[\"DROPPED\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"USAGE\", data_joined_callnotes_churn_campaign[\"USAGE\"].cast(DecimalType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"RATEPLAN\", data_joined_callnotes_churn_campaign[\"RATEPLAN\"].cast(IntegerType())) data_joined_callnotes_churn_campaign = data_joined_callnotes_churn_campaign.withColumn(\"SMSCOUNT\", data_joined_callnotes_churn_campaign[\"SMSCOUNT\"].cast(IntegerType())) data_joined_callnotes_churn_campaign.show(10) data_joined_callnotes_churn_campaign.printSchema() pandas_df_callnotes_campaign_churn = data_joined_callnotes_churn_campaign.toPandas() pandas_df_callnotes_campaign_churn.head(12) +----------+-----------------+----------------+----+------+--------+------+------------+---+-------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----------+-----+------------------+----+--------+ | SENTIMENT|KEYWORD_COMPONENT| KEYWORD_QUERY| ID|GENDER|CHILDREN|INCOME|CAROWNERSHIP|AGE|MARITALSTATUS|ZIPCODE|LONGDISTANCE|INTERNATIONAL|LOCAL|DROPPED|PAYMETHOD|LOCALBILLTYPE|LONGDISTANCEBILLTYPE|USAGE|RATEPLAN|DEVICEOWNED|CHURN|RESPONDED_CAMPAIGN|OMPN|SMSCOUNT| +----------+-----------------+----------------+----+------+--------+------+------------+---+-------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----------+-----+------------------+----+--------+ | | help| support| 148| M| 2| 91272| Y| 25| Married| null| 27| 0| 13| 0| CC| FreeLocal| Standard| 40| 3| ipho| F| Android Phone| N| 1900| | | update records| address| 463| M| 0| 69168| Y| 62| Married| null| 14| 6| 215| 0| CC| Budget| Standard| 235| 2| sam| T| Dual SIM| Y| 1586| |analytical| billing| charges| 471| M| 2| 90104| N| 34| Married| null| 12| 8| 45| 0| CC| Budget| Intnl_discount| 66| 3| sam| F| More Storage| N| 1114| | satisfied| battery|unpredictability|1238| F| 2| 3194| N| 54| Married| null| 4| 0| 115| 1| CH| Budget| Standard| 119| 3| ipho| F| Large Display| N| 1697| |frustrated| charger| switch carrier|1342| M| 0| 94928| N| 40| Single| null| 14| 5| 74| 0| CC| FreeLocal| Standard| 94| 1| ipho| T| Dual SIM| Y| 1540| |analytical| new number| customer care|1591| F| 0| 45613| N| 14| Single| null| 13| 0| 311| 0| CC| Budget| Standard| 324| 4| ipho| F| Android Phone| N| 1681| |frustrated| call forwarding| features|1645| M| 1| 92648| N| 56| Single| null| 16| 5| 10| 0| CC| Budget| Standard| 32| 4| ipho| T| Android Phone| N| 2291| |analytical| tablet| new offering|1959| F| 1| 13829| N| 19| Married| null| 42| 0| 160| 0| CC| FreeLocal| Standard| 177| 2| ipho| T| Android Phone| N| 1821| |analytical| rate plan| customer care|1959| F| 1| 13829| N| 19| Married| null| 42| 0| 160| 0| CC| FreeLocal| Standard| 177| 2| ipho| T| Android Phone| N| 1821| | | new number| service|2122| M| 2| 49911| Y| 51| Married| null| 27| 0| 24| 0| CC| Budget| Standard| 51| 1| ipho| F| Android Phone| N| 1487| +----------+-----------------+----------------+----+------+--------+------+------------+---+-------------+-------+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----------+-----+------------------+----+--------+ only showing top 10 rows root |-- SENTIMENT: string (nullable = false) |-- KEYWORD_COMPONENT: string (nullable = false) |-- KEYWORD_QUERY: string (nullable = false) |-- ID: integer (nullable = true) |-- GENDER: string (nullable = true) |-- CHILDREN: integer (nullable = true) |-- INCOME: decimal(10,0) (nullable = true) |-- CAROWNERSHIP: string (nullable = true) |-- AGE: integer (nullable = true) |-- MARITALSTATUS: string (nullable = true) |-- ZIPCODE: integer (nullable = true) |-- LONGDISTANCE: decimal(10,0) (nullable = true) |-- INTERNATIONAL: decimal(10,0) (nullable = true) |-- LOCAL: decimal(10,0) (nullable = true) |-- DROPPED: integer (nullable = true) |-- PAYMETHOD: string (nullable = true) |-- LOCALBILLTYPE: string (nullable = true) |-- LONGDISTANCEBILLTYPE: string (nullable = true) |-- USAGE: decimal(10,0) (nullable = true) |-- RATEPLAN: integer (nullable = true) |-- DEVICEOWNED: string (nullable = true) |-- CHURN: string (nullable = true) |-- RESPONDED_CAMPAIGN: string (nullable = true) |-- OMPN: string (nullable = true) |-- SMSCOUNT: integer (nullable = true) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } SENTIMENT KEYWORD_COMPONENT KEYWORD_QUERY ID GENDER CHILDREN INCOME CAROWNERSHIP AGE MARITALSTATUS ... PAYMETHOD LOCALBILLTYPE LONGDISTANCEBILLTYPE USAGE RATEPLAN DEVICEOWNED CHURN RESPONDED_CAMPAIGN OMPN SMSCOUNT 0 help support 148 M 2 91272 Y 25 Married ... CC FreeLocal Standard 40 3 ipho F Android Phone N 1900 1 update records address 463 M 0 69168 Y 62 Married ... CC Budget Standard 235 2 sam T Dual SIM Y 1586 2 analytical billing charges 471 M 2 90104 N 34 Married ... CC Budget Intnl_discount 66 3 sam F More Storage N 1114 3 satisfied battery unpredictability 1238 F 2 3194 N 54 Married ... CH Budget Standard 119 3 ipho F Large Display N 1697 4 frustrated charger switch carrier 1342 M 0 94928 N 40 Single ... CC FreeLocal Standard 94 1 ipho T Dual SIM Y 1540 5 analytical new number customer care 1591 F 0 45613 N 14 Single ... CC Budget Standard 324 4 ipho F Android Phone N 1681 6 frustrated call forwarding features 1645 M 1 92648 N 56 Single ... CC Budget Standard 32 4 ipho T Android Phone N 2291 7 analytical tablet new offering 1959 F 1 13829 N 19 Married ... CC FreeLocal Standard 177 2 ipho T Android Phone N 1821 8 analytical rate plan customer care 1959 F 1 13829 N 19 Married ... CC FreeLocal Standard 177 2 ipho T Android Phone N 1821 9 new number service 2122 M 2 49911 Y 51 Married ... CC Budget Standard 51 1 ipho F Android Phone N 1487 10 lost phone service suspension 2366 M 2 1765 N 18 Married ... CC FreeLocal Standard 48 4 sam F More Storage Y 1865 11 frustrated data plan speed 2659 F 1 98680 Y 43 Married ... CH FreeLocal Intnl_discount 26 3 sam F Windows Phone N 1732 12 rows \u00d7 25 columns","title":"The following code block is intended to get a feel for Spark DataFrame APIs. We attempt to fix some of the column titles to promote readability, and also remove a duplicate column (Status and Marital Status are the same).  Finally convert the DataFrame to Python Pandas structure for visualization. Since some fields are string types from DB2 tables, let us change some of them to other types"},{"location":"ml/CustomerChurnAnalysisDSXICP/#the-following-brunel-based-visualization-can-also-be-performed-from-data-refinery-shown-here-to-get-the-feel-for-apis","text":"%brunel data('pandas_df_callnotes_campaign_churn') bar y(#count) stack polar color(Sentiment) sort(#count) label(Sentiment, ' (', #count, '%)') tooltip(#all) percent(#count) legends(none) \u0002wzxhzdk:9\u0003 \u0002wzxhzdk:10\u0003 ##### The following cell shows an example of how pixiedust can be used to build interactive dashboards, and how it can be exported out \u0002wzxhzdk:11\u0003 .pd_warning{display:none;} Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter ##### Building RandomForest based classifier \u0002wzxhzdk:12\u0003 \u0002wzxhzdk:13\u0003 ##### Split the dataset into training and test using 70:30 split ratio and build the model \u0002wzxhzdk:14\u0003 ##### Testing the test dataset \u0002wzxhzdk:15\u0003 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } ID CHURN label predictedLabel prediction probability 0 148 F 0.0 F 0.0 [0.8728832555, 0.1271167445] 1 1959 T 1.0 T 1.0 [0.285629950796, 0.714370049204] 2 3749 T 1.0 T 1.0 [0.165697434112, 0.834302565888] 3 2659 F 0.0 F 0.0 [0.714051839779, 0.285948160221] 4 1238 F 0.0 F 0.0 [0.87563833506, 0.12436166494] 5 1460 F 0.0 F 0.0 [0.925838160212, 0.0741618397881] ##### Model Evaluation \u0002wzxhzdk:16\u0003 Precision model1 = 0.89. Area under ROC curve = 0.88. \u0002wzxhzdk:17\u0003 \u0002wzxhzdk:18\u0003 \u0002wzxhzdk:19\u0003 ##### Save pipeline and model artifacts to Machine Learning repository: \u0002wzxhzdk:20\u0003 \u0002wzxhzdk:21\u0003 modelType: sparkml-model-2.0 creationTime: 2018-02-19 22:18:53.157000+00:00 modelVersionHref: https://internal-nginx-svc.ibm-private-cloud.svc.cluster.local:12443/v2/artifacts/models/56af74d4-93f4-41bd-8069-850a5117475d/versions/4de851a3-4d05-4957-a27f-6701418e6ce4 label: CHURN","title":"The following brunel based visualization can also be performed from Data Refinery. Shown here to get the feel for APIs"},{"location":"ml/icp-dsx-ml-model/","text":"Building the machine learning model using DSX in ICP For a pure DSX, Db2 Warehouse and Spark cluster approach, running inside ICP the high level view may look like the figure below: We use Db2 warehouse to persist the three types of data. The data ingestions can be done with external tools like ETL, or by combining the Remote table definition inside Db2 Warehouse and scripts. We explained the remote table approach in this note . Data Ingestion Customer data The approach was to use Db2 Warehouse remote table to get access to the data. See this note for detail . CallNotes The CallNotes were loaded using the csv file export and load from file capabilities in Db2 Warehouse. ( See this note on how to similarly do csv loading ). The source for the CallNotes.csv file is under data folder. The IDs used in this file match the ID within the DB2 customer table. Campaign Responses The marketing campaign are in the csv file named CampainResponses_Expanded.csv under the data folder. The CSV file is loaded into Db2 warehouse. Specific jupyter notebook The Jupyter notebook is slightly different than the one developed within Watson Data Platform as the data sources are different. You can read the exported as markdown file here or load the src/dsx/CustomerChurnAnalysisDSXICP.ipynb file into DSX. Deploying to spark The model can be deployed to a Spark Cluster as a service. We did not finish this step yet. To integrate the call to the deployed model there is a specific Javascript module for the model deployed on ICP. Others For deploying of DSX on ICP see this detailed section","title":"Building the machine learning model using DSX in ICP"},{"location":"ml/icp-dsx-ml-model/#building-the-machine-learning-model-using-dsx-in-icp","text":"For a pure DSX, Db2 Warehouse and Spark cluster approach, running inside ICP the high level view may look like the figure below: We use Db2 warehouse to persist the three types of data. The data ingestions can be done with external tools like ETL, or by combining the Remote table definition inside Db2 Warehouse and scripts. We explained the remote table approach in this note .","title":"Building the machine learning model using DSX in ICP"},{"location":"ml/icp-dsx-ml-model/#data-ingestion","text":"","title":"Data Ingestion"},{"location":"ml/icp-dsx-ml-model/#customer-data","text":"The approach was to use Db2 Warehouse remote table to get access to the data. See this note for detail .","title":"Customer data"},{"location":"ml/icp-dsx-ml-model/#callnotes","text":"The CallNotes were loaded using the csv file export and load from file capabilities in Db2 Warehouse. ( See this note on how to similarly do csv loading ). The source for the CallNotes.csv file is under data folder. The IDs used in this file match the ID within the DB2 customer table.","title":"CallNotes"},{"location":"ml/icp-dsx-ml-model/#campaign-responses","text":"The marketing campaign are in the csv file named CampainResponses_Expanded.csv under the data folder. The CSV file is loaded into Db2 warehouse.","title":"Campaign Responses"},{"location":"ml/icp-dsx-ml-model/#specific-jupyter-notebook","text":"The Jupyter notebook is slightly different than the one developed within Watson Data Platform as the data sources are different. You can read the exported as markdown file here or load the src/dsx/CustomerChurnAnalysisDSXICP.ipynb file into DSX.","title":"Specific jupyter notebook"},{"location":"ml/icp-dsx-ml-model/#deploying-to-spark","text":"The model can be deployed to a Spark Cluster as a service. We did not finish this step yet. To integrate the call to the deployed model there is a specific Javascript module for the model deployed on ICP.","title":"Deploying to spark"},{"location":"ml/icp-dsx-ml-model/#others","text":"For deploying of DSX on ICP see this detailed section","title":"Others"},{"location":"persist/chattranscripts/","text":"Persisting the chat transcripts To persist the conversation content, we selected a document oriented database running on IBM Cloud, public offering. The Web application can persist the conversation interactions in a single document. The control is done with the parameter conversation.usePersistence in the config.json under the server/config folder. Pre-requisite You need to have an active account on IBM Cloud , an organization and a space created. Create a Cloudant Service in IBM Cloud Use the Create resource button on top of the main IBM Cloud dashboard page, select Data & Analytics under the Platform category and then the Cloudant NoSQL DB service: Be sure to select the region, organization and space you want to service to be added to. Once created go to the service main page and Launch the client tool to create new database. This web application helps administrators to access database instances. Create a Database: wcsdb From the top menu select Create Database and enter wcsdb as a name: The main database dashboard is now displayed. We do not need to create any document yet, as the code will do it. Get service credentials So to make to code accessing the database we need to get the service credentials. At the Cloudant service main page select Service Credentials and add a new credential . Open the config.json file to add the URL of the service and enable persistence in the conversation settings. ATTENTION: when deploying into IBM Cloud private the configuration is defined in the deployment configuration. So you may want to tune both. \"watsonassistant\": { \"usePersistence\": false }, \"dbCredentials\" : { \"url\": \"https://...-bluemix:cd....@...e50-bluemix.cloudant.com\" }, Implement service client The code is in the server/routes/features/persist.js . The method is using Cloudant API module, and the conversation response. The code is using the persistId and revId of cloudant response to modify the Watson Assistant context with those two variables so a unique document is created for all interaction, and the document is updated at each interaction. saveConversation : function(config,conv,next){ var cloudant = require('cloudant')(config.dbCredentials.url); var db = cloudant.use('wcsdb'); if (conv.context !== undefined) { if (conv.context.revId !== undefined) { conv._id=conv.context.persistId; conv._rev=conv.context.revId; } } db.insert(conv, function(err, data) { if (err) { next({error: err.message}); } else { next(data); } }); }, // saveConversation Browse conversation content in Cloudant console After few conversation sessions, you can access some of the created documents using the Cloudant console: Each document has a unique ID and revision id. All the content of the conversation is persisted. It is helpful for assessing the conversation that did not terminate well, or with gaps in the scope of the dialog. Access all conversations from a given timestamp The persist.js add a timestamp variable in the persisted document so queries can be done on the document creation date. Cloudant exposes a query editor to search for documents in the database. The query is a JSON document.","title":"Persisting the chatbot transcripts"},{"location":"persist/chattranscripts/#persisting-the-chat-transcripts","text":"To persist the conversation content, we selected a document oriented database running on IBM Cloud, public offering. The Web application can persist the conversation interactions in a single document. The control is done with the parameter conversation.usePersistence in the config.json under the server/config folder.","title":"Persisting the chat transcripts"},{"location":"persist/chattranscripts/#pre-requisite","text":"You need to have an active account on IBM Cloud , an organization and a space created.","title":"Pre-requisite"},{"location":"persist/chattranscripts/#create-a-cloudant-service-in-ibm-cloud","text":"Use the Create resource button on top of the main IBM Cloud dashboard page, select Data & Analytics under the Platform category and then the Cloudant NoSQL DB service: Be sure to select the region, organization and space you want to service to be added to. Once created go to the service main page and Launch the client tool to create new database. This web application helps administrators to access database instances.","title":"Create a Cloudant Service in IBM Cloud"},{"location":"persist/chattranscripts/#create-a-database-wcsdb","text":"From the top menu select Create Database and enter wcsdb as a name: The main database dashboard is now displayed. We do not need to create any document yet, as the code will do it.","title":"Create a Database: wcsdb"},{"location":"persist/chattranscripts/#get-service-credentials","text":"So to make to code accessing the database we need to get the service credentials. At the Cloudant service main page select Service Credentials and add a new credential . Open the config.json file to add the URL of the service and enable persistence in the conversation settings. ATTENTION: when deploying into IBM Cloud private the configuration is defined in the deployment configuration. So you may want to tune both. \"watsonassistant\": { \"usePersistence\": false }, \"dbCredentials\" : { \"url\": \"https://...-bluemix:cd....@...e50-bluemix.cloudant.com\" },","title":"Get service credentials"},{"location":"persist/chattranscripts/#implement-service-client","text":"The code is in the server/routes/features/persist.js . The method is using Cloudant API module, and the conversation response. The code is using the persistId and revId of cloudant response to modify the Watson Assistant context with those two variables so a unique document is created for all interaction, and the document is updated at each interaction. saveConversation : function(config,conv,next){ var cloudant = require('cloudant')(config.dbCredentials.url); var db = cloudant.use('wcsdb'); if (conv.context !== undefined) { if (conv.context.revId !== undefined) { conv._id=conv.context.persistId; conv._rev=conv.context.revId; } } db.insert(conv, function(err, data) { if (err) { next({error: err.message}); } else { next(data); } }); }, // saveConversation","title":"Implement service client"},{"location":"persist/chattranscripts/#browse-conversation-content-in-cloudant-console","text":"After few conversation sessions, you can access some of the created documents using the Cloudant console: Each document has a unique ID and revision id. All the content of the conversation is persisted. It is helpful for assessing the conversation that did not terminate well, or with gaps in the scope of the dialog.","title":"Browse conversation content in Cloudant console"},{"location":"persist/chattranscripts/#access-all-conversations-from-a-given-timestamp","text":"The persist.js add a timestamp variable in the persisted document so queries can be done on the document creation date. Cloudant exposes a query editor to search for documents in the database. The query is a JSON document.","title":"Access all conversations from a given timestamp"},{"location":"wcs/","text":"Support Chat bot with Watson Assistant We recommend to follow this tutorial to learn how to develop a Watson Assistant service and dialog flow. The implemented Watson Assistant workspace is under the folder src/wcs , named telco-support-wcs-wks.json and you can import it into your own Watson Assistant service: Once loaded the workspace is named \"Telco Support\". What we built: For this project, looking at the use case description, and the type of interactions expected by end users, like Eddie, we implemented the following intents: Request status* to address when a user is asking from an existing rebate request, or support request. network data quality to support query about data issue while roaming. Then we worked on the dialog flow to define how to respond to those intents. We need to have one node per intent: Add one to support the rate plan request* intent, one to assess the status of an existing ticket, and one for data quality... In the Handle the ticket status request we use the conversation context to drive control of the chatbot flow: setting the context variables action and item to search and UserRequest . Also as conversation about a ticket status may be emotional, we trigger the call to ToneAnalyzer by setting a new context variable to true: When the system returns the ticket information, the code could have returned the answer directly, but it is good practice to come back to the dialog flow to manage response, usingthe context variables to control the response as illustrated in figure below: Finally when the tone analyzer returns a potential negative tone, the code calls the churn scoring service, adds the to the conversation context, and now the dialog can prepare an adequate response: using the churn risk scoring: If the scoring risk is below .7 the processing will use automatic email processing, the reported message will look like: as the node condition: When the score is higher than .7 , we want to like transfer to a human. Recall that you can unit test the conversation inside the Editor. See the tutorial on how to do so. Assistant","title":"Watson assistant  implementation"},{"location":"wcs/#support-chat-bot-with-watson-assistant","text":"We recommend to follow this tutorial to learn how to develop a Watson Assistant service and dialog flow. The implemented Watson Assistant workspace is under the folder src/wcs , named telco-support-wcs-wks.json and you can import it into your own Watson Assistant service: Once loaded the workspace is named \"Telco Support\".","title":"Support Chat bot with Watson Assistant"},{"location":"wcs/#what-we-built","text":"For this project, looking at the use case description, and the type of interactions expected by end users, like Eddie, we implemented the following intents: Request status* to address when a user is asking from an existing rebate request, or support request. network data quality to support query about data issue while roaming. Then we worked on the dialog flow to define how to respond to those intents. We need to have one node per intent: Add one to support the rate plan request* intent, one to assess the status of an existing ticket, and one for data quality... In the Handle the ticket status request we use the conversation context to drive control of the chatbot flow: setting the context variables action and item to search and UserRequest . Also as conversation about a ticket status may be emotional, we trigger the call to ToneAnalyzer by setting a new context variable to true: When the system returns the ticket information, the code could have returned the answer directly, but it is good practice to come back to the dialog flow to manage response, usingthe context variables to control the response as illustrated in figure below: Finally when the tone analyzer returns a potential negative tone, the code calls the churn scoring service, adds the to the conversation context, and now the dialog can prepare an adequate response: using the churn risk scoring: If the scoring risk is below .7 the processing will use automatic email processing, the reported message will look like: as the node condition: When the score is higher than .7 , we want to like transfer to a human. Recall that you can unit test the conversation inside the Editor. See the tutorial on how to do so. Assistant","title":"What we built:"}]}